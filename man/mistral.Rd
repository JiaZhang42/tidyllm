% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/api_functions.R
\name{mistral}
\alias{mistral}
\title{Send LLMMessage to Mistral API}
\usage{
mistral(
  .llm,
  .model = "mistral-large-latest",
  .stream = FALSE,
  .seed = NULL,
  .json = FALSE,
  .temperature = NULL,
  .timeout = 120,
  .wait = TRUE,
  .min_tokens_reset = 0L,
  .max_tokens = 1024,
  .min_tokens = NULL,
  .dry_run = FALSE,
  .verbose = FALSE
)
}
\arguments{
\item{.llm}{An existing LLMMessage object or an initial text prompt.}

\item{.model}{The model identifier (default: "mistral-large-latest").}

\item{.stream}{Should the answer be streamed to console as it comes (optional)}

\item{.seed}{Which seed should be used for random numbers  (optional).}

\item{.json}{Should output be structured as JSON  (default: FALSE).}

\item{.temperature}{Control for randomness in response generation (optional).}

\item{.timeout}{When should our connection time out (default: 120 seconds).}

\item{.wait}{Should we wait for rate limits if necessary?}

\item{.min_tokens_reset}{How many tokens should be remaining to wait until we wait for token reset?}

\item{.max_tokens}{Maximum number of tokens for response (default: 1024).}

\item{.min_tokens}{Minimum number of tokens for response (optional).}

\item{.dry_run}{If TRUE, perform a dry run and return the request object.}

\item{.verbose}{Should additional information be shown after the API call}
}
\value{
Returns an updated LLMMessage object.
}
\description{
Send LLMMessage to Mistral API
}
