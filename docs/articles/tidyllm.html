<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Get Started • tidyllm</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../apple-touch-icon-60x60.png">
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.4.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.4.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Get Started">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">tidyllm</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.1.8</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="active nav-item"><a class="nav-link" href="../articles/tidyllm.html">Get started</a></li>
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><a class="dropdown-item" href="../articles/tidyllm_classifiers.html">Classifying Texts with tidyllm</a></li>
    <li><a class="dropdown-item" href="../articles/tidyllm-pdfquestions.html">Structured Question Answering from PDFs</a></li>
    <li><a class="dropdown-item" href="../articles/tidyllm-synthetic-data.html">Generate Synthetic Data with tidyllm</a></li>
  </ul>
</li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">Changelog</a></li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/edubruell/tidyllm/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../logo.png" class="logo" alt=""><h1>Get Started</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/edubruell/tidyllm/blob/HEAD/vignettes/tidyllm.Rmd" class="external-link"><code>vignettes/tidyllm.Rmd</code></a></small>
      <div class="d-none name"><code>tidyllm.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="introduction-to-tidyllm">Introduction to tidyllm<a class="anchor" aria-label="anchor" href="#introduction-to-tidyllm"></a>
</h2>
<p>tidyllm is an R package designed to provide a unified interface for
interacting with various large language model APIs. This vignette will
guide you through the basic setup and usage of tidyllm.</p>
<div class="section level3">
<h3 id="installation">Installation<a class="anchor" aria-label="anchor" href="#installation"></a>
</h3>
<p>To install <strong>tidyllm</strong> from CRAN , use:</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/install.packages.html" class="external-link">install.packages</a></span><span class="op">(</span><span class="st">"tidyllm"</span><span class="op">)</span></span></code></pre></div>
<p>Or install a current development version directly from GitHub using
devtools:</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Install devtools if not already installed</span></span>
<span><span class="kw">if</span> <span class="op">(</span><span class="op">!</span><span class="fu"><a href="https://rdrr.io/r/base/ns-load.html" class="external-link">requireNamespace</a></span><span class="op">(</span><span class="st">"devtools"</span>, quietly <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/utils/install.packages.html" class="external-link">install.packages</a></span><span class="op">(</span><span class="st">"devtools"</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># Install TidyLLM from GitHub</span></span>
<span><span class="fu">devtools</span><span class="fu">::</span><span class="fu"><a href="https://remotes.r-lib.org/reference/install_github.html" class="external-link">install_github</a></span><span class="op">(</span><span class="st">"edubruell/tidyllm"</span><span class="op">)</span></span></code></pre></div>
<div class="section level4">
<h4 id="setting-up-api-keys-or-ollama">Setting up API Keys or ollama<a class="anchor" aria-label="anchor" href="#setting-up-api-keys-or-ollama"></a>
</h4>
<p>Before using tidyllm, you need to set up API keys for the services
you plan to use. Here’s how to set them up for different providers:</p>
<ol style="list-style-type: decimal">
<li>For Claude models you can get an API key in the <a href="https://console.anthropic.com/settings/keys" class="external-link">Anthropic
Console</a>:</li>
</ol>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Sys.setenv.html" class="external-link">Sys.setenv</a></span><span class="op">(</span>ANTHROPIC_API_KEY <span class="op">=</span> <span class="st">"YOUR-ANTHROPIC-API-KEY"</span><span class="op">)</span></span></code></pre></div>
<ol start="2" style="list-style-type: decimal">
<li>For ChatGPT you can obtain an API key by signing up at <a href="https://platform.openai.com/account/api-keys" class="external-link">OpenAI</a> and set
it with:</li>
</ol>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Sys.setenv.html" class="external-link">Sys.setenv</a></span><span class="op">(</span>OPENAI_API_KEY <span class="op">=</span> <span class="st">"YOUR-OPENAI-API-KEY"</span><span class="op">)</span></span></code></pre></div>
<ol start="3" style="list-style-type: decimal">
<li>For Mistral you can set the API key on the <a href="https://console.mistral.ai/api-keys/" class="external-link">Mistral console page</a> and
set it by</li>
</ol>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Sys.setenv.html" class="external-link">Sys.setenv</a></span><span class="op">(</span>MISTRAL_API_KEY <span class="op">=</span> <span class="st">"MISTRAL-API-KEY-GOES-HERE"</span><span class="op">)</span></span></code></pre></div>
<ol start="4" style="list-style-type: decimal">
<li>For groq (not be confused with <a href="https://x.ai/" class="external-link">grok</a>) you
can setup you API keys in the <a href="https://console.groq.com/playground" class="external-link">Groq Console</a>:</li>
</ol>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Sys.setenv.html" class="external-link">Sys.setenv</a></span><span class="op">(</span>GROQ_API_KEY <span class="op">=</span> <span class="st">"YOUR-GROQ-API-KEY"</span><span class="op">)</span></span></code></pre></div>
<p>Alternatively, you can set these keys in your <code>.Renviron</code>
file for persistent storage. For this, execute
<code><a href="https://usethis.r-lib.org/reference/edit.html" class="external-link">usethis::edit_r_environ()</a></code>, and add a line with with an API
key in this file, for example:</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">ANTHROPIC_API_KEY</span><span class="op">=</span><span class="st">"YOUR-ANTHROPIC-API-KEY"</span></span></code></pre></div>
<p>If you want to work with local large lange models via
<code>ollama</code> you need to install it from <a href="https://ollama.com/" class="external-link">the official project website</a>. Ollama sets
up a local large language model server that you can use to run
open-source models on your own devices.</p>
</div>
<div class="section level4">
<h4 id="basic-usage">Basic Usage<a class="anchor" aria-label="anchor" href="#basic-usage"></a>
</h4>
<p>Let’s start with a simple example using tidyllm to interact with
different language models:</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/edubruell/tidyllm" class="external-link">tidyllm</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Start a conversation with Claude</span></span>
<span><span class="va">conversation</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/llm_message.html">llm_message</a></span><span class="op">(</span><span class="st">"What is the capital of France?"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/claude.html">claude</a></span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="co">#Standard way that llm_messages are printed</span></span>
<span><span class="va">conversation</span></span></code></pre></div>
<pre><code><span><span class="co">## Message History:</span></span>
<span><span class="co">## system: You are a helpful assistant</span></span>
<span><span class="co">## --------------------------------------------------------------</span></span>
<span><span class="co">## user: What is the capital of France?</span></span>
<span><span class="co">## --------------------------------------------------------------</span></span>
<span><span class="co">## assistant: The capital of France is Paris.</span></span>
<span><span class="co">## --------------------------------------------------------------</span></span></code></pre>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Continue the conversation with ChatGPT</span></span>
<span><span class="va">conversation</span> <span class="op">&lt;-</span> <span class="va">conversation</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/llm_message.html">llm_message</a></span><span class="op">(</span><span class="st">"What's a famous landmark in this city?"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/chatgpt.html">chatgpt</a></span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="../reference/last_reply.html">last_reply</a></span><span class="op">(</span><span class="va">conversation</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] "A famous landmark in Paris is the Eiffel Tower."</span></span></code></pre>
</div>
<div class="section level4">
<h4 id="sending-images-to-models">Sending Images to Models<a class="anchor" aria-label="anchor" href="#sending-images-to-models"></a>
</h4>
<p>tidyllm also supports sending images to multimodal models. Let’s send
this picture here: <img src="picture.jpeg" width="70%"></p>
<p>Here we let ChatGPT guess where the picture was made:</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Describe an image using a llava model on ollama</span></span>
<span><span class="va">image_description</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/llm_message.html">llm_message</a></span><span class="op">(</span><span class="st">"Describe this picture? Can you guess where it was made?"</span>,</span>
<span>                                 .imagefile <span class="op">=</span> <span class="st">"picture.jpeg"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/chatgpt.html">chatgpt</a></span><span class="op">(</span>.model <span class="op">=</span> <span class="st">"gpt-4o"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Get the last reply</span></span>
<span><span class="fu"><a href="../reference/last_reply.html">last_reply</a></span><span class="op">(</span><span class="va">image_description</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] "The picture shows a beautiful landscape with a lake, mountains, and a town nestled below. The sun is shining brightly, casting a serene glow over the water. The area appears lush and green, with agricultural fields visible. \n\nThis type of scenery is reminiscent of northern Italy, particularly around Lake Garda, which features similar large mountains, picturesque water, and charming towns."</span></span></code></pre>
</div>
<div class="section level4">
<h4 id="adding-pdfs-to-messages">Adding PDFs to messages<a class="anchor" aria-label="anchor" href="#adding-pdfs-to-messages"></a>
</h4>
<p>The <code><a href="../reference/llm_message.html">llm_message()</a></code> function also supports extracting text
from PDFs and including it in the message. This allows you to easily
provide context from a PDF document when interacting with an AI
assistant.</p>
<p>To use this feature, you need to have the <code>pdftools</code>
package installed. If it is not already installed, you can install it
with:</p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/install.packages.html" class="external-link">install.packages</a></span><span class="op">(</span><span class="st">"pdftools"</span><span class="op">)</span></span></code></pre></div>
<p>To include text from a PDF in your prompt, simply pass the file path
to the <code>.pdf</code> argument of the <code>chat</code> function:</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/llm_message.html">llm_message</a></span><span class="op">(</span><span class="st">"Please summarize the key points from the provided PDF document."</span>, </span>
<span>     .pdf <span class="op">=</span> <span class="st">"die_verwandlung.pdf"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>     <span class="fu"><a href="../reference/chatgpt.html">chatgpt</a></span><span class="op">(</span>.model <span class="op">=</span> <span class="st">"gpt-4o-mini"</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Message History:</span></span>
<span><span class="co">## system: You are a helpful assistant</span></span>
<span><span class="co">## --------------------------------------------------------------</span></span>
<span><span class="co">## user: Please summarize the key points from the provided PDF document.</span></span>
<span><span class="co">##  -&gt; Attached Media Files:  die_verwandlung.pdf </span></span>
<span><span class="co">## --------------------------------------------------------------</span></span>
<span><span class="co">## assistant: Here are the key points from the provided PDF document 'Die Verwandlung' by Franz Kafka:</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## 1. The story centers around Gregor Samsa, who wakes up one morning to find that he has been transformed into a giant insect-like creature.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## 2. Gregor's transformation causes distress and disruption for his family. They struggle to come to terms with the situation and how to deal with Gregor in his new state.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## 3. Gregor's family, especially his sister Grete, initially tries to care for him, but eventually decides they need to get rid of him. They lock him in his room and discuss finding a way to remove him.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## 4. Gregor becomes increasingly isolated and neglected by his family. He becomes weaker and less mobile due to his injuries and lack of proper care.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## 5. Eventually, Gregor dies, and his family is relieved. They then begin to make plans to move to a smaller, more affordable apartment and start looking for new jobs and opportunities.</span></span>
<span><span class="co">## --------------------------------------------------------------</span></span></code></pre>
<p>The package will automatically extract the text from the PDF and
include it in the prompt sent to the an API. The text will be wrapped in
<code>&lt;pdf&gt;</code> tags to clearly indicate the content from the
PDF:</p>
<pre><code>Please summarize the key points from the provided PDF document.

&lt;pdf filename="example_document.pdf"&gt;
Extracted text from the PDF file...
&lt;/pdf&gt;</code></pre>
</div>
<div class="section level4">
<h4 id="sending-r-outputs-to-language-models">Sending R Outputs to Language Models<a class="anchor" aria-label="anchor" href="#sending-r-outputs-to-language-models"></a>
</h4>
<p>You can automatically include R code outputs in your prompts.
<code><a href="../reference/llm_message.html">llm_message()</a></code> has an optional argument <code>.f</code> in
which you can specify a (anonymous) function, which will be run and
which console output will be captured and appended to the message when
you run it.</p>
<p>In addition you can use <code>.caputre_plot</code> to send the last
plot pane to a model.</p>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://tidyverse.tidyverse.org" class="external-link">tidyverse</a></span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## ── <span style="font-weight: bold;">Attaching core tidyverse packages</span> ──────────────────────── tidyverse 2.0.0 ──</span></span>
<span><span class="co">## <span style="color: #00BB00;">✔</span> <span style="color: #0000BB;">dplyr    </span> 1.1.4     <span style="color: #00BB00;">✔</span> <span style="color: #0000BB;">readr    </span> 2.1.5</span></span>
<span><span class="co">## <span style="color: #00BB00;">✔</span> <span style="color: #0000BB;">forcats  </span> 1.0.0     <span style="color: #00BB00;">✔</span> <span style="color: #0000BB;">stringr  </span> 1.5.1</span></span>
<span><span class="co">## <span style="color: #00BB00;">✔</span> <span style="color: #0000BB;">ggplot2  </span> 3.5.1     <span style="color: #00BB00;">✔</span> <span style="color: #0000BB;">tibble   </span> 3.2.1</span></span>
<span><span class="co">## <span style="color: #00BB00;">✔</span> <span style="color: #0000BB;">lubridate</span> 1.9.3     <span style="color: #00BB00;">✔</span> <span style="color: #0000BB;">tidyr    </span> 1.3.1</span></span>
<span><span class="co">## <span style="color: #00BB00;">✔</span> <span style="color: #0000BB;">purrr    </span> 1.0.2     </span></span>
<span><span class="co">## ── <span style="font-weight: bold;">Conflicts</span> ────────────────────────────────────────── tidyverse_conflicts() ──</span></span>
<span><span class="co">## <span style="color: #BB0000;">✖</span> <span style="color: #0000BB;">dplyr</span>::<span style="color: #00BB00;">filter()</span> masks <span style="color: #0000BB;">stats</span>::filter()</span></span>
<span><span class="co">## <span style="color: #BB0000;">✖</span> <span style="color: #0000BB;">dplyr</span>::<span style="color: #00BB00;">lag()</span>    masks <span style="color: #0000BB;">stats</span>::lag()</span></span>
<span><span class="co">## <span style="color: #00BBBB;">ℹ</span> Use the conflicted package (<span style="color: #0000BB; font-style: italic;">&lt;http://conflicted.r-lib.org/&gt;</span>) to force all conflicts to become errors</span></span></code></pre>
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Create a plot for the mtcars example data</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html" class="external-link">ggplot</a></span><span class="op">(</span><span class="va">mtcars</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html" class="external-link">aes</a></span><span class="op">(</span><span class="va">wt</span>, <span class="va">mpg</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html" class="external-link">geom_point</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_smooth.html" class="external-link">geom_smooth</a></span><span class="op">(</span>method <span class="op">=</span> <span class="st">"lm"</span>, formula <span class="op">=</span> <span class="st">'y ~ x'</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html" class="external-link">labs</a></span><span class="op">(</span>x<span class="op">=</span><span class="st">"Weight"</span>,y<span class="op">=</span><span class="st">"Miles per gallon"</span><span class="op">)</span></span></code></pre></div>
<p><img src="tidyllm_files/figure-html/routputs_base-1.png" width="700"></p>
<p>Now we can send the plot and data summary to a language model:</p>
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://tidyverse.tidyverse.org" class="external-link">tidyverse</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="../reference/llm_message.html">llm_message</a></span><span class="op">(</span><span class="st">"Analyze this plot and data summary:"</span>, </span>
<span>                  .capture_plot <span class="op">=</span> <span class="cn">TRUE</span>, <span class="co">#Send the plot pane to a model</span></span>
<span>                  .f <span class="op">=</span> <span class="op">~</span><span class="op">{</span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">mtcars</span><span class="op">)</span><span class="op">}</span><span class="op">)</span> <span class="op">|&gt;</span> <span class="co">#Run summary(data) and send the output</span></span>
<span>  <span class="fu"><a href="../reference/claude.html">claude</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Message History:</span></span>
<span><span class="co">## system: You are a helpful assistant</span></span>
<span><span class="co">## --------------------------------------------------------------</span></span>
<span><span class="co">## user: Analyze this plot and data summary:</span></span>
<span><span class="co">##  -&gt; Attached Media Files:  file1568f6c1b4565.png, RConsole.txt </span></span>
<span><span class="co">## --------------------------------------------------------------</span></span>
<span><span class="co">## assistant: Based on the plot and data summary provided, here's an analysis:</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## 1. Relationship between Weight and MPG:</span></span>
<span><span class="co">##    The scatter plot shows a clear negative correlation between weight (wt) and miles per gallon (mpg). As the weight of the car increases, the fuel efficiency (mpg) decreases.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## 2. Linear Trend:</span></span>
<span><span class="co">##    The blue line in the plot represents a linear regression fit. The downward slope confirms the negative relationship between weight and mpg.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## 3. Data Distribution:</span></span>
<span><span class="co">##    - The weight of cars in the dataset ranges from 1.513 to 5.424 (likely in thousands of pounds).</span></span>
<span><span class="co">##    - The mpg values range from 10.40 to 33.90.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## 4. Variability:</span></span>
<span><span class="co">##    There's some scatter around the regression line, indicating that while weight is a strong predictor of mpg, other factors also influence fuel efficiency.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## 5. Other Variables:</span></span>
<span><span class="co">##    While not shown in the plot, the summary statistics provide information on other variables:</span></span>
<span><span class="co">##    - Cylinder count (cyl) ranges from 4 to 8, with a median of 6.</span></span>
<span><span class="co">##    - Horsepower (hp) ranges from 52 to 335, with a mean of 146.7.</span></span>
<span><span class="co">##    - Transmission type (am) is binary (0 or 1), likely indicating automatic vs. manual.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## 6. Model Fit:</span></span>
<span><span class="co">##    The grey shaded area around the regression line represents the confidence interval. It widens at the extremes of the weight range, indicating less certainty in predictions for very light or very heavy vehicles.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## 7. Outliers:</span></span>
<span><span class="co">##    There are a few potential outliers, particularly at the lower and higher ends of the weight spectrum, that deviate from the general trend.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## In conclusion, this analysis strongly suggests that weight is a significant factor in determining a car's fuel efficiency, with heavier cars generally having lower mpg. However, the presence of scatter in the data indicates that other factors (possibly related to engine characteristics, transmission type, or aerodynamics) also play a role in determining fuel efficiency.</span></span>
<span><span class="co">## --------------------------------------------------------------</span></span></code></pre>
</div>
<div class="section level4">
<h4 id="getting-the-last-reply-as-raw-text-or-structured-data">Getting the last reply (as raw text or structured data)<a class="anchor" aria-label="anchor" href="#getting-the-last-reply-as-raw-text-or-structured-data"></a>
</h4>
<p>You can retrieve the last assistant reply from a message history with
<code><a href="../reference/last_reply.html">last_reply()</a></code>. Typically, it returns a character vector with
the text of the assistant’s reply. However, if API functions have
requested replies in <a href="https://de.wikipedia.org/wiki/JavaScript_Object_Notation" class="external-link">JSON</a>
mode, it can directly validate and return them as structured output. The
function handles these different response types automatically.</p>
<p>If a JSON reply is detected, it returns a list with the following
fields:</p>
<ul>
<li>
<code>parsed_content</code>: The parsed JSON content (or
<code>NULL</code> in case of parsing errors).</li>
<li>
<code>raw_response</code>: The direct string format of the
reply.</li>
<li>
<code>is_parsed</code>: A flag set to <code>TRUE</code> if JSON
parsing was successful, or <code>FALSE</code> otherwise.</li>
</ul>
<p>You can also force standard raw text replies, even when JSON mode is
detected, using the <code>.raw</code> argument.</p>
<div class="section level5">
<h5 id="example-1-getting-standard-text-replies">Example 1: Getting standard text replies<a class="anchor" aria-label="anchor" href="#example-1-getting-standard-text-replies"></a>
</h5>
<div class="sourceCode" id="cb23"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">reply_text</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/llm_message.html">llm_message</a></span><span class="op">(</span><span class="st">"Imagine a German adress."</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>     <span class="fu"><a href="../reference/groq.html">groq</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>     <span class="fu"><a href="../reference/last_reply.html">last_reply</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Message History:</span></span>
<span><span class="co">## system: You are a helpful assistant</span></span>
<span><span class="co">## --------------------------------------------------------------</span></span>
<span><span class="co">## user: Imagine a German adress.</span></span>
<span><span class="co">## --------------------------------------------------------------</span></span>
<span><span class="co">## assistant: Let's imagine a German address. </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Herr Müller</span></span>
<span><span class="co">## Musterstraße 12</span></span>
<span><span class="co">## 53111 Bonn</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## This address is formatted according to German conventions:</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## - 'Herr Müller' is the recipient's name (Mr. Müller).</span></span>
<span><span class="co">## - 'Musterstraße 12' is the street name and number.</span></span>
<span><span class="co">## - '53111 Bonn' is the postal code and city (Bonn).</span></span>
<span><span class="co">## --------------------------------------------------------------</span></span></code></pre>
</div>
<div class="section level5">
<h5 id="example-2-getting-structured-replies-from-apis-in-json-mode">Example 2: Getting structured replies from APIs in JSON mode<a class="anchor" aria-label="anchor" href="#example-2-getting-structured-replies-from-apis-in-json-mode"></a>
</h5>
<div class="sourceCode" id="cb25"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">address</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/llm_message.html">llm_message</a></span><span class="op">(</span><span class="st">'Imagine a German adress in JSON format. Reply only with JSON.'</span><span class="op">)</span> </span>
<span></span>
<span><span class="va">address</span><span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/ollama.html">ollama</a></span><span class="op">(</span>.json <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span> <span class="op">|&gt;</span>  <span class="co"># API is asked to return JSON</span></span>
<span>  <span class="fu"><a href="../reference/last_reply.html">last_reply</a></span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/str.html" class="external-link">str</a></span><span class="op">(</span><span class="va">address</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## List of 3</span></span>
<span><span class="co">##  $ raw_response  : chr "{\n   \"street\": \"Kurfürstenstraße\",\n   \"houseNumber\": 23,\n   \"postcode\": \"10785\",\n   \"city\": \"B"| __truncated__</span></span>
<span><span class="co">##  $ parsed_content:List of 6</span></span>
<span><span class="co">##   ..$ street     : chr "Kurfürstenstraße"</span></span>
<span><span class="co">##   ..$ houseNumber: int 23</span></span>
<span><span class="co">##   ..$ postcode   : chr "10785"</span></span>
<span><span class="co">##   ..$ city       : chr "Berlin"</span></span>
<span><span class="co">##   ..$ region     : chr "Berlin"</span></span>
<span><span class="co">##   ..$ country    : chr "Deutschland"</span></span>
<span><span class="co">##  $ is_parsed     : logi TRUE</span></span></code></pre>
<p>All API functions have a <code>.json</code>-argument that enables
JSON-mode. Note that <code><a href="../reference/claude.html">claude()</a></code> does not have an explicit
JSON-mode in the API-request but you need to specify that you want only
JSON-output and ideally your shema in the prompt to the assistant.</p>
</div>
</div>
<div class="section level4">
<h4 id="api-parameters">API parameters<a class="anchor" aria-label="anchor" href="#api-parameters"></a>
</h4>
<p>Different API functions support different model parameters like how
deterministic the response should be via parameters like temperature.
Please read API-documentation and the documentation of the model
functions for specific examples.</p>
<div class="sourceCode" id="cb27"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span>  <span class="va">temp_example</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/llm_message.html">llm_message</a></span><span class="op">(</span><span class="st">"Explain how temperature parameters work in large language models  and why temperature 0 gives you deterministic outputs in one sentence."</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="co">#per default it is non-zero</span></span>
<span>  <span class="va">temp_example</span> <span class="op">|&gt;</span> <span class="fu"><a href="../reference/ollama.html">ollama</a></span><span class="op">(</span>.temperature<span class="op">=</span><span class="fl">0</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Message History:</span></span>
<span><span class="co">## system: You are a helpful assistant</span></span>
<span><span class="co">## --------------------------------------------------------------</span></span>
<span><span class="co">## user: Explain how temperature parameters work in large language models  and why temperature 0 gives you deterministic outputs in one sentence.</span></span>
<span><span class="co">## --------------------------------------------------------------</span></span>
<span><span class="co">## assistant: In large language models, temperature parameters control the randomness of generated text by scaling the output probabilities, with higher temperatures introducing more uncertainty and lower temperatures favoring more likely outcomes; specifically, setting temperature to 0 effectively eliminates all randomness, resulting in deterministic outputs because it sets the probability of each token to its maximum likelihood value.</span></span>
<span><span class="co">## --------------------------------------------------------------</span></span></code></pre>
<div class="sourceCode" id="cb29"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span>  <span class="co">#Retrying with .temperature=0</span></span>
<span>  <span class="va">temp_example</span> <span class="op">|&gt;</span> <span class="fu"><a href="../reference/ollama.html">ollama</a></span><span class="op">(</span>.temperature<span class="op">=</span><span class="fl">0</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Message History:</span></span>
<span><span class="co">## system: You are a helpful assistant</span></span>
<span><span class="co">## --------------------------------------------------------------</span></span>
<span><span class="co">## user: Explain how temperature parameters work in large language models  and why temperature 0 gives you deterministic outputs in one sentence.</span></span>
<span><span class="co">## --------------------------------------------------------------</span></span>
<span><span class="co">## assistant: In large language models, temperature parameters control the randomness of generated text by scaling the output probabilities, with higher temperatures introducing more uncertainty and lower temperatures favoring more likely outcomes; specifically, setting temperature to 0 effectively eliminates all randomness, resulting in deterministic outputs because it sets the probability of each token to its maximum likelihood value.</span></span>
<span><span class="co">## --------------------------------------------------------------</span></span></code></pre>
</div>
<div class="section level4">
<h4 id="streaming-back-responses-experimental">Streaming back responses (Experimental)<a class="anchor" aria-label="anchor" href="#streaming-back-responses-experimental"></a>
</h4>
<p>At the moment <code><a href="../reference/ollama.html">ollama()</a></code>, <code><a href="../reference/chatgpt.html">chatgpt()</a></code>,
<code><a href="../reference/mistral.html">mistral()</a></code> and <code><a href="../reference/claude.html">claude()</a></code> support real-time
streaming of reply tokens to the console while the model works with the
<code>.stream=TRUE</code> argument. While this feature offers slightly
better feedback on model behavior in real-time, it’s not particularly
useful for data-analysis workflows. We consider this feature
experimental and recommend using non-streaming responses for production
tasks. Note that error handling in streaming callbacks varies by API and
differs in quality at this time.</p>
</div>
<div class="section level4">
<h4 id="choosing-the-right-model-and-api">Choosing the Right Model and API<a class="anchor" aria-label="anchor" href="#choosing-the-right-model-and-api"></a>
</h4>
<p>tidyllm supports multiple APIs, each offering distinct large language
models with varying strengths. The choice of which model or API to use
often depends on the specific task, cost considerations, and data
privacy concerns.</p>
<ol style="list-style-type: decimal">
<li><p><strong>Claude (Anthropic API):</strong> Claude is known for
generating thoughtful, nuanced responses, making it ideal for tasks that
require more human-like reasoning, such as summarization or creative
writing. Calude Sonnet 3.5 currently is one of the top-performing models
on many benchmarks. However, it can sometimes be more verbose than
necessary, and it lacks direct JSON support, which requires additional
prompting and validation to ensure structured output.</p></li>
<li><p><strong>ChatGPT (OpenAI API):</strong> ChatGPT, particularly the
GPT-4o model, is extremely versatile and performs well across a wide
range of tasks, including text generation, code completion, and
multimodal analysis. While it is good for most cases, it tends to be
more expensive than other alternatives, especially for large-scale
usage.</p></li>
<li><p><strong>Mistral (EU-based):</strong> Mistral offers
lighter-weight, open-source models developed and hosted in the EU,
making it particularly appealing if data protection (e.g., GDPR
compliance) is a concern. While the models may not be as powerful as
GPT-4o or Claude Sonnet, Mistral offers good performance for standard
text generation tasks.</p></li>
<li><p><strong>Groq (Fast):</strong> Groq offers a unique advantage with
its custom AI accelerator hardware, that get you the fastest output
available on any API. It delivers high performance at low costs,
especially for tasks that require fast execution. It hosts many strong
open-source models, like <strong>lamma3:70b</strong>.</p></li>
<li><p><strong>ollama (Local Models):</strong> If data privacy is a
priority, running open-source models like <strong>gemma2::9B</strong>
locally via <code><a href="../reference/ollama.html">ollama()</a></code> gives you full control over model
execution and data. However, the trade-off is that local models require
significant computational resources, and are often not quite as powerful
as the large API-providers. The <a href="https://ollama.com/blog" class="external-link">ollama
blog</a> regularly has posts about new models and their advantages that
you can download via <code><a href="../reference/ollama_download_model.html">ollama_download_model()</a></code>.</p></li>
</ol>
</div>
</div>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Eduard Brüll.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.1.</p>
</div>

    </footer>
</div>





  </body>
</html>
