<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Get Started • tidyllm</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../apple-touch-icon-60x60.png">
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.4.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.4.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Get Started">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">tidyllm</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.2.2</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="active nav-item"><a class="nav-link" href="../articles/tidyllm.html">Get started</a></li>
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><a class="dropdown-item" href="../articles/tidyllm_classifiers.html">Classifying Texts with tidyllm</a></li>
    <li><a class="dropdown-item" href="../articles/tidyllm-pdfquestions.html">Structured Question Answering from PDFs</a></li>
    <li><a class="dropdown-item" href="../articles/tidyllm-synthetic-data.html">Generate Synthetic Data with tidyllm</a></li>
  </ul>
</li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">Changelog</a></li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/edubruell/tidyllm/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../logo.png" class="logo" alt=""><h1>Get Started</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/edubruell/tidyllm/blob/HEAD/vignettes/tidyllm.Rmd" class="external-link"><code>vignettes/tidyllm.Rmd</code></a></small>
      <div class="d-none name"><code>tidyllm.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="introduction-to-tidyllm">Introduction to tidyllm<a class="anchor" aria-label="anchor" href="#introduction-to-tidyllm"></a>
</h2>
<p><strong>tidyllm</strong> is an R package providing a unified
interface for interacting with various large language model APIs. This
vignette will guide you through the basic setup and usage of
<strong>tidyllm</strong>.</p>
<div class="section level3">
<h3 id="installation">Installation<a class="anchor" aria-label="anchor" href="#installation"></a>
</h3>
<p>To install <strong>tidyllm</strong> from CRAN, use:</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/install.packages.html" class="external-link">install.packages</a></span><span class="op">(</span><span class="st">"tidyllm"</span><span class="op">)</span></span></code></pre></div>
<p>Or, to install the current development version directly from GitHub
using devtools:</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Install devtools if not already installed</span></span>
<span><span class="kw">if</span> <span class="op">(</span><span class="op">!</span><span class="fu"><a href="https://rdrr.io/r/base/ns-load.html" class="external-link">requireNamespace</a></span><span class="op">(</span><span class="st">"devtools"</span>, quietly <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/utils/install.packages.html" class="external-link">install.packages</a></span><span class="op">(</span><span class="st">"devtools"</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># Install TidyLLM from GitHub</span></span>
<span><span class="fu">devtools</span><span class="fu">::</span><span class="fu"><a href="https://remotes.r-lib.org/reference/install_github.html" class="external-link">install_github</a></span><span class="op">(</span><span class="st">"edubruell/tidyllm"</span><span class="op">)</span></span></code></pre></div>
<div class="section level4">
<h4 id="setting-up-api-keys-or-ollama">Setting up API Keys or ollama<a class="anchor" aria-label="anchor" href="#setting-up-api-keys-or-ollama"></a>
</h4>
<p>Before using <strong>tidyllm</strong>, set up API keys for the
services you plan to use. Here’s how to set them up for different
providers:</p>
<ol style="list-style-type: decimal">
<li>For Claude models you can get an API key in the <a href="https://console.anthropic.com/settings/keys" class="external-link">Anthropic
Console</a>:</li>
</ol>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Sys.setenv.html" class="external-link">Sys.setenv</a></span><span class="op">(</span>ANTHROPIC_API_KEY <span class="op">=</span> <span class="st">"YOUR-ANTHROPIC-API-KEY"</span><span class="op">)</span></span></code></pre></div>
<ol start="2" style="list-style-type: decimal">
<li>For OpenAI you can obtain an API key by signing up at <a href="https://platform.openai.com/account/api-keys" class="external-link">OpenAI</a> and set
it with:</li>
</ol>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Sys.setenv.html" class="external-link">Sys.setenv</a></span><span class="op">(</span>OPENAI_API_KEY <span class="op">=</span> <span class="st">"YOUR-OPENAI-API-KEY"</span><span class="op">)</span></span></code></pre></div>
<ol start="3" style="list-style-type: decimal">
<li>In the latest development version Google Gemini is also supported.
Setup an API-key in the <a href="https://aistudio.google.com/app/apikey" class="external-link">Google AI Studio</a>
</li>
</ol>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Sys.setenv.html" class="external-link">Sys.setenv</a></span><span class="op">(</span>GOOGLE_API_KEY <span class="op">=</span> <span class="st">'YOUR-GOOGLE-API-KEY'</span><span class="op">)</span></span></code></pre></div>
<ol start="4" style="list-style-type: decimal">
<li>For Mistral you can set the API key on the <a href="https://console.mistral.ai/api-keys/" class="external-link">Mistral console page</a> and
set it by</li>
</ol>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Sys.setenv.html" class="external-link">Sys.setenv</a></span><span class="op">(</span>MISTRAL_API_KEY <span class="op">=</span> <span class="st">"YOUR-MISTRAL-API-KEY"</span><span class="op">)</span></span></code></pre></div>
<ol start="5" style="list-style-type: decimal">
<li>For groq (not be confused with <a href="https://x.ai/" class="external-link">grok</a>) you
can setup you API keys in the <a href="https://console.groq.com/playground" class="external-link">Groq Console</a>:</li>
</ol>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Sys.setenv.html" class="external-link">Sys.setenv</a></span><span class="op">(</span>GROQ_API_KEY <span class="op">=</span> <span class="st">"YOUR-GROQ-API-KEY"</span><span class="op">)</span></span></code></pre></div>
<p>Alternatively, for persistent storage, add these keys to your
<code>.Renviron</code> file:</p>
<p>For this, run <code><a href="https://usethis.r-lib.org/reference/edit.html" class="external-link">usethis::edit_r_environ()</a></code>, and add a line
with with an API key in this file, for example:</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">ANTHROPIC_API_KEY</span><span class="op">=</span><span class="st">"YOUR-ANTHROPIC-API-KEY"</span></span></code></pre></div>
<p>If you want to work with local large lange models via
<code>ollama</code> you need to install it from <a href="https://ollama.com/" class="external-link">the official project website</a>. Ollama sets
up a local large language model server that you can use to run
open-source models on your own devices.</p>
</div>
<div class="section level4">
<h4 id="basic-usage">Basic Usage<a class="anchor" aria-label="anchor" href="#basic-usage"></a>
</h4>
<p>Let’s start with a simple example using tidyllm to interact with
different language models:</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://edubruell.github.io/tidyllm/">tidyllm</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Start a conversation with Claude</span></span>
<span><span class="va">conversation</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/llm_message.html">llm_message</a></span><span class="op">(</span><span class="st">"What is the capital of France?"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/claude.html">claude</a></span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="co">#Standard way that llm_messages are printed</span></span>
<span><span class="va">conversation</span></span></code></pre></div>
<pre><code><span><span class="co">## Message History:</span></span>
<span><span class="co">## system: You are a helpful assistant</span></span>
<span><span class="co">## --------------------------------------------------------------</span></span>
<span><span class="co">## user: What is the capital of France?</span></span>
<span><span class="co">## --------------------------------------------------------------</span></span>
<span><span class="co">## assistant: The capital of France is Paris.</span></span>
<span><span class="co">## --------------------------------------------------------------</span></span></code></pre>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Continue the conversation with ChatGPT</span></span>
<span><span class="va">conversation</span> <span class="op">&lt;-</span> <span class="va">conversation</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/llm_message.html">llm_message</a></span><span class="op">(</span><span class="st">"What's a famous landmark in this city?"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/openai.html">openai</a></span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="../reference/get_reply.html">get_reply</a></span><span class="op">(</span><span class="va">conversation</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] "A famous landmark in Paris is the Eiffel Tower."</span></span></code></pre>
</div>
<div class="section level4">
<h4 id="sending-images-to-models">Sending Images to Models<a class="anchor" aria-label="anchor" href="#sending-images-to-models"></a>
</h4>
<p><strong>tidyllm</strong> also supports sending images to multimodal
models. Let’s send this picture here:
<img src="picture.jpeg" width="70%"></p>
<p>Here we let ChatGPT guess where the picture was made:</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Describe an image using a llava model on ollama</span></span>
<span><span class="va">image_description</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/llm_message.html">llm_message</a></span><span class="op">(</span><span class="st">"Describe this picture? Can you guess where it was made?"</span>,</span>
<span>                                 .imagefile <span class="op">=</span> <span class="st">"picture.jpeg"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/openai.html">openai</a></span><span class="op">(</span>.model <span class="op">=</span> <span class="st">"gpt-4o"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Get the last reply</span></span>
<span><span class="fu"><a href="../reference/get_reply.html">get_reply</a></span><span class="op">(</span><span class="va">image_description</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] "The picture shows a beautiful landscape with a lake, mountains, and a town nestled below. The sun is shining brightly, casting a serene glow over the water. The area appears lush and green, with agricultural fields visible. \n\nThis type of scenery is reminiscent of northern Italy, particularly around Lake Garda, which features similar large mountains, picturesque water, and charming towns."</span></span></code></pre>
</div>
<div class="section level4">
<h4 id="adding-pdfs-to-messages">Adding PDFs to messages<a class="anchor" aria-label="anchor" href="#adding-pdfs-to-messages"></a>
</h4>
<p>The <code><a href="../reference/llm_message.html">llm_message()</a></code> function also supports extracting text
from PDFs and including it in the message. This allows you to easily
provide context from a PDF document when interacting with an AI
assistant.</p>
<p>To use this feature, you need to have the <code>pdftools</code>
package installed. If it is not already installed, you can install it
with:</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/install.packages.html" class="external-link">install.packages</a></span><span class="op">(</span><span class="st">"pdftools"</span><span class="op">)</span></span></code></pre></div>
<p>To include text from a PDF in your prompt, simply pass the file path
to the <code>.pdf</code> argument of the <code>chat</code> function:</p>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/llm_message.html">llm_message</a></span><span class="op">(</span><span class="st">"Please summarize the key points from the provided PDF document."</span>, </span>
<span>     .pdf <span class="op">=</span> <span class="st">"die_verwandlung.pdf"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>     <span class="fu"><a href="../reference/openai.html">openai</a></span><span class="op">(</span>.model <span class="op">=</span> <span class="st">"gpt-4o-mini"</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Message History:</span></span>
<span><span class="co">## system: You are a helpful assistant</span></span>
<span><span class="co">## --------------------------------------------------------------</span></span>
<span><span class="co">## user: Please summarize the key points from the provided PDF document.</span></span>
<span><span class="co">##  -&gt; Attached Media Files:  die_verwandlung.pdf </span></span>
<span><span class="co">## --------------------------------------------------------------</span></span>
<span><span class="co">## assistant: Here are the key points from the provided PDF document 'Die Verwandlung' by Franz Kafka:</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## 1. The story centers around Gregor Samsa, who wakes up one morning to find that he has been transformed into a giant insect-like creature.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## 2. Gregor's transformation causes distress and disruption for his family. They struggle to come to terms with the situation and how to deal with Gregor in his new state.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## 3. Gregor's family, especially his sister Grete, initially tries to care for him, but eventually decides they need to get rid of him. They lock him in his room and discuss finding a way to remove him.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## 4. Gregor becomes increasingly isolated and neglected by his family. He becomes weaker and less mobile due to his injuries and lack of proper care.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## 5. Eventually, Gregor dies, and his family is relieved. They then begin to make plans to move to a smaller, more affordable apartment and start looking for new jobs and opportunities.</span></span>
<span><span class="co">## --------------------------------------------------------------</span></span></code></pre>
<p>The package will automatically extract the text from the PDF and
include it in the prompt sent to the an API. The text will be wrapped in
<code>&lt;pdf&gt;</code> tags to clearly indicate the content from the
PDF:</p>
<pre><code>Please summarize the key points from the provided PDF document.

&lt;pdf filename="example_document.pdf"&gt;
Extracted text from the PDF file...
&lt;/pdf&gt;</code></pre>
</div>
<div class="section level4">
<h4 id="sending-r-outputs-to-language-models">Sending R Outputs to Language Models<a class="anchor" aria-label="anchor" href="#sending-r-outputs-to-language-models"></a>
</h4>
<p>You can automatically include R code outputs in your prompts.
<code><a href="../reference/llm_message.html">llm_message()</a></code> has an optional argument <code>.f</code> in
which you can specify a (anonymous) function, which will be run and
which console output will be captured and appended to the message when
you run it.</p>
<p>In addition you can use <code>.capture_plot</code> to send the last
plot pane to a model.</p>
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://tidyverse.tidyverse.org" class="external-link">tidyverse</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Create a plot for the mtcars example data</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html" class="external-link">ggplot</a></span><span class="op">(</span><span class="va">mtcars</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html" class="external-link">aes</a></span><span class="op">(</span><span class="va">wt</span>, <span class="va">mpg</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html" class="external-link">geom_point</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_smooth.html" class="external-link">geom_smooth</a></span><span class="op">(</span>method <span class="op">=</span> <span class="st">"lm"</span>, formula <span class="op">=</span> <span class="st">'y ~ x'</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html" class="external-link">labs</a></span><span class="op">(</span>x<span class="op">=</span><span class="st">"Weight"</span>,y<span class="op">=</span><span class="st">"Miles per gallon"</span><span class="op">)</span></span></code></pre></div>
<p><img src="tidyllm_files/figure-html/routputs_base-1.png" width="700"></p>
<p>Now we can send the plot and data summary to a language model:</p>
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://tidyverse.tidyverse.org" class="external-link">tidyverse</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="../reference/llm_message.html">llm_message</a></span><span class="op">(</span><span class="st">"Analyze this plot and data summary:"</span>, </span>
<span>                  .capture_plot <span class="op">=</span> <span class="cn">TRUE</span>, <span class="co">#Send the plot pane to a model</span></span>
<span>                  .f <span class="op">=</span> <span class="op">~</span><span class="op">{</span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">mtcars</span><span class="op">)</span><span class="op">}</span><span class="op">)</span> <span class="op">|&gt;</span> <span class="co">#Run summary(data) and send the output</span></span>
<span>  <span class="fu"><a href="../reference/claude.html">claude</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Message History:</span></span>
<span><span class="co">## system: You are a helpful assistant</span></span>
<span><span class="co">## --------------------------------------------------------------</span></span>
<span><span class="co">## user: Analyze this plot and data summary:</span></span>
<span><span class="co">##  -&gt; Attached Media Files:  file1568f6c1b4565.png, RConsole.txt </span></span>
<span><span class="co">## --------------------------------------------------------------</span></span>
<span><span class="co">## assistant: Based on the plot and data summary provided, here's an analysis:</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## 1. Relationship between Weight and MPG:</span></span>
<span><span class="co">##    The scatter plot shows a clear negative correlation between weight (wt) and miles per gallon (mpg). As the weight of the car increases, the fuel efficiency (mpg) decreases.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## 2. Linear Trend:</span></span>
<span><span class="co">##    The blue line in the plot represents a linear regression fit. The downward slope confirms the negative relationship between weight and mpg.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## 3. Data Distribution:</span></span>
<span><span class="co">##    - The weight of cars in the dataset ranges from 1.513 to 5.424 (likely in thousands of pounds).</span></span>
<span><span class="co">##    - The mpg values range from 10.40 to 33.90.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## 4. Variability:</span></span>
<span><span class="co">##    There's some scatter around the regression line, indicating that while weight is a strong predictor of mpg, other factors also influence fuel efficiency.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## 5. Other Variables:</span></span>
<span><span class="co">##    While not shown in the plot, the summary statistics provide information on other variables:</span></span>
<span><span class="co">##    - Cylinder count (cyl) ranges from 4 to 8, with a median of 6.</span></span>
<span><span class="co">##    - Horsepower (hp) ranges from 52 to 335, with a mean of 146.7.</span></span>
<span><span class="co">##    - Transmission type (am) is binary (0 or 1), likely indicating automatic vs. manual.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## 6. Model Fit:</span></span>
<span><span class="co">##    The grey shaded area around the regression line represents the confidence interval. It widens at the extremes of the weight range, indicating less certainty in predictions for very light or very heavy vehicles.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## 7. Outliers:</span></span>
<span><span class="co">##    There are a few potential outliers, particularly at the lower and higher ends of the weight spectrum, that deviate from the general trend.</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## In conclusion, this analysis strongly suggests that weight is a significant factor in determining a car's fuel efficiency, with heavier cars generally having lower mpg. However, the presence of scatter in the data indicates that other factors (possibly related to engine characteristics, transmission type, or aerodynamics) also play a role in determining fuel efficiency.</span></span>
<span><span class="co">## --------------------------------------------------------------</span></span></code></pre>
</div>
<div class="section level4">
<h4 id="getting-replies-from-the-api">Getting replies from the API<a class="anchor" aria-label="anchor" href="#getting-replies-from-the-api"></a>
</h4>
<p>Retrieve an assistant reply as text from a message history with
<code><a href="../reference/get_reply.html">get_reply()</a></code>. Specify an index to choose which assistant
message to get:</p>
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">conversation</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/llm_message.html">llm_message</a></span><span class="op">(</span><span class="st">"Imagine a German adress."</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>     <span class="fu"><a href="../reference/groq.html">groq</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>     <span class="fu"><a href="../reference/llm_message.html">llm_message</a></span><span class="op">(</span><span class="st">"Imagine another address"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>     <span class="fu"><a href="../reference/claude.html">claude</a></span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="va">conversation</span></span></code></pre></div>
<pre><code><span><span class="co">## Message History:</span></span>
<span><span class="co">## system: You are a helpful assistant</span></span>
<span><span class="co">## --------------------------------------------------------------</span></span>
<span><span class="co">## user: Imagine a German adress.</span></span>
<span><span class="co">## --------------------------------------------------------------</span></span>
<span><span class="co">## assistant: Let's imagine a German address: </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Herr Müller</span></span>
<span><span class="co">## Musterstraße 12</span></span>
<span><span class="co">## 53111 Bonn</span></span>
<span><span class="co">## --------------------------------------------------------------</span></span>
<span><span class="co">## user: Imagine another address</span></span>
<span><span class="co">## --------------------------------------------------------------</span></span>
<span><span class="co">## assistant: Let's imagine another German address:</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Frau Schmidt</span></span>
<span><span class="co">## Fichtenweg 78</span></span>
<span><span class="co">## 42103 Wuppertal</span></span>
<span><span class="co">## --------------------------------------------------------------</span></span></code></pre>
<p>By default <code><a href="../reference/get_reply.html">get_reply()</a></code> gets the last assistant message.
Alternatively you can also use <code><a href="../reference/last_reply.html">last_reply()</a></code> as a shortcut
for the latest response.</p>
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">#Getting the first </span></span>
<span><span class="va">conversation</span> <span class="op">|&gt;</span> <span class="fu"><a href="../reference/get_reply.html">get_reply</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] "Let's imagine a German address: \n\nHerr Müller\nMusterstraße 12\n53111 Bonn"</span></span></code></pre>
<div class="sourceCode" id="cb26"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">#By default it gets the last reply</span></span>
<span><span class="va">conversation</span> <span class="op">|&gt;</span> <span class="fu"><a href="../reference/get_reply.html">get_reply</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] "Let's imagine another German address:\n\nFrau Schmidt\nFichtenweg 78\n42103 Wuppertal"</span></span></code></pre>
<div class="sourceCode" id="cb28"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">#Or if you can more easily remember last_reply this works too</span></span>
<span><span class="va">conversation</span> <span class="op">|&gt;</span> <span class="fu"><a href="../reference/last_reply.html">last_reply</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] "Let's imagine another German address:\n\nFrau Schmidt\nFichtenweg 78\n42103 Wuppertal"</span></span></code></pre>
<p>In the current development version, you can use the
<code><a href="../reference/get_metadata.html">get_metadata()</a></code> function to retrieve metadata from assistant
replies:</p>
<div class="sourceCode" id="cb30"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">conversation</span> <span class="op">|&gt;</span> <span class="fu"><a href="../reference/get_metadata.html">get_metadata</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## <span style="color: #949494;"># A tibble: 2 × 5</span></span></span>
<span><span class="co">##   model  timestamp           prompt_tokens completion_tokens</span></span>
<span><span class="co">##   <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>  <span style="color: #949494; font-style: italic;">&lt;dttm&gt;</span>                      <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span>             <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span></span></span>
<span><span class="co">## <span style="color: #BCBCBC;">1</span> groq-… 2024-11-08 <span style="color: #949494;">14:25:43</span>            20                45</span></span>
<span><span class="co">## <span style="color: #BCBCBC;">2</span> claud… 2024-11-08 <span style="color: #949494;">14:26:02</span>            80                40</span></span>
<span><span class="co">## <span style="color: #949494;"># ℹ 1 more variable: total_tokens &lt;dbl&gt;</span></span></span></code></pre>
<p>By default it collects metadata for the whole message history, but
you can also set an <code>.index</code> to only get metadata for a
specific reply. Alternatively you can print out metadata with the
standard print method with the <code>.meta</code>-argument in
<code><a href="https://rdrr.io/r/base/print.html" class="external-link">print()</a></code> or via the <code>tidyllm_print_metadata</code>
option.</p>
</div>
<div class="section level4">
<h4 id="working-with-structured-model-outputs">Working with structured model outputs<a class="anchor" aria-label="anchor" href="#working-with-structured-model-outputs"></a>
</h4>
<p>To make model responses easy to interpret and integrate into your
workflow, <strong>tidyllm</strong> supports defining schemas to ensure
that models reply with structured outputs in <a href="https://de.wikipedia.org/wiki/JavaScript_Object_Notation" class="external-link">JSON
(JavaScript Object Notation)</a> following your specifications. JSON is
a standard format for organizing data in simple key-value pairs, which
is both human-readable and machine-friendly.</p>
<p>Currently, <code><a href="../reference/openai.html">openai()</a></code> is the only API function in
<strong>tidyllm</strong> that supports schema enforcement through the
<code>.json_schema</code> argument. This ensures that replies conform to
a pre-defined consistent data formatting.</p>
<p>To create schemas, you can use the <code><a href="../reference/tidyllm_schema.html">tidyllm_schema()</a></code>
function, which translates your data format specifications into the <a href="https://json-schema.org/" class="external-link">JSON-schema format</a> the API requires.
This helper function standardizes the data layout by ensuring flat
(non-nested) JSON structures with defined data types. Here’s how to
define a schema:</p>
<ul>
<li>
<strong><code>name</code></strong>: A name identifier for the schema
(which is needed by the API).</li>
<li>
<strong><code>...</code> (fields)</strong>: Named arguments for
field names and their data types, including:
<ul>
<li>
<code>"character"</code> or <code>"string"</code>: Text fields.</li>
<li>
<code>"factor(...)"</code>: Enumerations with allowable values, like
<code>factor(Germany, France)</code>.</li>
<li>
<code>"logical"</code>: <code>TRUE</code> or <code>FALSE</code>
</li>
<li>
<code>"numeric"</code>: Numeric fields.</li>
<li>
<code>"type[]"</code>: Lists of a given type, such as
<code>"character[]"</code>.</li>
</ul>
</li>
</ul>
<p>Here’s an example schema defining an address format:</p>
<div class="sourceCode" id="cb32"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">address_schema</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/tidyllm_schema.html">tidyllm_schema</a></span><span class="op">(</span></span>
<span>  name <span class="op">=</span> <span class="st">"AddressSchema"</span>,</span>
<span>  street <span class="op">=</span> <span class="st">"character"</span>,</span>
<span>  houseNumber <span class="op">=</span> <span class="st">"numeric"</span>,</span>
<span>  postcode <span class="op">=</span> <span class="st">"character"</span>,</span>
<span>  city <span class="op">=</span> <span class="st">"character"</span>,</span>
<span>  region <span class="op">=</span> <span class="st">"character"</span>,</span>
<span>  country <span class="op">=</span> <span class="st">"factor(Germany,France)"</span></span>
<span><span class="op">)</span></span>
<span><span class="va">address</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/llm_message.html">llm_message</a></span><span class="op">(</span><span class="st">"Imagine an address in JSON format that matches the schema."</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>        <span class="fu"><a href="../reference/openai.html">openai</a></span><span class="op">(</span>.json_schema <span class="op">=</span> <span class="va">address_schema</span><span class="op">)</span></span>
<span><span class="va">address</span></span></code></pre></div>
<pre><code><span><span class="co">## Message History:</span></span>
<span><span class="co">## system: You are a helpful assistant</span></span>
<span><span class="co">## --------------------------------------------------------------</span></span>
<span><span class="co">## user: Imagine an address in JSON format that matches the schema.</span></span>
<span><span class="co">## --------------------------------------------------------------</span></span>
<span><span class="co">## assistant: {"street":"Hauptstraße","houseNumber":123,"postcode":"10115","city":"Berlin","region":"Berlin","country":"Germany"}</span></span>
<span><span class="co">## --------------------------------------------------------------</span></span></code></pre>
<p>The model responded in JSON format, organizing data into key-value
pairs like specified. You can then convert this JSON output into an R
list for easier handling with <code><a href="../reference/get_reply_data.html">get_reply_data()</a></code>:</p>
<div class="sourceCode" id="cb34"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">address</span> <span class="op">|&gt;</span> <span class="fu"><a href="../reference/get_reply_data.html">get_reply_data</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://rdrr.io/r/utils/str.html" class="external-link">str</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## List of 6</span></span>
<span><span class="co">##  $ street     : chr "Hauptstraße"</span></span>
<span><span class="co">##  $ houseNumber: int 123</span></span>
<span><span class="co">##  $ postcode   : chr "10115"</span></span>
<span><span class="co">##  $ city       : chr "Berlin"</span></span>
<span><span class="co">##  $ region     : chr "Berlin"</span></span>
<span><span class="co">##  $ country    : chr "Germany"</span></span></code></pre>
<p>Other API functions like <code><a href="../reference/ollama.html">ollama()</a></code>, <code><a href="../reference/groq.html">groq()</a></code>,
and <code><a href="../reference/mistral.html">mistral()</a></code> also support structured outputs through a
simpler JSON mode, accessible with the <code>.json</code> argument.
Since these APIs do not currently support native schema enforcement,
you’ll need to prompt the model to follow a specified format directly in
your messages. Although <code><a href="../reference/get_reply_data.html">get_reply_data()</a></code> can help extract
structured data from these responses when you set
<code>.json=TRUE</code> in each of the API functions, the model may not
always adhere strictly to the specified structure. Once these APIs
support native schema enforcement, <strong>tidyllm</strong> will
integrate full schema functionality for them.</p>
</div>
<div class="section level4">
<h4 id="api-parameters">API parameters<a class="anchor" aria-label="anchor" href="#api-parameters"></a>
</h4>
<p>Different API functions support different model parameters like, how
deterministic the response should be via parameters like temperature.
Please read API-documentation and the documentation of the model
functions for specific examples.</p>
<div class="sourceCode" id="cb36"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span>  <span class="va">temp_example</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/llm_message.html">llm_message</a></span><span class="op">(</span><span class="st">"Explain how temperature parameters work in large language models  and why temperature 0 gives you deterministic outputs in one sentence."</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="co">#per default it is non-zero</span></span>
<span>  <span class="va">temp_example</span> <span class="op">|&gt;</span> <span class="fu"><a href="../reference/ollama.html">ollama</a></span><span class="op">(</span>.temperature<span class="op">=</span><span class="fl">0</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Message History:</span></span>
<span><span class="co">## system: You are a helpful assistant</span></span>
<span><span class="co">## --------------------------------------------------------------</span></span>
<span><span class="co">## user: Explain how temperature parameters work in large language models  and why temperature 0 gives you deterministic outputs in one sentence.</span></span>
<span><span class="co">## --------------------------------------------------------------</span></span>
<span><span class="co">## assistant: In large language models, temperature parameters control the randomness of generated text by scaling the output probabilities, with higher temperatures introducing more uncertainty and lower temperatures favoring more likely outcomes; specifically, setting temperature to 0 effectively eliminates all randomness, resulting in deterministic outputs because it sets the probability of each token to its maximum likelihood value.</span></span>
<span><span class="co">## --------------------------------------------------------------</span></span></code></pre>
<div class="sourceCode" id="cb38"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span>  <span class="co">#Retrying with .temperature=0</span></span>
<span>  <span class="va">temp_example</span> <span class="op">|&gt;</span> <span class="fu"><a href="../reference/ollama.html">ollama</a></span><span class="op">(</span>.temperature<span class="op">=</span><span class="fl">0</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Message History:</span></span>
<span><span class="co">## system: You are a helpful assistant</span></span>
<span><span class="co">## --------------------------------------------------------------</span></span>
<span><span class="co">## user: Explain how temperature parameters work in large language models  and why temperature 0 gives you deterministic outputs in one sentence.</span></span>
<span><span class="co">## --------------------------------------------------------------</span></span>
<span><span class="co">## assistant: In large language models, temperature parameters control the randomness of generated text by scaling the output probabilities, with higher temperatures introducing more uncertainty and lower temperatures favoring more likely outcomes; specifically, setting temperature to 0 effectively eliminates all randomness, resulting in deterministic outputs because it sets the probability of each token to its maximum likelihood value.</span></span>
<span><span class="co">## --------------------------------------------------------------</span></span></code></pre>
</div>
<div class="section level4">
<h4 id="embeddings">Embeddings<a class="anchor" aria-label="anchor" href="#embeddings"></a>
</h4>
<p><a href="https://cohere.com/llmu/text-embeddings" class="external-link">Embedding
models</a> in <strong>tidyllm</strong> transform textual inputs into
vector representations, capturing semantic information that can enhance
similarity comparisons, clustering, and retrieval tasks. You can
generate embeddings using functions like
<code><a href="../reference/openai_embedding.html">openai_embedding()</a></code>, <code><a href="../reference/mistral_embedding.html">mistral_embedding()</a></code>, and
<code><a href="../reference/ollama_embedding.html">ollama_embedding()</a></code> which each interface with their
respective APIs. These functions create vector representations for
texts. These functions return vector representations either for each
message in a message history or, more typically for this application,
for each entry in a character vector.</p>
</div>
<div class="section level4">
<h4 id="batch-requests">Batch requests<a class="anchor" aria-label="anchor" href="#batch-requests"></a>
</h4>
<p>Anthropic and OpenAI, offer batch request options that are around 50%
cheaper than standard single-interaction APIs. Batch processing allows
you to submit multiple message histories at once, which are then
processed together on the model providers servers, usually within a
24-hour period. In <strong>tidyllm</strong>, you can use the
<code><a href="../reference/send_claude_batch.html">send_claude_batch()</a></code> or <code><a href="../reference/send_openai_batch.html">send_openai_batch()</a></code>
functions to submit these batch requests.</p>
<p>Here’s an example of how to send a batch request to Claude’s batch
API:</p>
<div class="sourceCode" id="cb40"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">#Create a message batch and save it to disk to fetch it later</span></span>
<span><span class="fu"><a href="https://glue.tidyverse.org/reference/glue.html" class="external-link">glue</a></span><span class="op">(</span><span class="st">"Write a poem about {x}"</span>, x<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"cats"</span>,<span class="st">"dogs"</span>,<span class="st">"hamsters"</span><span class="op">)</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">purrr</span><span class="fu">::</span><span class="fu"><a href="https://purrr.tidyverse.org/reference/map.html" class="external-link">map</a></span><span class="op">(</span><span class="va">llm_message</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/send_claude_batch.html">send_claude_batch</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/readRDS.html" class="external-link">saveRDS</a></span><span class="op">(</span><span class="st">"claude_batch.rds"</span><span class="op">)</span></span></code></pre></div>
<p>The <code><a href="../reference/send_claude_batch.html">send_claude_batch()</a></code> function returns the same list
of message histories that was input, but marked with an attribute that
contains a batch-id from the Claude API as well as unique names for each
list element that can be used to stitch together messages with replies,
once they are ready. If you provide a named list of messages, tidyllm
will use these names as identifiers in the batch, if these names are
unique.</p>
<p><strong>Tip:</strong> Saving batch requests to a file allows you to
persist them across R sessions, making it easier to manage large jobs
and access results later.</p>
<div class="sourceCode" id="cb41"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">#Create a message batch and save it to disk to fetch it later</span></span>
<span><span class="fu"><a href="https://glue.tidyverse.org/reference/glue.html" class="external-link">glue</a></span><span class="op">(</span><span class="st">"Write a poem about {x}"</span>, x<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"cats"</span>,<span class="st">"dogs"</span>,<span class="st">"hamsters"</span><span class="op">)</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu">purrr</span><span class="fu">::</span><span class="fu"><a href="https://purrr.tidyverse.org/reference/map.html" class="external-link">map</a></span><span class="op">(</span><span class="va">llm_message</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/send_claude_batch.html">send_claude_batch</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/readRDS.html" class="external-link">saveRDS</a></span><span class="op">(</span><span class="st">"claude_batch.rds"</span><span class="op">)</span></span></code></pre></div>
<p>After sending a batch request, you can check its status with
<code><a href="../reference/check_openai_batch.html">check_openai_batch()</a></code> or <code><a href="../reference/check_claude_batch.html">check_claude_batch()</a></code> .
For example:</p>
<div class="sourceCode" id="cb42"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">#Check the status of the batch</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/readRDS.html" class="external-link">readRDS</a></span><span class="op">(</span><span class="st">"claude_batch.rds"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>   <span class="fu"><a href="../reference/check_claude_batch.html">check_claude_batch</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## <span style="color: #949494;"># A tibble: 1 × 8</span></span></span>
<span><span class="co">##   batch_id          status created_at          expires_at          req_succeeded</span></span>
<span><span class="co">##   <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>             <span style="color: #949494; font-style: italic;">&lt;chr&gt;</span>  <span style="color: #949494; font-style: italic;">&lt;dttm&gt;</span>              <span style="color: #949494; font-style: italic;">&lt;dttm&gt;</span>                      <span style="color: #949494; font-style: italic;">&lt;dbl&gt;</span></span></span>
<span><span class="co">## <span style="color: #BCBCBC;">1</span> msgbatch_02A1B2C… ended  2024-11-01 <span style="color: #949494;">10:30:00</span> 2024-11-02 <span style="color: #949494;">10:30:00</span>             3</span></span>
<span><span class="co">## <span style="color: #949494;"># ℹ 3 more variables: req_errored &lt;dbl&gt;, req_expired &lt;dbl&gt;, req_canceled &lt;dbl&gt;</span></span></span></code></pre>
<p>The status output shows details such as the number of successful,
errored, expired, and canceled requests in the batch, as well as the
current status. You can also see all your batch requests with
<code><a href="../reference/list_claude_batches.html">list_claude_batches()</a></code> or in the batches dashboard of the
Anthropic console. Once the processing of a batch is completed you can
fetch its results with <code><a href="../reference/fetch_claude_batch.html">fetch_claude_batch()</a></code> or
<code><a href="../reference/fetch_openai_batch.html">fetch_openai_batch()</a></code>:</p>
<div class="sourceCode" id="cb44"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">conversations</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/readRDS.html" class="external-link">readRDS</a></span><span class="op">(</span><span class="st">"claude_batch.rds"</span><span class="op">)</span> <span class="op">|&gt;</span></span>
<span>  <span class="fu"><a href="../reference/fetch_claude_batch.html">fetch_claude_batch</a></span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="va">poems</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://purrr.tidyverse.org/reference/map.html" class="external-link">map_chr</a></span><span class="op">(</span><span class="va">conversations</span>, <span class="va">get_reply</span><span class="op">)</span></span></code></pre></div>
<p>The output is a list of message histories, each now updated with new
assistant replies. You can further process these responses with
<strong>tidyllm’s</strong> standard tools.Before launching a large batch
operation, it’s good practice to run a few test requests and review
outputs with the standard API fuctions. This approach helps confirm that
prompt settings and model configurations produce the desired responses,
minimizing potential errors or resource waste.</p>
</div>
<div class="section level4">
<h4 id="streaming-back-responses-experimental">Streaming back responses (Experimental)<a class="anchor" aria-label="anchor" href="#streaming-back-responses-experimental"></a>
</h4>
<p>All standard chat API-functions support real-time streaming of reply
tokens to the console while the model works with the
<code>.stream=TRUE</code> argument. While this feature offers slightly
better feedback on model behavior in real-time, it’s not particularly
useful for data-analysis workflows. We consider this feature
experimental and recommend using non-streaming responses for production
tasks. Note that error handling in streaming callbacks varies by API and
differs in quality at this time.</p>
</div>
<div class="section level4">
<h4 id="choosing-the-right-model-and-api">Choosing the Right Model and API<a class="anchor" aria-label="anchor" href="#choosing-the-right-model-and-api"></a>
</h4>
<p>tidyllm supports multiple APIs, each offering distinct large language
models with varying strengths. The choice of which model or API to use
often depends on the specific task, cost considerations, and data
privacy concerns.</p>
<ol style="list-style-type: decimal">
<li><p><strong><code><a href="../reference/claude.html">claude()</a></code> - Anthropic API:</strong> <a href="https://docs.anthropic.com/en/docs/welcome" class="external-link">Claude</a> is known
for generating thoughtful, nuanced responses, making it ideal for tasks
that require more human-like reasoning, such as summarization or
creative writing. Claude Sonnet 3.5 currently is one of the
top-performing models on many benchmarks. However, it can sometimes be
more verbose than necessary, and it lacks direct JSON support, which
requires additional prompting and validation to ensure structured
output.</p></li>
<li><p><strong><code><a href="../reference/openai.html">openai()</a></code> (OpenAI API):</strong> Models by <a href="https://platform.openai.com/docs/api-reference/chat" class="external-link">OpenAI
API</a>, particularly the GPT-4o model, are extremely versatile and
perform well across a wide range of tasks, including text generation,
code completion, and multimodal analysis. In addition the o1-reasoning
models offer very good performance for a set of specific task (at a
relatively high price). There is also an <code><a href="../reference/azure_openai.html">azure_openai()</a></code>
function if you prefer to use the OpenAI API on Microsoft
Azure.</p></li>
<li><p><strong><code><a href="../reference/mistral.html">mistral()</a></code> (EU-based):</strong> <a href="https://docs.mistral.ai/" class="external-link">Mistral</a> offers lighter-weight,
open-source models developed and hosted in the EU, making it
particularly appealing if data protection (e.g., GDPR compliance) is a
concern. While the models may not be as powerful as GPT-4o or Claude
Sonnet, Mistral offers good performance for standard text generation
tasks.</p></li>
<li><p><strong><code><a href="../reference/groq.html">groq()</a></code> (Fast):</strong> <a href="https://console.groq.com/docs/quickstart" class="external-link">Groq</a> offers a unique
advantage with its custom AI accelerator hardware, that get you the
fastest output available on any API. It delivers high performance at low
costs, especially for tasks that require fast execution. It hosts many
strong open-source models, like <strong>lamma3:70b</strong>. There is
also a <code><a href="../reference/groq_transcribe.html">groq_transcribe()</a></code> function available that allows you
to transcribe audio files with the Whipser-Large model on the Groq
API.</p></li>
<li><p><strong><code><a href="../reference/ollama.html">ollama()</a></code> (Local Models):</strong> If data
privacy is a priority, running open-source models like
<strong>gemma2::9B</strong> locally via <a href="https://ollama.com/" class="external-link">ollama</a> gives you full control over model
execution and data. However, the trade-off is that local models require
significant computational resources, and are often not quite as powerful
as the large API-providers. The <a href="https://ollama.com/blog" class="external-link">ollama
blog</a> regularly has posts about new models and their advantages that
you can download via <code><a href="../reference/ollama_download_model.html">ollama_download_model()</a></code>.</p></li>
<li><p><strong>Other OpenAI-compatible Local Models:</strong> Besides
ollama, there are many solutions to run local models that are mostly
compatible to the OpenAI API like <a href="https://github.com/ggerganov/llama.cpp/blob/master/examples/server/README.md" class="external-link">llama.cpp</a>,
<a href="https://github.com/vllm-project/vllm" class="external-link">vllm</a> and <a href="https://www.reddit.com/r/LocalLLaMA/comments/16csz5n/best_openai_api_compatible_application_server/" class="external-link">many
more</a>. To use such an API you can set the base url of the api with
<code>.api_url</code> as well as the path to the model-endpoint with
<code>.api_path</code> argument in the <code><a href="../reference/openai.html">openai()</a></code> function.
Set <code>.compatible=TRUE</code> to skip api-key checks and rate-limit
tracking. Compatibility with local models solutions may vary depending
on the specific API’s implementation, and full functionality cannot be
guaranteed.</p></li>
</ol>
</div>
</div>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Eduard Brüll.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.1.</p>
</div>

    </footer>
</div>





  </body>
</html>
