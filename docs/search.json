[{"path":"https://edubruell.github.io/tidyllm/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2024 Eduard Brüll Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm.html","id":"introduction-to-tidyllm","dir":"Articles","previous_headings":"","what":"Introduction to tidyllm","title":"Get Started","text":"tidyllm R package designed provide unified interface interacting various large language model APIs. vignette guide basic setup usage tidyllm.","code":""},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm.html","id":"installation","dir":"Articles","previous_headings":"Introduction to tidyllm","what":"Installation","title":"Get Started","text":"install tidyllm CRAN , use: install current development version directly GitHub using devtools:","code":"install.packages(\"tidyllm\") # Install devtools if not already installed if (!requireNamespace(\"devtools\", quietly = TRUE)) {   install.packages(\"devtools\") }  # Install TidyLLM from GitHub devtools::install_github(\"edubruell/tidyllm\")"},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm.html","id":"setting-up-api-keys-or-ollama","dir":"Articles","previous_headings":"Introduction to tidyllm > Installation","what":"Setting up API Keys or ollama","title":"Get Started","text":"using tidyllm, need set API keys services plan use. ’s set different providers: Anthropic (Claude): OpenAI (ChatGPT): Mistral can set API key Mistral console page set Groq: Alternatively, can set keys .Renviron file persistent storage. , execute usethis::edit_r_environ(), add line API key file, example: want work local large lange models via ollama need install official project website. Ollama sets local large language model server can use run open-source models devices.","code":"Sys.setenv(ANTHROPIC_API_KEY = \"YOUR-ANTHROPIC-API-KEY\") Sys.setenv(OPENAI_API_KEY = \"YOUR-OPENAI-API-KEY\") Sys.setenv(MISTRAL_API_KEY = \"MISTRAL-API-KEY-GOES-HERE\") Sys.setenv(GROQ_API_KEY = \"YOUR-GROQ-API-KEY\") ANTHROPIC_API_KEY=\"YOUR-ANTHROPIC-API-KEY\""},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm.html","id":"basic-usage","dir":"Articles","previous_headings":"Introduction to tidyllm > Installation","what":"Basic Usage","title":"Get Started","text":"Let’s start simple example using tidyllm interact different language models:","code":"library(tidyllm)  # Start a conversation with Claude conversation <- llm_message(\"What is the capital of France?\") |>   claude()  #Standard way that llm_messages are printed conversation ## Message History: ## system: You are a helpful assistant ## -------------------------------------------------------------- ## user: What is the capital of France? ## -------------------------------------------------------------- ## assistant: The capital of France is Paris. ## -------------------------------------------------------------- # Continue the conversation with ChatGPT conversation <- conversation |>   llm_message(\"What's a famous landmark in this city?\") |>   chatgpt()  last_reply(conversation) ## [1] \"A famous landmark in Paris is the Eiffel Tower.\""},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm.html","id":"sending-images-to-models","dir":"Articles","previous_headings":"Introduction to tidyllm > Installation","what":"Sending Images to Models","title":"Get Started","text":"tidyllm also supports sending images multimodal models. Let’s send picture : let ChatGPT guess picture made:","code":"# Describe an image using a llava model on ollama image_description <- llm_message(\"Describe this picture? Can you guess where it was made?\",                                  .imagefile = \"picture.jpeg\") |>   chatgpt(.model = \"gpt-4o\")  # Get the last reply last_reply(image_description) ## [1] \"The picture shows a beautiful landscape with a lake, mountains, and a town nestled below. The sun is shining brightly, casting a serene glow over the water. The area appears lush and green, with agricultural fields visible. \\n\\nThis type of scenery is reminiscent of northern Italy, particularly around Lake Garda, which features similar large mountains, picturesque water, and charming towns.\""},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm.html","id":"adding-pdfs-to-messages","dir":"Articles","previous_headings":"Introduction to tidyllm > Installation","what":"Adding PDFs to messages","title":"Get Started","text":"llm_message() function also supports extracting text PDFs including message. allows easily provide context PDF document interacting AI assistant. use feature, need pdftools package installed. already installed, can install : include text PDF prompt, simply pass file path .pdf argument chat function: package automatically extract text PDF include prompt sent API. text wrapped <pdf> tags clearly indicate content PDF:","code":"install.packages(\"pdftools\") llm_message(\"Please summarize the key points from the provided PDF document.\",       .pdf = \"die_verwandlung.pdf\") |>      chatgpt(.model = \"gpt-4o-mini\") ## Message History: ## system: You are a helpful assistant ## -------------------------------------------------------------- ## user: Please summarize the key points from the provided PDF document. ##  -> Attached Media Files:  die_verwandlung.pdf  ## -------------------------------------------------------------- ## assistant: Here are the key points from the provided PDF document 'Die Verwandlung' by Franz Kafka: ##  ## 1. The story centers around Gregor Samsa, who wakes up one morning to find that he has been transformed into a giant insect-like creature. ##  ## 2. Gregor's transformation causes distress and disruption for his family. They struggle to come to terms with the situation and how to deal with Gregor in his new state. ##  ## 3. Gregor's family, especially his sister Grete, initially tries to care for him, but eventually decides they need to get rid of him. They lock him in his room and discuss finding a way to remove him. ##  ## 4. Gregor becomes increasingly isolated and neglected by his family. He becomes weaker and less mobile due to his injuries and lack of proper care. ##  ## 5. Eventually, Gregor dies, and his family is relieved. They then begin to make plans to move to a smaller, more affordable apartment and start looking for new jobs and opportunities. ## -------------------------------------------------------------- Please summarize the key points from the provided PDF document.  <pdf filename=\"example_document.pdf\"> Extracted text from the PDF file... <\/pdf>"},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm.html","id":"sending-r-outputs-to-language-models","dir":"Articles","previous_headings":"Introduction to tidyllm > Installation","what":"Sending R Outputs to Language Models","title":"Get Started","text":"can automatically include R code outputs prompts. llm_message() optional argument .f can specify (anonymous) function, run console output captured appended message run . addition can use .caputre_plot send last plot pane model.  Now can send plot data summary language model:","code":"library(tidyverse) ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr     1.1.4     ✔ readr     2.1.5 ## ✔ forcats   1.0.0     ✔ stringr   1.5.1 ## ✔ ggplot2   3.5.1     ✔ tibble    3.2.1 ## ✔ lubridate 1.9.3     ✔ tidyr     1.3.1 ## ✔ purrr     1.0.2      ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag()    masks stats::lag() ## ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors # Create a plot for the mtcars example data ggplot(mtcars, aes(wt, mpg)) +   geom_point() +   geom_smooth(method = \"lm\", formula = 'y ~ x') +   labs(x=\"Weight\",y=\"Miles per gallon\") library(tidyverse) llm_message(\"Analyze this plot and data summary:\",                    .capture_plot = TRUE, #Send the plot pane to a model                   .f = ~{summary(mtcars)}) |> #Run summary(data) and send the output   claude() ## Message History: ## system: You are a helpful assistant ## -------------------------------------------------------------- ## user: Analyze this plot and data summary: ##  -> Attached Media Files:  file1568f6c1b4565.png, RConsole.txt  ## -------------------------------------------------------------- ## assistant: Based on the plot and data summary provided, here's an analysis: ##  ## 1. Relationship between Weight and MPG: ##    The scatter plot shows a clear negative correlation between weight (wt) and miles per gallon (mpg). As the weight of the car increases, the fuel efficiency (mpg) decreases. ##  ## 2. Linear Trend: ##    The blue line in the plot represents a linear regression fit. The downward slope confirms the negative relationship between weight and mpg. ##  ## 3. Data Distribution: ##    - The weight of cars in the dataset ranges from 1.513 to 5.424 (likely in thousands of pounds). ##    - The mpg values range from 10.40 to 33.90. ##  ## 4. Variability: ##    There's some scatter around the regression line, indicating that while weight is a strong predictor of mpg, other factors also influence fuel efficiency. ##  ## 5. Other Variables: ##    While not shown in the plot, the summary statistics provide information on other variables: ##    - Cylinder count (cyl) ranges from 4 to 8, with a median of 6. ##    - Horsepower (hp) ranges from 52 to 335, with a mean of 146.7. ##    - Transmission type (am) is binary (0 or 1), likely indicating automatic vs. manual. ##  ## 6. Model Fit: ##    The grey shaded area around the regression line represents the confidence interval. It widens at the extremes of the weight range, indicating less certainty in predictions for very light or very heavy vehicles. ##  ## 7. Outliers: ##    There are a few potential outliers, particularly at the lower and higher ends of the weight spectrum, that deviate from the general trend. ##  ## In conclusion, this analysis strongly suggests that weight is a significant factor in determining a car's fuel efficiency, with heavier cars generally having lower mpg. However, the presence of scatter in the data indicates that other factors (possibly related to engine characteristics, transmission type, or aerodynamics) also play a role in determining fuel efficiency. ## --------------------------------------------------------------"},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm.html","id":"getting-the-last-reply-as-raw-text-or-structured-data","dir":"Articles","previous_headings":"Introduction to tidyllm > Installation","what":"Getting the last reply (as raw text or structured data)","title":"Get Started","text":"can retrieve last assistant reply message history last_reply(). Typically, returns character vector text assistant’s reply. However, API functions requested replies JSON mode, can directly validate return structured output. function handles different response types automatically. JSON reply detected, returns list following fields: parsed_content: parsed JSON content (NULL case parsing errors). raw_response: direct string format reply. is_parsed: flag set TRUE JSON parsing successful, FALSE otherwise. can also force standard raw text replies, even JSON mode detected, using .raw argument.","code":""},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm.html","id":"example-1-getting-standard-text-replies","dir":"Articles","previous_headings":"Introduction to tidyllm > Installation > Getting the last reply (as raw text or structured data)","what":"Example 1: Getting standard text replies","title":"Get Started","text":"","code":"reply_text <- llm_message(\"Imagine a German adress.\") |>      groq() |>      last_reply() ## Message History: ## system: You are a helpful assistant ## -------------------------------------------------------------- ## user: Imagine a German adress. ## -------------------------------------------------------------- ## assistant: Let's imagine a German address.  ##  ## Herr Müller ## Musterstraße 12 ## 53111 Bonn ##  ## This address is formatted according to German conventions: ##  ## - 'Herr Müller' is the recipient's name (Mr. Müller). ## - 'Musterstraße 12' is the street name and number. ## - '53111 Bonn' is the postal code and city (Bonn). ## --------------------------------------------------------------"},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm.html","id":"example-2-getting-structured-replies-from-apis-in-json-mode","dir":"Articles","previous_headings":"Introduction to tidyllm > Installation > Getting the last reply (as raw text or structured data)","what":"Example 2: Getting structured replies from APIs in JSON mode","title":"Get Started","text":"API functions .json-argument enables JSON-mode. Note claude() explicit JSON-mode API-request need specify want JSON-output ideally shema prompt assistant.","code":"address <- llm_message('Imagine a German adress in JSON format. Reply only with JSON.')   address|>   ollama(.json = TRUE) |>  # API is asked to return JSON   last_reply()  str(address) ## List of 3 ##  $ raw_response  : chr \"{\\n   \\\"street\\\": \\\"Kurfürstenstraße\\\",\\n   \\\"houseNumber\\\": 23,\\n   \\\"postcode\\\": \\\"10785\\\",\\n   \\\"city\\\": \\\"B\"| __truncated__ ##  $ parsed_content:List of 6 ##   ..$ street     : chr \"Kurfürstenstraße\" ##   ..$ houseNumber: int 23 ##   ..$ postcode   : chr \"10785\" ##   ..$ city       : chr \"Berlin\" ##   ..$ region     : chr \"Berlin\" ##   ..$ country    : chr \"Deutschland\" ##  $ is_parsed     : logi TRUE"},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm.html","id":"api-parameters","dir":"Articles","previous_headings":"Introduction to tidyllm > Installation","what":"API parameters","title":"Get Started","text":"Different API functions support different model parameters like deterministic response via parameters like temperature. Please read API-documentation documentation model functions specific examples.","code":"temp_example <- llm_message(\"Explain how temperature parameters work in large language models  and why temperature 0 gives you deterministic outputs in one sentence.\")      #per default it is non-zero   temp_example |> ollama(.temperature=0) ## Message History: ## system: You are a helpful assistant ## -------------------------------------------------------------- ## user: Explain how temperature parameters work in large language models  and why temperature 0 gives you deterministic outputs in one sentence. ## -------------------------------------------------------------- ## assistant: In large language models, temperature parameters control the randomness of generated text by scaling the output probabilities, with higher temperatures introducing more uncertainty and lower temperatures favoring more likely outcomes; specifically, setting temperature to 0 effectively eliminates all randomness, resulting in deterministic outputs because it sets the probability of each token to its maximum likelihood value. ## -------------------------------------------------------------- #Retrying with .temperature=0   temp_example |> ollama(.temperature=0) ## Message History: ## system: You are a helpful assistant ## -------------------------------------------------------------- ## user: Explain how temperature parameters work in large language models  and why temperature 0 gives you deterministic outputs in one sentence. ## -------------------------------------------------------------- ## assistant: In large language models, temperature parameters control the randomness of generated text by scaling the output probabilities, with higher temperatures introducing more uncertainty and lower temperatures favoring more likely outcomes; specifically, setting temperature to 0 effectively eliminates all randomness, resulting in deterministic outputs because it sets the probability of each token to its maximum likelihood value. ## --------------------------------------------------------------"},{"path":"https://edubruell.github.io/tidyllm/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Eduard Brüll. Author, maintainer.","code":""},{"path":"https://edubruell.github.io/tidyllm/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Brüll E (2024). tidyllm: Tidy Integration Large Language Models. https://github.com/edubruell/tidyllm, https://edubruell.github.io/tidyllm/.","code":"@Manual{,   title = {tidyllm: Tidy Integration of Large Language Models},   author = {Eduard Brüll},   year = {2024},   note = {https://github.com/edubruell/tidyllm, https://edubruell.github.io/tidyllm/}, }"},{"path":"https://edubruell.github.io/tidyllm/index.html","id":"tidyllm-","dir":"","previous_headings":"","what":"Tidy Integration of Large Language Models","title":"Tidy Integration of Large Language Models","text":"tidyllm R package designed access various large language model APIs, including Claude, ChatGPT, Groq, Mistral, local models via Ollama. Built simplicity functionality, helps generate text, analyze media, integrate model feedback data workflows ease.","code":""},{"path":"https://edubruell.github.io/tidyllm/index.html","id":"features","dir":"","previous_headings":"","what":"Features","title":"Tidy Integration of Large Language Models","text":"Multiple Model Support: Seamlessly switch various model providers like Claude, ChatGPT, Groq, Mistral Ollama using best offer. Media Handling: Extract process text PDFs capture console outputs messaging. Upload imagefiles last plotpane multimodal models. Interactive Messaging History: Manage ongoing conversation models, maintaining structured history messages media interactions, automatically formatted API Stateful handling rate limits: API rate limits handled statefully within R Session API functions can wait automatically rate limits reset Tidy Workflow: Use R’s functional programming features side-effect-free, pipeline-oriented operation style.","code":""},{"path":"https://edubruell.github.io/tidyllm/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Tidy Integration of Large Language Models","text":"install tidyllm CRAN, use: development version GitHub:","code":"install.packages(\"tidyllm\") # Install devtools if not already installed if (!requireNamespace(\"devtools\", quietly = TRUE)) {   install.packages(\"devtools\") } devtools::install_github(\"edubruell/tidyllm\")"},{"path":"https://edubruell.github.io/tidyllm/index.html","id":"basic-example","dir":"","previous_headings":"","what":"Basic Example","title":"Tidy Integration of Large Language Models","text":"’s quick example using tidyllm describe image using Claude model follow local open-source models: examples advanced usage, check Get Started vignette. Please note: use tidyllm, need either installation ollama active API key one supported providers (e.g., Claude, ChatGPT). See Get Started vignette setup instructions.","code":"library(\"tidyllm\")  # Describe an image with  claude conversation <- llm_message(\"Describe this image\",                                .imagefile = here(\"image.png\")) |>   claude()  # Use the description to query further with groq conversation |>   llm_message(\"Based on the previous description,   what could the research in the figure be about?\") |>   ollama(.model = \"gemma2\")"},{"path":"https://edubruell.github.io/tidyllm/index.html","id":"learn-more","dir":"","previous_headings":"","what":"Learn More","title":"Tidy Integration of Large Language Models","text":"detailed instructions advanced features, see: Get Started tidyllm Changelog Documentation","code":""},{"path":"https://edubruell.github.io/tidyllm/index.html","id":"contributing","dir":"","previous_headings":"","what":"Contributing","title":"Tidy Integration of Large Language Models","text":"welcome contributions! Feel free open issues submit pull requests GitHub.","code":""},{"path":"https://edubruell.github.io/tidyllm/index.html","id":"license","dir":"","previous_headings":"","what":"License","title":"Tidy Integration of Large Language Models","text":"project licensed MIT License - see LICENSE file details.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":null,"dir":"Reference","previous_headings":"","what":"Large Language Model Message Class — LLMMessage","title":"Large Language Model Message Class — LLMMessage","text":"Large Language Model Message Class Large Language Model Message Class","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Large Language Model Message Class — LLMMessage","text":"class manages history messages media interactions intended use large language  models. allows adding messages, converting messages API usage, printing history structured format.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"Large Language Model Message Class — LLMMessage","text":"message_history List store message interactions. system_prompt system prompt used conversation","code":""},{"path":[]},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Large Language Model Message Class — LLMMessage","text":"LLMMessage$new() LLMMessage$clone_deep() LLMMessage$add_message() LLMMessage$to_api_format() LLMMessage$has_image() LLMMessage$remove_message() LLMMessage$print() LLMMessage$clone()","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Large Language Model Message Class — LLMMessage","text":"Initializes LLMMessage object optional system prompt.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Large Language Model Message Class — LLMMessage","text":"","code":"LLMMessage$new(system_prompt = \"You are a helpful assistant\")"},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Large Language Model Message Class — LLMMessage","text":"system_prompt string sets initial system prompt.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Large Language Model Message Class — LLMMessage","text":"new LLMMessage object. Deep Clone LLMMessage Object method creates deep copy LLMMessage object. ensures internal states, including message histories settings, copied original object remains unchanged mutations applied copy. particularly useful maintaining immutability tidyverse-like functional programming context functions side effects inputs.","code":""},{"path":[]},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Large Language Model Message Class — LLMMessage","text":"","code":"LLMMessage$clone_deep()"},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"Large Language Model Message Class — LLMMessage","text":"new LLMMessage object deep copy original. Add message Adds message history. Optionally includes media.","code":""},{"path":[]},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Large Language Model Message Class — LLMMessage","text":"","code":"LLMMessage$add_message(role, content, media = NULL, json = FALSE)"},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Large Language Model Message Class — LLMMessage","text":"role role message sender (e.g., \"user\", \"assistant\"). content textual content message. media Optional; media content attach message. json message raw string contains json response? Convert API format Converts message history format suitable various API calls.","code":""},{"path":[]},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Large Language Model Message Class — LLMMessage","text":"","code":"LLMMessage$to_api_format(api_type, cgpt_image_detail = \"auto\")"},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Large Language Model Message Class — LLMMessage","text":"api_type type API (e.g., \"claude\",\"groq\",\"chatgpt\"). cgpt_image_detail Specific option ChatGPT API (imagedetail - set auto)","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"returns-2","dir":"Reference","previous_headings":"","what":"Returns","title":"Large Language Model Message Class — LLMMessage","text":"message history target API format Simple helper function determine whether message history contains image check function whenever call models support images can post warning user images found sent model","code":""},{"path":[]},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"usage-4","dir":"Reference","previous_headings":"","what":"Usage","title":"Large Language Model Message Class — LLMMessage","text":"","code":"LLMMessage$has_image()"},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"returns-3","dir":"Reference","previous_headings":"","what":"Returns","title":"Large Language Model Message Class — LLMMessage","text":"Returns TRUE message hisotry contains images Remove Message Index Removes message message history specified index.","code":""},{"path":[]},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"usage-5","dir":"Reference","previous_headings":"","what":"Usage","title":"Large Language Model Message Class — LLMMessage","text":"","code":"LLMMessage$remove_message(index)"},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"Large Language Model Message Class — LLMMessage","text":"index positive integer indicating position message remove.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"returns-4","dir":"Reference","previous_headings":"","what":"Returns","title":"Large Language Model Message Class — LLMMessage","text":"LLMMessage object, invisibly.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"method-print-","dir":"Reference","previous_headings":"","what":"Method print()","title":"Large Language Model Message Class — LLMMessage","text":"Prints current message history structured format.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"usage-6","dir":"Reference","previous_headings":"","what":"Usage","title":"Large Language Model Message Class — LLMMessage","text":"","code":"LLMMessage$print()"},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Large Language Model Message Class — LLMMessage","text":"objects class cloneable method.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"usage-7","dir":"Reference","previous_headings":"","what":"Usage","title":"Large Language Model Message Class — LLMMessage","text":"","code":"LLMMessage$clone(deep = FALSE)"},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"arguments-4","dir":"Reference","previous_headings":"","what":"Arguments","title":"Large Language Model Message Class — LLMMessage","text":"deep Whether make deep clone.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/chatgpt.html","id":null,"dir":"Reference","previous_headings":"","what":"Call the OpenAI API to interact with ChatGPT or o-reasoning models — chatgpt","title":"Call the OpenAI API to interact with ChatGPT or o-reasoning models — chatgpt","text":"Call OpenAI API interact ChatGPT o-reasoning models","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/chatgpt.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Call the OpenAI API to interact with ChatGPT or o-reasoning models — chatgpt","text":"","code":"chatgpt(   .llm,   .model = \"gpt-4o\",   .max_tokens = 1024,   .temperature = NULL,   .top_p = NULL,   .top_k = NULL,   .frequency_penalty = NULL,   .presence_penalty = NULL,   .api_url = \"https://api.openai.com/\",   .timeout = 60,   .verbose = FALSE,   .wait = TRUE,   .json = FALSE,   .min_tokens_reset = 0L,   .stream = FALSE,   .dry_run = FALSE )"},{"path":"https://edubruell.github.io/tidyllm/reference/chatgpt.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Call the OpenAI API to interact with ChatGPT or o-reasoning models — chatgpt","text":".llm existing LLMMessage object initial text prompt. .model model identifier (default: \"gpt-4o\"). .max_tokens maximum number tokens generate (default: 1024). .temperature Control randomness response generation (optional). .top_p Nucleus sampling parameter (optional). .top_k Top k sampling parameter (optional). .frequency_penalty Controls repetition frequency (optional). .presence_penalty Controls much penalize repeating content (optional) .api_url Base URL API (default: https://api.openai.com/v1/completions). .timeout Request timeout seconds (default: 60). .verbose additional information shown API call .wait wait rate limits necessary? .json output JSON  mode (default: FALSE). .min_tokens_reset many tokens remaining wait wait token reset? .stream Stream back response piece piece (default: FALSE). .dry_run TRUE, perform dry run return request object.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/chatgpt.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Call the OpenAI API to interact with ChatGPT or o-reasoning models — chatgpt","text":"Returns updated LLMMessage object.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/claude.html","id":null,"dir":"Reference","previous_headings":"","what":"Call the Anthropic API to interact with Claude models — claude","title":"Call the Anthropic API to interact with Claude models — claude","text":"Call Anthropic API interact Claude models","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/claude.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Call the Anthropic API to interact with Claude models — claude","text":"","code":"claude(   .llm,   .model = \"claude-3-5-sonnet-20240620\",   .max_tokens = 1024,   .temperature = NULL,   .top_k = NULL,   .top_p = NULL,   .metadata = NULL,   .stop_sequences = NULL,   .tools = NULL,   .api_url = \"https://api.anthropic.com/\",   .verbose = FALSE,   .wait = TRUE,   .min_tokens_reset = 0L,   .timeout = 60,   .json = FALSE,   .stream = FALSE,   .dry_run = FALSE )"},{"path":"https://edubruell.github.io/tidyllm/reference/claude.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Call the Anthropic API to interact with Claude models — claude","text":".llm existing LLMMessage object initial text prompt. .model model identifier (default: \"claude-3-5-sonnet-20240620\"). .max_tokens maximum number tokens generate (default: 1024). .temperature Control randomness response generation (optional). .top_k Top k sampling parameter (optional). .top_p Nucleus sampling parameter (optional). .metadata Additional metadata request (optional). .stop_sequences Sequences stop generation (optional). .tools Additional tools used model (optional). .api_url Base URL API (default: \"https://api.anthropic.com/v1/messages\"). .verbose additional information shown API call .wait wait rate limits necessary? .min_tokens_reset many tokens remaining wait wait token reset? .timeout Request timeout seconds (default: 60). .json output JSON  (default: FALSE). .stream Stream back response piece piece (default: FALSE). .dry_run TRUE, perform dry run return request object.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/claude.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Call the Anthropic API to interact with Claude models — claude","text":"Returns updated LLMMessage object.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/df_llm_message.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert a Data Frame to an LLMMessage Object — df_llm_message","title":"Convert a Data Frame to an LLMMessage Object — df_llm_message","text":"function takes data frame converts LLMMessage object representing conversation history. data frame contain specific columns (role content) row representing message conversation.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/df_llm_message.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert a Data Frame to an LLMMessage Object — df_llm_message","text":"","code":"df_llm_message(.df)"},{"path":"https://edubruell.github.io/tidyllm/reference/df_llm_message.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert a Data Frame to an LLMMessage Object — df_llm_message","text":".df data frame least two rows columns role content. column role contain values \"user\", \"assistant\", \"system\", content type character.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/df_llm_message.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert a Data Frame to an LLMMessage Object — df_llm_message","text":"LLMMessage object containing structured messages per input data frame.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/generate_callback_function.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate API-Specific Callback Function for Streaming Responses — generate_callback_function","title":"Generate API-Specific Callback Function for Streaming Responses — generate_callback_function","text":"function generates callback function processes streaming responses different language model APIs. callback function specific API provided (claude, ollama, \"mistral\", chatgpt) processes incoming data streams, printing content console updating global environment use.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/generate_callback_function.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate API-Specific Callback Function for Streaming Responses — generate_callback_function","text":"","code":"generate_callback_function(.api)"},{"path":"https://edubruell.github.io/tidyllm/reference/generate_callback_function.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate API-Specific Callback Function for Streaming Responses — generate_callback_function","text":".api character string indicating API type. Supported values \"claude\", \"ollama\", \"mistral\", \"chatgpt\".","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/generate_callback_function.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate API-Specific Callback Function for Streaming Responses — generate_callback_function","text":"function serves callback handle streaming responses specified API. callback function processes raw data, updates .tidyllm_stream_env$stream object, prints streamed content console. function returns TRUE streaming continue, FALSE streaming finished.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/generate_callback_function.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generate API-Specific Callback Function for Streaming Responses — generate_callback_function","text":"Claude API: function processes event data lines, handles message_start message_stop events control streaming flow. Ollama API: function directly parses stream content JSON extracts message$content field. ChatGPT API: function handles JSON data streams processes content deltas. stops processing [DONE] message encountered. Mistral API: function similar ChatGPT callback function. stops processing [DONE] message encountered.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/get_reply.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve an Assistant Reply by Index — get_reply","title":"Retrieve an Assistant Reply by Index — get_reply","text":"Extracts content assistant's reply LLMMessage object specific index. function can handle replies expected JSON format attempting parse . parsing fails user opts raw text, function gracefully return original content.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/get_reply.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve an Assistant Reply by Index — get_reply","text":"","code":"get_reply(.llm, .index = NULL, .raw = FALSE)"},{"path":"https://edubruell.github.io/tidyllm/reference/get_reply.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve an Assistant Reply by Index — get_reply","text":".llm LLMMessage object containing history messages exchanged assistant. parameter must valid LLMMessage object; otherwise, function throw error. .index positive integer indicating assistant reply retrieve. Defaults NULL, retrieves last reply. .raw logical value indicating whether return raw text even message marked JSON. Defaults FALSE, meaning function attempt parse JSON.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/get_reply.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieve an Assistant Reply by Index — get_reply","text":"Returns content assistant's reply specified index, based following conditions: assistant replies, NULL returned. reply marked JSON parsing successful, list containing: parsed_content: parsed JSON content. raw_response: original raw content. json: flag indicating successful JSON parsing (TRUE). JSON parsing fails, list containing: parsed_content: NULL. raw_response: original raw content. json: FALSE. .raw TRUE message marked JSON, returns raw text content directly.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/get_user_message.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve a User Message by Index — get_user_message","title":"Retrieve a User Message by Index — get_user_message","text":"Extracts content user's message LLMMessage object specific index.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/get_user_message.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve a User Message by Index — get_user_message","text":"","code":"get_user_message(.llm, .index = NULL)"},{"path":"https://edubruell.github.io/tidyllm/reference/get_user_message.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve a User Message by Index — get_user_message","text":".llm LLMMessage object. .index positive integer indicating user message retrieve. Defaults NULL, retrieves last message.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/get_user_message.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieve a User Message by Index — get_user_message","text":"Returns content user's message specified index. messages found, returns NULL.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/groq.html","id":null,"dir":"Reference","previous_headings":"","what":"Call the Groq API to interact with fast opensource models on Groq — groq","title":"Call the Groq API to interact with fast opensource models on Groq — groq","text":"Call Groq API interact fast opensource models Groq","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/groq.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Call the Groq API to interact with fast opensource models on Groq — groq","text":"","code":"groq(   .llm,   .model = \"llama-3.2-11b-vision-preview\",   .max_tokens = 1024,   .temperature = NULL,   .top_p = NULL,   .frequency_penalty = NULL,   .presence_penalty = NULL,   .api_url = \"https://api.groq.com/\",   .json = FALSE,   .timeout = 60,   .verbose = FALSE,   .wait = TRUE,   .min_tokens_reset = 0L,   .dry_run = FALSE )"},{"path":"https://edubruell.github.io/tidyllm/reference/groq.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Call the Groq API to interact with fast opensource models on Groq — groq","text":".llm existing LLMMessage object initial text prompt. .model model identifier (default: \"llama-3.2-11b-vision-preview\"). .max_tokens maximum number tokens generate (default: 1024). .temperature Control randomness response generation (optional). .top_p Nucleus sampling parameter (optional). .frequency_penalty Controls repetition frequency (optional). .presence_penalty Controls much penalize repeating content (optional) .api_url Base URL API (default: \"https://api.anthropic.com/v1/messages\"). .json output structured JSON  (default: FALSE). .timeout Request timeout seconds (default: 60). .verbose additional information shown API call .wait wait rate limits necessary? .min_tokens_reset many tokens remaining wait wait token reset? .dry_run TRUE, perform dry run return request object.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/groq.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Call the Groq API to interact with fast opensource models on Groq — groq","text":"Returns updated LLMMessage object.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/initialize_api_env.html","id":null,"dir":"Reference","previous_headings":"","what":"Initialize or Retrieve API-specific Environment — initialize_api_env","title":"Initialize or Retrieve API-specific Environment — initialize_api_env","text":"function initializes named environment storing rate limit information specific API. ensures API's rate limit data stored separately.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/initialize_api_env.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Initialize or Retrieve API-specific Environment — initialize_api_env","text":"","code":"initialize_api_env(.api_name)"},{"path":"https://edubruell.github.io/tidyllm/reference/initialize_api_env.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Initialize or Retrieve API-specific Environment — initialize_api_env","text":".api_name name API initialize retrieve environment","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/last_reply.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve the Last Assistant Reply — last_reply","title":"Retrieve the Last Assistant Reply — last_reply","text":"wrapper around get_reply() retrieve recent assistant reply.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/last_reply.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve the Last Assistant Reply — last_reply","text":"","code":"last_reply(.llm, .raw = FALSE)"},{"path":"https://edubruell.github.io/tidyllm/reference/last_reply.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve the Last Assistant Reply — last_reply","text":".llm LLMMessage object. .raw logical value indicating whether return raw text even message marked JSON. Defaults FALSE, meaning function attempt parse JSON.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/last_reply.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieve the Last Assistant Reply — last_reply","text":"Returns content assistant's reply specified index, based following conditions: assistant replies, NULL returned. reply marked JSON parsing successful, list containing: parsed_content: parsed JSON content. raw_response: original raw content. json: flag indicating successful JSON parsing (TRUE). JSON parsing fails, list containing: parsed_content: NULL. raw_response: original raw content. json: FALSE. .raw TRUE message marked JSON, returns raw text content directly.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/last_user_message.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve the Last User Message — last_user_message","title":"Retrieve the Last User Message — last_user_message","text":"wrapper around get_user_message() retrieve recent user message.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/last_user_message.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve the Last User Message — last_user_message","text":"","code":"last_user_message(.llm)"},{"path":"https://edubruell.github.io/tidyllm/reference/last_user_message.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve the Last User Message — last_user_message","text":".llm LLMMessage object.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/last_user_message.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieve the Last User Message — last_user_message","text":"content last user message.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/llm_message.html","id":null,"dir":"Reference","previous_headings":"","what":"Create or Update Large Language Model Message Object — llm_message","title":"Create or Update Large Language Model Message Object — llm_message","text":"function allows creation new LLMMessage object updating existing one. can handle addition text prompts various media types images, PDFs, text files, plots.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/llm_message.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create or Update Large Language Model Message Object — llm_message","text":"","code":"llm_message(   .llm = NULL,   .prompt = NULL,   .role = \"user\",   .system_prompt = \"You are a helpful assistant\",   .imagefile = NULL,   .pdf = NULL,   .textfile = NULL,   .capture_plot = FALSE,   .f = NULL )"},{"path":"https://edubruell.github.io/tidyllm/reference/llm_message.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create or Update Large Language Model Message Object — llm_message","text":".llm existing LLMMessage object initial text prompt. .prompt Text prompt add message history. .role role message sender, typically \"user\" \"assistant\". .system_prompt Default system prompt new LLMMessage needs created. .imagefile Path image file attached (optional). .pdf Path PDF file attached (optional). .textfile Path text file read attached (optional). .capture_plot Boolean indicate whether plot captured attached image (optional). .f R function whose output captured attached (optional).","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/llm_message.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create or Update Large Language Model Message Object — llm_message","text":"Returns updated new LLMMessage object.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/mistral.html","id":null,"dir":"Reference","previous_headings":"","what":"Send LLMMessage to Mistral API — mistral","title":"Send LLMMessage to Mistral API — mistral","text":"Send LLMMessage Mistral API","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/mistral.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Send LLMMessage to Mistral API — mistral","text":"","code":"mistral(   .llm,   .model = \"mistral-large-latest\",   .stream = FALSE,   .seed = NULL,   .json = FALSE,   .temperature = NULL,   .timeout = 120,   .wait = TRUE,   .min_tokens_reset = 0L,   .max_tokens = 1024,   .min_tokens = NULL,   .dry_run = FALSE,   .verbose = FALSE )"},{"path":"https://edubruell.github.io/tidyllm/reference/mistral.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Send LLMMessage to Mistral API — mistral","text":".llm existing LLMMessage object initial text prompt. .model model identifier (default: \"mistral-large-latest\"). .stream answer streamed console comes (optional) .seed seed used random numbers  (optional). .json output structured JSON  (default: FALSE). .temperature Control randomness response generation (optional). .timeout connection time (default: 120 seconds). .wait wait rate limits necessary? .min_tokens_reset many tokens remaining wait wait token reset? .max_tokens Maximum number tokens response (default: 1024). .min_tokens Minimum number tokens response (optional). .dry_run TRUE, perform dry run return request object. .verbose additional information shown API call","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/mistral.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Send LLMMessage to Mistral API — mistral","text":"Returns updated LLMMessage object.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/ollama.html","id":null,"dir":"Reference","previous_headings":"","what":"Send LLMMessage to ollama API — ollama","title":"Send LLMMessage to ollama API — ollama","text":"Send LLMMessage ollama API","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/ollama.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Send LLMMessage to ollama API — ollama","text":"","code":"ollama(   .llm,   .model = \"llama3\",   .stream = FALSE,   .seed = NULL,   .json = FALSE,   .temperature = NULL,   .num_ctx = 2048,   .ollama_server = \"http://localhost:11434\",   .timeout = 120,   .dry_run = FALSE )"},{"path":"https://edubruell.github.io/tidyllm/reference/ollama.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Send LLMMessage to ollama API — ollama","text":".llm existing LLMMessage object initial text prompt. .model model identifier (default: \"llama3\"). .stream answer streamed console comes (optional) .seed seed used random numbers  (optional). .json output structured JSON  (default: FALSE). .temperature Control randomness response generation (optional). .num_ctx size context window tokens (optional) .ollama_server URL ollama server used .timeout connection time .dry_run TRUE, perform dry run return request object.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/ollama.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Send LLMMessage to ollama API — ollama","text":"Returns updated LLMMessage object.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/ollama_download_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Download a model from the Ollama API — ollama_download_model","title":"Download a model from the Ollama API — ollama_download_model","text":"function sends request Ollama API download specified model. can operate streaming mode provides live updates download status progress, single response mode.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/ollama_download_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Download a model from the Ollama API — ollama_download_model","text":"","code":"ollama_download_model(.model, .ollama_server = \"http://localhost:11434\")"},{"path":"https://edubruell.github.io/tidyllm/reference/ollama_download_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Download a model from the Ollama API — ollama_download_model","text":".model name model download. .ollama_server base URL Ollama API (default \"http://localhost:11434\").","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/ollama_embedding.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Embeddings Using Ollama API — ollama_embedding","title":"Generate Embeddings Using Ollama API — ollama_embedding","text":"Generate Embeddings Using Ollama API","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/ollama_embedding.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Embeddings Using Ollama API — ollama_embedding","text":"","code":"ollama_embedding(   .llm,   .model = \"all-minilm\",   .truncate = TRUE,   .ollama_server = \"http://localhost:11434\",   .timeout = 120,   .dry_run = FALSE )"},{"path":"https://edubruell.github.io/tidyllm/reference/ollama_embedding.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Embeddings Using Ollama API — ollama_embedding","text":".llm existing LLMMessage object (charachter vector texts embed) .model embedding model identifier (default: \"-minilm\"). .truncate Whether truncate inputs fit model's context length (default: TRUE). .ollama_server URL Ollama server used (default: \"http://localhost:11434\"). .timeout Timeout API request seconds (default: 120). .dry_run TRUE, perform dry run return request object.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/ollama_embedding.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Embeddings Using Ollama API — ollama_embedding","text":"matrix column corresponds embedding message message history.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/ollama_list_models.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve and return model information from the Ollama API — ollama_list_models","title":"Retrieve and return model information from the Ollama API — ollama_list_models","text":"function connects Ollama API retrieves information available models, returning tibble.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/ollama_list_models.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve and return model information from the Ollama API — ollama_list_models","text":"","code":"ollama_list_models(.ollama_server = \"http://localhost:11434\")"},{"path":"https://edubruell.github.io/tidyllm/reference/ollama_list_models.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve and return model information from the Ollama API — ollama_list_models","text":".ollama_server URL ollama server used","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/ollama_list_models.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieve and return model information from the Ollama API — ollama_list_models","text":"tibble containing model information, NULL models found.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/parse_duration_to_seconds.html","id":null,"dir":"Reference","previous_headings":"","what":"An internal function to parse the duration strings that OpenAI APIs return for ratelimit resets — parse_duration_to_seconds","title":"An internal function to parse the duration strings that OpenAI APIs return for ratelimit resets — parse_duration_to_seconds","text":"internal function parses duration strings returned OpenAI API","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/parse_duration_to_seconds.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"An internal function to parse the duration strings that OpenAI APIs return for ratelimit resets — parse_duration_to_seconds","text":"","code":"parse_duration_to_seconds(.duration_str)"},{"path":"https://edubruell.github.io/tidyllm/reference/parse_duration_to_seconds.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"An internal function to parse the duration strings that OpenAI APIs return for ratelimit resets — parse_duration_to_seconds","text":".duration_str duration string.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/parse_duration_to_seconds.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"An internal function to parse the duration strings that OpenAI APIs return for ratelimit resets — parse_duration_to_seconds","text":"numeric number seconds","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/pdf_page_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Batch Process PDF into LLM Messages — pdf_page_batch","title":"Batch Process PDF into LLM Messages — pdf_page_batch","text":"function processes PDF file page page. page, extracts text converts page image. creates list LLMMessage objects text image multimodal processing. Users can specify range pages process provide custom function generate prompts page.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/pdf_page_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Batch Process PDF into LLM Messages — pdf_page_batch","text":"","code":"pdf_page_batch(   .pdf,   .general_prompt,   .system_prompt = \"You are a helpful assistant\",   .page_range = NULL,   .prompt_fn = NULL )"},{"path":"https://edubruell.github.io/tidyllm/reference/pdf_page_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Batch Process PDF into LLM Messages — pdf_page_batch","text":".pdf Path PDF file. .general_prompt default prompt applied page .prompt_fn provided. .system_prompt Optional system prompt initialize LLMMessage (default \"helpful assistant\"). .page_range vector two integers specifying start end pages process. NULL, pages processed. .prompt_fn optional custom function generates prompt page. function takes page text input returns string. NULL, .general_prompt used pages.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/pdf_page_batch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Batch Process PDF into LLM Messages — pdf_page_batch","text":"list LLMMessage objects, containing text image page.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/perform_api_request.html","id":null,"dir":"Reference","previous_headings":"","what":"Perform an API request to interact with language models — perform_api_request","title":"Perform an API request to interact with language models — perform_api_request","text":"Perform API request interact language models","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/perform_api_request.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Perform an API request to interact with language models — perform_api_request","text":"","code":"perform_api_request(   .request,   .api,   .stream = FALSE,   .timeout = 60,   .parse_response_fn = NULL,   .dry_run = FALSE )"},{"path":"https://edubruell.github.io/tidyllm/reference/perform_api_request.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Perform an API request to interact with language models — perform_api_request","text":".request httr2 request object. .api API identifier (e.g., \"claude\", \"openai\"). .stream Stream response TRUE. .timeout Request timeout seconds. .parse_response_fn function parse assistant's reply. .dry_run TRUE, perform dry run return request object.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/perform_api_request.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Perform an API request to interact with language models — perform_api_request","text":"list containing assistant's reply response headers.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/rate_limit_info.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the current rate limit information for all or a specific API — rate_limit_info","title":"Get the current rate limit information for all or a specific API — rate_limit_info","text":"function retrieves rate limit details specified API, APIs stored .tidyllm_rate_limit_env API specified.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/rate_limit_info.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the current rate limit information for all or a specific API — rate_limit_info","text":"","code":"rate_limit_info(.api_name = NULL)"},{"path":"https://edubruell.github.io/tidyllm/reference/rate_limit_info.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the current rate limit information for all or a specific API — rate_limit_info","text":".api_name (Optional) name API whose rate limit info want print. provided, rate limit info APIs environment returned","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/rate_limit_info.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the current rate limit information for all or a specific API — rate_limit_info","text":"tibble containing rate limit information.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/tidyllm-package.html","id":null,"dir":"Reference","previous_headings":"","what":"tidyllm: Tidy Integration of Large Language Models — tidyllm-package","title":"tidyllm: Tidy Integration of Large Language Models — tidyllm-package","text":"tidy interface integrating large language model (LLM) APIs 'Claude', 'ChatGPT', 'Groq','Mistral' local models via 'Ollama' R workflows. package supports text media-based interactions, interactive message history, stateful rate limit handling, tidy, pipeline-oriented interface streamlined integration data workflows. Web services available https://www.anthropic.com, https://openai.com, https://groq.com, https://mistral.ai/ https://ollama.com.","code":""},{"path":[]},{"path":"https://edubruell.github.io/tidyllm/reference/tidyllm-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"tidyllm: Tidy Integration of Large Language Models — tidyllm-package","text":"Maintainer: Eduard Brüll eduard.bruell@zew.de","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/update_rate_limit.html","id":null,"dir":"Reference","previous_headings":"","what":"Update the standard API rate limit info in the hidden .tidyllm_rate_limit_env environment — update_rate_limit","title":"Update the standard API rate limit info in the hidden .tidyllm_rate_limit_env environment — update_rate_limit","text":"function initializes stores ratelimit information API functions future use","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/update_rate_limit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update the standard API rate limit info in the hidden .tidyllm_rate_limit_env environment — update_rate_limit","text":"","code":"update_rate_limit(.api_name, .response_object)"},{"path":"https://edubruell.github.io/tidyllm/reference/update_rate_limit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update the standard API rate limit info in the hidden .tidyllm_rate_limit_env environment — update_rate_limit","text":".api_name name API initialize retrieve environment. .response_object preparsed response object cotaining info remaining requests, tokens rest times","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/wait_rate_limit.html","id":null,"dir":"Reference","previous_headings":"","what":"Wait for ratelimit restore times to ellapse if necessary — wait_rate_limit","title":"Wait for ratelimit restore times to ellapse if necessary — wait_rate_limit","text":"function implements standardized wait rate limit resets","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/wait_rate_limit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Wait for ratelimit restore times to ellapse if necessary — wait_rate_limit","text":"","code":"wait_rate_limit(.api_name, .min_tokens_reset)"},{"path":"https://edubruell.github.io/tidyllm/reference/wait_rate_limit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Wait for ratelimit restore times to ellapse if necessary — wait_rate_limit","text":".api_name name API rate limit want wait .min_tokens_reset token boundary wish reset (Typically larger size message)","code":""},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"changelog-for-development-version-017","dir":"Changelog","previous_headings":"","what":"Changelog for Development Version 0.1.7","title":"Changelog for Development Version 0.1.7","text":"Changes since last CRAN Release 0.1.0","code":""},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"breaking-changes-compared-to-cran-release-0-1-7","dir":"Changelog","previous_headings":"","what":"Breaking Changes (Compared to CRAN Release 0.1.0)","title":"Changelog for Development Version 0.1.7","text":"last_reply() Changes: .json argument longer used, JSON replies automatically parsed. Use .raw raw text. Groq Models: System prompts longer sent Groq models, since many models groq support multimodal models groq allow .","code":""},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"new-features-0-1-7","dir":"Changelog","previous_headings":"","what":"New Features","title":"Changelog for Development Version 0.1.7","text":"Message Retrieval Functions: Added functions retrieve single messages conversations: last_user_message(), get_reply(index), get_user_message(index) Updated last_reply(): Now wrapper around get_reply() consistent behavior. New Ollama functions: Model Download: Introduced ollama_download_model() function download models Ollama API. supports streaming mode provides live progress bar updates download progress. Embedding Generation: Added ollama_embedding() generate embeddings using Ollama API. processes message histories combines text message content media attachements embeddings. PDF Page Batch Processing: Introduced pdf_page_batch() function, processes PDF files page page, extracting text converting page image allows general prompt page specific prompts. function generates list LLMMessage objects can sent API Support Mistral API: New mistral() function use Mistral Models Le Platforme servers hosted EU. rate-limiting streaming-support.","code":""},{"path":[]},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"groq-support-for-vision-0-1-7","dir":"Changelog","previous_headings":"","what":"Groq support for vision","title":"Changelog for Development Version 0.1.7","text":"groq() function now supports images. Since modern models groq, especially ones multimodal abilities support system prompts, system role deleted groq api calls.","code":""},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"json-mode-improvements-0-1-7","dir":"Changelog","previous_headings":"","what":"JSON Mode Improvements","title":"Changelog for Development Version 0.1.7","text":"Since version 0.1.1, JSON mode now widely supported across API functions, allowing structured outputs APIs support . .json argument now passed API functions, specifying API respond, needed anymore last_reply(). Additionally, behavior reply functions changed. now automatically handle JSON replies parsing structured data falling back raw text case errors. can still force raw text replies even JSON output using .raw argument.","code":""},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"new-tests-for-api-functions-0-1-7","dir":"Changelog","previous_headings":"","what":"New tests for API functions","title":"Changelog for Development Version 0.1.7","text":"Easier Troubleshooting API-function: API functions now support .dry_run argument, allowing users generate httr2-request easier debugging inspection. API Function Tests: Implemented httptest2-based tests mock responses API functions, covering basic functionality rate-limiting.","code":""}]
