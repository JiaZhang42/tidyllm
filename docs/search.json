[{"path":"https://edubruell.github.io/tidyllm/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2024 Eduard Brüll Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm-pdfquestions.html","id":"what-is-json-and-json-mode","dir":"Articles","previous_headings":"","what":"What is JSON and JSON Mode?","title":"Structured Question Answering from PDFs","text":"JSON (JavaScript Object Notation) lightweight, text-based format representing structured data. ’s commonly used transmitting data server web application different parts system. JSON object consists key-value pairs, making ideal capturing structured data like title paper, authors, answers specific research questions. tidyllm, can use ability many large language models write answers JSON ensure AI model returns answers structured format directly R. particularly useful automating processes, extracting information multiple documents, allows easy conversion tables data manipulation.","code":""},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm-pdfquestions.html","id":"context-length","dir":"Articles","previous_headings":"","what":"Context length","title":"Structured Question Answering from PDFs","text":"working long documents research papers, reports, even entire books, one challenge arises much document model can handle . Large language models limitation known context length, refers maximum amount text can process single query. important document exceeds limit, model won’t able process entire content —leading missing sections incomplete answers. models, context length measured tokens (basic units text). example, maximum context length many smaller models 8192 tokens typically covers 30-35 pages text. means longer documents, like academic paper exceeds length, first portion document seen model, potentially omitting key sections like bibliography appendices, important citations results may reside. mitigate , common approach limit number pages sent model processing. example, workflow, restrict input first 35 pages, typically includes abstract, introduction, methodology, results, discussion—sections relevant summarizing paper. approach ensures model can process core content academic papers, mean information typically found later document missed. Alternatively, large documents, split smaller chunks process chunk separately. way, can cover entire content without exceeding model’s context window. However, keep mind might disrupt flow text make harder model retain overall context across chunks.","code":""},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm-pdfquestions.html","id":"example-workflow","dir":"Articles","previous_headings":"","what":"Example Workflow","title":"Structured Question Answering from PDFs","text":"Imagine folder looks something like —many downloaded papers, structure yet: can write simple function processes document passes model. demonstration purposes, send first 35 pages pdf model, even though gpt-4o can handle 128,000 tokens. feature primarily needed, large reports whole books larger typical models context window. Usually papers 35 pages provide model enough information reasonably answer main summarisation queries. want upload complete files though, possible larger models, just need specify file name .pdf argument string variable. example, use chatgpt() default gpt-4o model answer questions. 35-page paper, API cost approximately $0.08, making affordable process multiple papers different queries. See details pricing . also try open-source local models, paid remote models like gpt-4o typically handle longer documents much effectively. Smaller local models, gemma2:9B ollama(), often struggle extensive content even though context window large. use , might need reduce number processed pages simplify queries. Alternatively, larger local models like llama3::70B work, require significant hardware resources run effectively. ⚠️ Note: using paid remote models, ’s important consider data privacy security, especially ’re processing sensitive documents, uploading data external servers may introduce risks — local models provide control cases. enabling .json = TRUE, instruct model return response JSON format. effectively extract key information multiple academic papers, also need consistent way prompt large language model. prompt use specifies details want document, title, authors, type paper, answers specific questions. example questions cover empirical methods, theoretical framework, main point, paper’s key contribution. can course ask specific questions can extract information even relevant use case. nice start. also give model exact schema need answers order make easier work results R. example output lengthy prompt one paper: output function seems quite daunting. close prompt, specified schema. structured three key components: parsed_content, raw_response, is_parsed. structure ensures always access model’s reply raw form structured R list: raw_response: Even response successfully parsed, ’s useful keep original raw output model. field stores unprocessed response returned LLM, can valuable debugging comparison purposes. models explicitly check whether return valid JSON. However, APIs without explicit JSON support like Anthropic,often add sentences like “Based document, requested information JSON format:” actual json. cases can parsed easy fix raw_response, stringr jsonlite::fromJSON(). However, printing raw response example gives us JSON structure asked : parsed_content: field contains structured response successfully parsed list R. allows directly access specific elements reply, title, authors, answers questions, making easy integrate results analysis save human readable document. example can directly get suggested file name response: is_parsed: logical flag indicating whether parsing successful. TRUE, means parsed_content available output conforms expected JSON structure. FALSE, parsed_content returned, can rely raw_response inspection manual handling. can now run function generate kind response papers folder: Let’s look files successfully parsed: Luckily, parse successfully. Now can proceed getting structured output response saving easily readable format. example name, author names,paper type answers questions together row tibble model output: can save summaries excel table can format manually better readability: Similarly can extract key references add separate Excel file. next step work output, go parsed content always pick key_citations map. example, encouter error : Looking cause see common problem JSON-based workflows. can see problem raw responses second paper parsed: key_citations put questions instead separate field. model completely adhere asked produce. quick fix problem check key_citations put : better solution check start whether model adheres schema requested. Many newer APIs, recent OpenAI API, now offer ability pre-specify valid output schema. means can enforce strict formatting rules model generates response, reducing need post-processing corrections. functionality supported future versions tidyllm, allowing smoother integration structured outputs. Another potential fix modify prompt ensure model outputs non-nested JSON format, often simplifies prompt adherence downstream handling. workflows like , ’s common need several iterations refine structure output, especially working complex queries. example, case, defining citations formatted might also improve consistency, since key_citation field varies responses. Additionally, may beneficial run separate pass just citations, perhaps asking model output BibTeX keys four important references together reason citation important note-field bibtex. now, processing steps wanted answers citations can proceed file names got structure folder:","code":"library(tidyverse) library(tidyllm) dir(\"aipapers\") #>  [1] \"2017_Brynjolfsson_etal_AI_Productivity_Paradox.pdf\"                                                #>  [2] \"2018_Autor_etal_AutomationLaborDisplacing.pdf\"                                                     #>  [3] \"2018_Felten_etal_AILinkOccupations.pdf\"                                                            #>  [4] \"2019_Acemoglu_etal_AutomationTasks.pdf\"                                                            #>  [5] \"2022_Arntz_etal_AutomationAngst.pdf\"                                                               #>  [6] \"2023_Brynjolfsson_etal_GenerativeAIWork.pdf\"                                                       #>  [7] \"2023_DellAcqua_etal_NavigatingTechnologicalFrontier.pdf\"                                           #>  [8] \"2023_Eloundou_etal_Labor_Impact_LLMs.pdf\"                                                          #>  [9] \"2023_Horton_etal_LLMs_as_Simulated_Agents.pdf\"                                                     #> [10] \"2023_Korinek_etal_GenerativeAIEconomicResearch.pdf\"                                                #> [11] \"2023_Lergetporer_etal_Automatability_of_Occupations.pdf\"                                           #> [12] \"2023_Noy_etal_ProductivityEffectsAI.pdf\"                                                           #> [13] \"2023_Peng_etal_AI_Productivity.pdf\"                                                                #> [14] \"2023_Svanberg_etal_BeyondAIExposure.pdf\"                                                           #> [15] \"2024_Acemoglu_etal_SimpleMacroeconomicsAI.pdf\"                                                     #> [16] \"2024_Bick_etal_RapidAdoption.pdf\"                                                                  #> [17] \"2024_Bloom_etal_AI_Skill_Premium.pdf\"                                                              #> [18] \"2024_Caplin_etal_ABCsofAI.pdf\"                                                                     #> [19] \"2024_Deming_etal_TechnologicalDisruption.pdf\"                                                      #> [20] \"2024_Falck_etal_KI_Nutzung_Unternehmen.pdf\"                                                        #> [21] \"2024_Falck_etal_KI_Nutzung.pdf\"                                                                    #> [22] \"2024_Falck_etal_KI_Usage_Companies.pdf\"                                                            #> [23] \"2024_Green_etal_AI_and_Skills_Demand.pdf\"                                                          #> [24] \"2301.07543v1.pdf\"                                                                                  #> [25] \"2302.06590v1.pdf\"                                                                                  #> [26] \"2303.10130v5.pdf\"                                                                                  #> [27] \"488.pdf\"                                                                                           #> [28] \"88684e36-en.pdf\"                                                                                   #> [29] \"ABCs_AI_Oct2024.pdf\"                                                                               #> [30] \"acemoglu-restrepo-2019-automation-and-new-tasks-how-technology-displaces-and-reinstates-labor.pdf\" #> [31] \"BBD_GenAI_NBER_Sept2024.pdf\"                                                                       #> [32] \"Deming-Ong-Summers-AESG-2024.pdf\"                                                                  #> [33] \"dp22036.pdf\"                                                                                       #> [34] \"FeltenRajSeamans_AIAbilities_AEA.pdf\"                                                              #> [35] \"JEL-2023-1736_published_version.pdf\"                                                               #> [36] \"Noy_Zhang_1.pdf\"                                                                                   #> [37] \"sd-2024-09-falck-etal-kuenstliche-intelligenz-unternehmen-1.pdf\"                                   #> [38] \"sd-2024-09-falck-etal-kuenstliche-intelligenz-unternehmen-2.pdf\"                                   #> [39] \"sd-2024-09-falck-etal-kuenstliche-intelligenz-unternehmen.pdf\"                                     #> [40] \"ssrn-4700751.pdf\"                                                                                  #> [41] \"SSRN-id4573321.pdf\"                                                                                #> [42] \"The Simple Macroeconomics of AI.pdf\"                                                               #> [43] \"w24001.pdf\"                                                                                        #> [44] \"w24871.pdf\"                                                                                        #> [45] \"w31161.pdf\"                                                                                        #> [46] \"w32430.pdf\" pdf_summary <- function(file, document_prompt){   summary <- llm_message(document_prompt,                .pdf = list(                 filename = file,                 start_page = 1,                 end_page = 35)                ) |>     chatgpt(.json=TRUE,.stream=TRUE) |>     last_reply()      summary } files <- paste0(\"aipapers/\",dir(\"aipapers\"))  example_output <- pdf_summary(files[8],document_prompt = ' Please analyze this document and provide the following details:     1. Title: The full title of the paper.     2. Authors: List of all authors.     3. Suggested new filename: A filename for the document in the format \"ReleaseYear_Author_etal_ShortTitle.pdf\"     4. Type: Is this a (brief) policy report or a research paper? Answer with either \"Policy\" or \"Research\"     5. Answer these four  questions based on the document. Each answer should be roughly one 100 words long:         Q1. What empirical methods are used in this work?         Q2. What theoretical framework is applied or discussed?         Q3. What is the main point or argument presented?         Q4. What is the key contribution of this work?     6. Key citations: List the four most important references that the document uses in describing its own contribution.      Please  answers only with a json output in the following format:  {   \"title\": \"\",   \"authors\": [],   \"suggested_new_filename\": \"\",   \"type\": \"\",   \"questions\": {     \"empirical_methods\": \"\",     \"theory\": \"\",     \"main_point\": \"\",     \"contribution\": \"\"   },   \"key_citations\": [] } ')  example_output #> $raw_response #> [1] \"{\\n  \\\"title\\\": \\\"The Rapid Adoption of Generative AI\\\",\\n  \\\"authors\\\": [\\\"Alexander Bick\\\", \\\"Adam Blandin\\\", \\\"David J. Deming\\\"],\\n  \\\"suggested_new_filename\\\": \\\"2024_Bick_etal_RapidAdoptionGenerativeAI.pdf\\\",\\n  \\\"type\\\": \\\"Research\\\",\\n  \\\"questions\\\": {\\n    \\\"empirical_methods\\\": \\\"The paper employs a nationally representative survey method, the Real-Time Population Survey (RPS), to collect data on generative AI usage among the U.S. population. The RPS methodology aligns with the structure of the Current Population Survey (CPS) and ensures representativeness by benchmarking against national employment and earnings statistics. The survey allows flexibility in adding and modifying questions, enabling tracking of generative AI usage over time. This empirical approach provides comprehensive data on the adoption rates and usage patterns of generative AI in the United States, allowing the authors to analyze trends and draw comparisons with historical technological adoption metrics like PCs and the internet.\\\",\\n    \\\"theory\\\": \\\"The theoretical framework revolves around the concept of technology adoption and its economic impact. The paper discusses generative AI as a general-purpose technology (GPT) that can be used across various occupations and tasks. It leverages existing theories on the diffusion of innovations and the role of technology in economic growth and inequality. References to historical analyses of previous GPTs, like personal computers and the internet, provide a backdrop for understanding the implications of generative AI. The authors discuss the potential for generative AI to affect productivity and labor market dynamics, drawing comparisons with past technological shifts.\\\",\\n    \\\"main_point\\\": \\\"The main argument of the paper is that the adoption of generative AI in the United States is occurring at a faster pace than previous technological advances, such as personal computers and the internet. The authors highlight the broad utilization of generative AI across different demographics and occupations. They present evidence showing high usage rates both at work and home, with significant implications for productivity and potentially influencing workplace inequalities. They suggest that, although its rapid adoption could transform economic activities and job tasks, generative AI may amplify existing disparities due to unequal access among demographics.\\\",\\n    \\\"contribution\\\": \\\"The key contribution of this work is the provision of first-of-its-kind empirical data on the adoption and use of generative AI among a representative sample of the U.S. population. By detailing generative AI's higher adoption rates compared to past technologies, the study offers insights into current trends and potential future impacts on labor markets and productivity. Moreover, it sheds light on demographic and occupational patterns in AI usage, informing debates on inequality and economic transformation. The paper's unique survey-based approach serves as a valuable resource for policymakers, researchers, and stakeholders interested in understanding and maximizing the potential benefits of generative AI adoption.\\\"\\n  },\\n  \\\"key_citations\\\": [\\n    \\\"Brynjolfsson, Li, and Raymond, 2023\\\",\\n    \\\"Acemoglu, Autor, Hazell, and Restrepo, 2022\\\",\\n    \\\"Autor, Levy, and Murnane, 2003\\\",\\n    \\\"Comin and Hobijn, 2010\\\"\\n  ]\\n}\" #>  #> $parsed_content #> $parsed_content$title #> [1] \"The Rapid Adoption of Generative AI\" #>  #> $parsed_content$authors #> [1] \"Alexander Bick\"  \"Adam Blandin\"    \"David J. Deming\" #>  #> $parsed_content$suggested_new_filename #> [1] \"2024_Bick_etal_RapidAdoptionGenerativeAI.pdf\" #>  #> $parsed_content$type #> [1] \"Research\" #>  #> $parsed_content$questions #> $parsed_content$questions$empirical_methods #> [1] \"The paper employs a nationally representative survey method, the Real-Time Population Survey (RPS), to collect data on generative AI usage among the U.S. population. The RPS methodology aligns with the structure of the Current Population Survey (CPS) and ensures representativeness by benchmarking against national employment and earnings statistics. The survey allows flexibility in adding and modifying questions, enabling tracking of generative AI usage over time. This empirical approach provides comprehensive data on the adoption rates and usage patterns of generative AI in the United States, allowing the authors to analyze trends and draw comparisons with historical technological adoption metrics like PCs and the internet.\" #>  #> $parsed_content$questions$theory #> [1] \"The theoretical framework revolves around the concept of technology adoption and its economic impact. The paper discusses generative AI as a general-purpose technology (GPT) that can be used across various occupations and tasks. It leverages existing theories on the diffusion of innovations and the role of technology in economic growth and inequality. References to historical analyses of previous GPTs, like personal computers and the internet, provide a backdrop for understanding the implications of generative AI. The authors discuss the potential for generative AI to affect productivity and labor market dynamics, drawing comparisons with past technological shifts.\" #>  #> $parsed_content$questions$main_point #> [1] \"The main argument of the paper is that the adoption of generative AI in the United States is occurring at a faster pace than previous technological advances, such as personal computers and the internet. The authors highlight the broad utilization of generative AI across different demographics and occupations. They present evidence showing high usage rates both at work and home, with significant implications for productivity and potentially influencing workplace inequalities. They suggest that, although its rapid adoption could transform economic activities and job tasks, generative AI may amplify existing disparities due to unequal access among demographics.\" #>  #> $parsed_content$questions$contribution #> [1] \"The key contribution of this work is the provision of first-of-its-kind empirical data on the adoption and use of generative AI among a representative sample of the U.S. population. By detailing generative AI's higher adoption rates compared to past technologies, the study offers insights into current trends and potential future impacts on labor markets and productivity. Moreover, it sheds light on demographic and occupational patterns in AI usage, informing debates on inequality and economic transformation. The paper's unique survey-based approach serves as a valuable resource for policymakers, researchers, and stakeholders interested in understanding and maximizing the potential benefits of generative AI adoption.\" #>  #>  #> $parsed_content$key_citations #> [1] \"Brynjolfsson, Li, and Raymond, 2023\"         #> [2] \"Acemoglu, Autor, Hazell, and Restrepo, 2022\" #> [3] \"Autor, Levy, and Murnane, 2003\"              #> [4] \"Comin and Hobijn, 2010\"                      #>  #>  #> $is_parsed #> [1] TRUE example_output$raw_response |> cat() #> { #>   \"title\": \"The Rapid Adoption of Generative AI\", #>   \"authors\": [\"Alexander Bick\", \"Adam Blandin\", \"David J. Deming\"], #>   \"suggested_new_filename\": \"2024_Bick_etal_RapidAdoptionGenerativeAI.pdf\", #>   \"type\": \"Research\", #>   \"questions\": { #>     \"empirical_methods\": \"The paper employs a nationally representative survey method, the Real-Time Population Survey (RPS), to collect data on generative AI usage among the U.S. population. The RPS methodology aligns with the structure of the Current Population Survey (CPS) and ensures representativeness by benchmarking against national employment and earnings statistics. The survey allows flexibility in adding and modifying questions, enabling tracking of generative AI usage over time. This empirical approach provides comprehensive data on the adoption rates and usage patterns of generative AI in the United States, allowing the authors to analyze trends and draw comparisons with historical technological adoption metrics like PCs and the internet.\", #>     \"theory\": \"The theoretical framework revolves around the concept of technology adoption and its economic impact. The paper discusses generative AI as a general-purpose technology (GPT) that can be used across various occupations and tasks. It leverages existing theories on the diffusion of innovations and the role of technology in economic growth and inequality. References to historical analyses of previous GPTs, like personal computers and the internet, provide a backdrop for understanding the implications of generative AI. The authors discuss the potential for generative AI to affect productivity and labor market dynamics, drawing comparisons with past technological shifts.\", #>     \"main_point\": \"The main argument of the paper is that the adoption of generative AI in the United States is occurring at a faster pace than previous technological advances, such as personal computers and the internet. The authors highlight the broad utilization of generative AI across different demographics and occupations. They present evidence showing high usage rates both at work and home, with significant implications for productivity and potentially influencing workplace inequalities. They suggest that, although its rapid adoption could transform economic activities and job tasks, generative AI may amplify existing disparities due to unequal access among demographics.\", #>     \"contribution\": \"The key contribution of this work is the provision of first-of-its-kind empirical data on the adoption and use of generative AI among a representative sample of the U.S. population. By detailing generative AI's higher adoption rates compared to past technologies, the study offers insights into current trends and potential future impacts on labor markets and productivity. Moreover, it sheds light on demographic and occupational patterns in AI usage, informing debates on inequality and economic transformation. The paper's unique survey-based approach serves as a valuable resource for policymakers, researchers, and stakeholders interested in understanding and maximizing the potential benefits of generative AI adoption.\" #>   }, #>   \"key_citations\": [ #>     \"Brynjolfsson, Li, and Raymond, 2023\", #>     \"Acemoglu, Autor, Hazell, and Restrepo, 2022\", #>     \"Autor, Levy, and Murnane, 2003\", #>     \"Comin and Hobijn, 2010\" #>   ] #> } example_output$parsed_content$suggested_new_filename #> [1] \"2024_Bick_etal_RapidAdoptionGenerativeAI.pdf\" pdf_responses <- map(files,pdf_summary) pdf_responses |>   map_lgl(\"is_parsed\")  #>  [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE #> [16] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE table1 <- pdf_responses |>   map(\"parsed_content\") |>   map_dfr(~{     #Collapse authors into a single string     base_info <- tibble(       authors = str_c(.x$authors, collapse = \"; \"),       title   = .x$title,       type    = .x$type     )           answers <- tibble(      main_point   = .x$questions$main_point,      contribution =.x$questions$contribution,      theory       = .x$questions$theory,      methods      = .x$questions$empirical_methods     )     bind_cols(base_info,answers)   })  table1  #> # A tibble: 23 × 7 #>    authors                    title type  main_point contribution theory methods #>    <chr>                      <chr> <chr> <chr>      <chr>        <chr>  <chr>   #>  1 John J. Horton             Larg… Rese… The main … The key con… The t… The pa… #>  2 Sida Peng; Eirini Kalliam… The … Rese… The main … The key con… The t… The em… #>  3 Tyna Eloundou; Sam Mannin… GPTs… Rese… The main … The key con… The t… The pa… #>  4 Philipp Lergetporer; Kath… Auto… Rese… The prima… The paper's… The t… The wo… #>  5 Andrew Green               Arti… Rese… The main … This work's… The t… The do… #>  6 Andrew Caplin; David J. D… The … Rese… The main … This work's… The t… The em… #>  7 Daron Acemoglu; Pascual R… Auto… Rese… The main … The key con… The t… The au… #>  8 Alexander Bick; Adam Blan… The … Rese… The main … The key con… The t… The em… #>  9 David Deming; Christopher… Tech… Poli… The main … The key con… The t… The pa… #> 10 Melanie Arntz; Sebastian … The … Rese… The main … The key con… The p… The au… #> # ℹ 13 more rows table1 |> writexl::write_xlsx(\"papers_summaries.xlsx\") pdf_responses |>   map(\"parsed_content\") |>   map(\"key_citations\") |>   map_dfr(~as_tibble) #> Error in `dplyr::bind_rows()`: #> ! Argument 1 must be a data frame or a named atomic vector. #> { #>   \"title\": [\"The Impact of AI on Developer Productivity: Evidence from GitHub Copilot\"], #>   \"authors\": [\"Sida Peng\", \"Eirini Kalliamvakou\", \"Peter Cihon\", \"Mert Demirer\"], #>   \"suggested_new_filename\": [\"2023_Peng_etal_AI_Productivity.pdf\"], #>   \"type\": [\"Research\"], #>   \"questions\": { #>     \"empirical_methods\": [\".. abbreviated ..\"], #>     \"theory\": [\".. abbreviated ..\"], #>     \"main_point\": [\".. abbreviated ..\"], #>     \"contribution\": [\".. abbreviated ..\"], #>     \"key_citations\": [\"Zhang et al., 2022\", \"Nguyen and Nadi, 2022\", \"Barke et al., 2022\", \"Mozannar et al., 2022\"] #>   } #> } pdf_responses |>   map(\"parsed_content\") |>   map_dfr(~{     # Check if key_citations is nested under questions or at the top level     citations <- if (!is.null(.x$key_citations)) {       as_tibble(.x$key_citations)     } else if (!is.null(.x$questions$key_citations)) {       as_tibble(.x$questions$key_citations)     }     citations   }) #> # A tibble: 91 × 1 #>    value                                                                         #>    <chr>                                                                         #>  1 Charness, G., & Rabin, M. (2002). Understanding social preferences with simp… #>  2 Kahneman, D., Knetsch, J. L., & Thaler, R. H. (1986). Fairness as a constrai… #>  3 Samuelson, W., & Zeckhauser, R. (1988). Status quo bias in decision making.   #>  4 Aher, M., Arriaga, X., & Kalai, A. (2022). Large language models as social a… #>  5 Zhang et al., 2022                                                            #>  6 Nguyen and Nadi, 2022                                                         #>  7 Barke et al., 2022                                                            #>  8 Mozannar et al., 2022                                                         #>  9 Brynjolfsson et al., 2018                                                     #> 10 Felten et al., 2018                                                           #> # ℹ 81 more rows tibble(old_name  = files,        new_name  = pdf_responses |>          map(\"parsed_content\") |>          map_chr(\"suggested_new_filename\")         ) |>   mutate(     success = file.rename(old_name, file.path(\"aipapers\", new_name))   )"},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm-pdfquestions.html","id":"outlook","dir":"Articles","previous_headings":"","what":"Outlook","title":"Structured Question Answering from PDFs","text":"structured question-answering workflow streamlines extraction key insights academic papers, can also adapted document-heavy tasks. Whether ’re working reports, policy documents, news articles approach can quickly help summarize categorize information analysis. Additionally, refining prompts leveraging schema validation, can expand use workflow handle various types structured data, extracting key insights legal documents, patents, even pictures historical documents— anywhere need structured information unstructured text.","code":""},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm-synthetic-data.html","id":"implementing-an-information-treatment","dir":"Articles","previous_headings":"","what":"Implementing an information treatment","title":"Generate Synthetic Data with tidyllm","text":"next step implement generate_synthetic_infotreatment() function designed simulate respondents might update answers receiving new information, known information treatment. example, treatment based study reports high exposure legal professionals automation generative AI. function takes two arguments: conversation, represents ongoing interaction LLM questionaire based responses generate_synthetic_answers(), treated, boolean flag indicating whether respondent receives information treatment. function starts retrieving synthetic respondent’s previous answers key questions output. provides AI automation-related prompt gauge respondent’s initial perception occupation’s automatable potential (“prior” belief). respondent treated group, receive additional information legal professionals’ AI exposure. presenting information, function prompts respondent reconsider initial answer, thus capturing “posterior” belief. Finally, function returns prior posterior beliefs along respondent’s demographic information, offering insights information treatment affects perceptions AI automation. simplified version function (without error-handling cleanup logic) might look like: lawyer familiar AI, low prior automatibility occupation, choose read info material update posterior downwards. Ironically, now lawyer ’ve fully “replaced” generative AI synthetic survey respondent seems believe job safe automation. now loop lawyer profile, make answer survey, add questions. basic setup generate synthetic data tidyllm","code":"generate_synthetic_infotreatment <- function(conversation, treated) {      # Extract key initial answers (gender, birth year, familiarity with AI)   answers_opener <- tibble(     gender      = get_reply(conversation, 1),     birth_year  = get_reply(conversation, 2),     ai_familiar = get_reply(conversation, 3)   )      # Ask the prior belief question (before treatment)   prior <- conversation |>     llm_message(\"Among all occupations, how automatable do you think is your occupation?                                      0 = Not Automatable                    1 = Among the 10 lowest percent                   2 = Among the 20 lowest percent                   3 = Among the 30 lowest percent                   4 = Among the 40 lowest percent                   5 = Right in the middle                   6 = Among the top 40 percent                   7 = Among the top 30 percent                   8 = Among the top 20 percent                   9 = Among the top 10 percent                   10 = At the very top                   99 = Prefer not to say                   \") |>     ollama(.model = \"gemma2\")      # Extract the prior answer (belief before the treatment)   prior_answer <- prior |> last_reply() |> str_squish()      # Default to use the conversation state of the prior answer for the untreated group   post_treatment <- prior      # Initialize the info-updating variable (0 means no treatment)   info_updating <- \"0\"      # Apply the information treatment if the treated flag is TRUE   if (treated) {      post_treatment <- prior |>       llm_message(\"A recent study titled *Occupational, Industry, and Geographic Exposure to Artificial Intelligence* by Ed Felten (Princeton), Manav Raj (University of Pennsylvania), and Robert Seamans (New York University) identified legal professionals, including lawyers and judges, as some of the occupations with the highest exposure to AI technologies.   According to the study, legal professionals are among the top 20 among 774 occupations most exposed to generative AI, suggesting that tasks traditionally performed by lawyers, such as legal research and document review, could be increasingly automated in the coming years.  Have you read this information? 1 = YES 2 = NO 99 = Prefer not to say \") |>       ollama(.model = \"gemma2\")          # Update info-updating based on whether the participant confirms reading the information     info_updating <- last_reply(post_treatment)   }      # Ask the posterior belief question (after treatment)   # Untreated ar also asked if they want to update   post_treatment |>     llm_message(\"Do you want to correct your previous answer? Which of these do you pick?                                      0 = Not Automatable                    1 = Among the 10 lowest percent                   2 = Among the 20 lowest percent                   3 = Among the 30 lowest percent                   4 = Among the 40 lowest percent                   5 = Right in the middle                   6 = Among the top 40 percent                   7 = Among the top 30 percent                   8 = Among the top 20 percent                   9 = Among the top 10 percent                   10 = At the very top                   99 = Prefer not to say                   \") |>     ollama(.model = \"gemma2\")      # Extract the posterior answer (belief after the treatment)   posterior_answer <- last_reply(post_treatment)      # Combine demographic data, prior and posterior beliefs, and info-updating status   answers_opener |>     mutate(prior = prior_answer,            info_updating = info_updating,            posterior = posterior_answer) }  #Let's generate this treatment under the assumption that our first example lawyer was treated profile1_info_treatment <- profile1_questionaire |>    generate_synthetic_infotreatment(treated = TRUE)  #Print the result tibble profile1_info_treatment #> # A tibble: 1 × 6 #>   gender birth_year ai_familiar prior info_updating posterior #>   <chr>  <chr>      <chr>       <chr> <chr>         <chr>     #> 1 \"1 \\n\" 1991       4           3     2             2"},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm-synthetic-data.html","id":"validity-and-limitations","dir":"Articles","previous_headings":"","what":"Validity and Limitations","title":"Generate Synthetic Data with tidyllm","text":"synthetic data LLMs offers valuable insights pretesting surveys, ’s important recognize limitations approach. LLM-generated responses approximations might miss nuances come real human respondents. instance, model might accurately reflect personal biases, experiences, diverse legal practices influence real lawyers’ perspectives automation. Additionally, AI models trained vast datasets, might overgeneralization, especially niche professions data (.e. divorce lawyers). Therefore, synthetic data can streamline early iterations survey design, complement, replace, actual human feedback later stages research.","code":""},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm-synthetic-data.html","id":"outlook","dir":"Articles","previous_headings":"","what":"Outlook","title":"Generate Synthetic Data with tidyllm","text":"Looking ahead, integration synthetic data generation tools like tidyllm traditional survey workflows offers exciting possibilities researchers. LLMs become advanced capable simulating nuanced human behaviors, accuracy synthetic responses likely improve. lead faster, efficient iterations survey design, enabling researchers refine questions test hypotheses diverse, simulated populations real-world deployment. Moreover, future advancements may allow greater customization synthetic respondents, capturing complex demographic variables behavioral patterns. instance, enhancing ability simulate specific professions, backgrounds, even emotional states, synthetic data evolve robust tool experimental pretesting fields beyond survey research, behavioral economics, political polling, educational assessment.","code":""},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm.html","id":"introduction-to-tidyllm","dir":"Articles","previous_headings":"","what":"Introduction to tidyllm","title":"Get Started","text":"tidyllm R package designed provide unified interface interacting various large language model APIs. vignette guide basic setup usage tidyllm.","code":""},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm.html","id":"installation","dir":"Articles","previous_headings":"Introduction to tidyllm","what":"Installation","title":"Get Started","text":"install tidyllm CRAN , use: install current development version directly GitHub using devtools:","code":"install.packages(\"tidyllm\") # Install devtools if not already installed if (!requireNamespace(\"devtools\", quietly = TRUE)) {   install.packages(\"devtools\") }  # Install TidyLLM from GitHub devtools::install_github(\"edubruell/tidyllm\")"},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm.html","id":"setting-up-api-keys-or-ollama","dir":"Articles","previous_headings":"Introduction to tidyllm > Installation","what":"Setting up API Keys or ollama","title":"Get Started","text":"using tidyllm, need set API keys services plan use. ’s set different providers: Claude models can get API key Anthropic Console: ChatGPT can obtain API key signing OpenAI set : Mistral can set API key Mistral console page set groq (confused grok) can setup API keys Groq Console: Alternatively, can set keys .Renviron file persistent storage. , execute usethis::edit_r_environ(), add line API key file, example: want work local large lange models via ollama need install official project website. Ollama sets local large language model server can use run open-source models devices.","code":"Sys.setenv(ANTHROPIC_API_KEY = \"YOUR-ANTHROPIC-API-KEY\") Sys.setenv(OPENAI_API_KEY = \"YOUR-OPENAI-API-KEY\") Sys.setenv(MISTRAL_API_KEY = \"MISTRAL-API-KEY-GOES-HERE\") Sys.setenv(GROQ_API_KEY = \"YOUR-GROQ-API-KEY\") ANTHROPIC_API_KEY=\"YOUR-ANTHROPIC-API-KEY\""},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm.html","id":"basic-usage","dir":"Articles","previous_headings":"Introduction to tidyllm > Installation","what":"Basic Usage","title":"Get Started","text":"Let’s start simple example using tidyllm interact different language models:","code":"library(tidyllm)  # Start a conversation with Claude conversation <- llm_message(\"What is the capital of France?\") |>   claude()  #Standard way that llm_messages are printed conversation ## Message History: ## system: You are a helpful assistant ## -------------------------------------------------------------- ## user: What is the capital of France? ## -------------------------------------------------------------- ## assistant: The capital of France is Paris. ## -------------------------------------------------------------- # Continue the conversation with ChatGPT conversation <- conversation |>   llm_message(\"What's a famous landmark in this city?\") |>   chatgpt()  last_reply(conversation) ## [1] \"A famous landmark in Paris is the Eiffel Tower.\""},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm.html","id":"sending-images-to-models","dir":"Articles","previous_headings":"Introduction to tidyllm > Installation","what":"Sending Images to Models","title":"Get Started","text":"tidyllm also supports sending images multimodal models. Let’s send picture : let ChatGPT guess picture made:","code":"# Describe an image using a llava model on ollama image_description <- llm_message(\"Describe this picture? Can you guess where it was made?\",                                  .imagefile = \"picture.jpeg\") |>   chatgpt(.model = \"gpt-4o\")  # Get the last reply last_reply(image_description) ## [1] \"The picture shows a beautiful landscape with a lake, mountains, and a town nestled below. The sun is shining brightly, casting a serene glow over the water. The area appears lush and green, with agricultural fields visible. \\n\\nThis type of scenery is reminiscent of northern Italy, particularly around Lake Garda, which features similar large mountains, picturesque water, and charming towns.\""},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm.html","id":"adding-pdfs-to-messages","dir":"Articles","previous_headings":"Introduction to tidyllm > Installation","what":"Adding PDFs to messages","title":"Get Started","text":"llm_message() function also supports extracting text PDFs including message. allows easily provide context PDF document interacting AI assistant. use feature, need pdftools package installed. already installed, can install : include text PDF prompt, simply pass file path .pdf argument chat function: package automatically extract text PDF include prompt sent API. text wrapped <pdf> tags clearly indicate content PDF:","code":"install.packages(\"pdftools\") llm_message(\"Please summarize the key points from the provided PDF document.\",       .pdf = \"die_verwandlung.pdf\") |>      chatgpt(.model = \"gpt-4o-mini\") ## Message History: ## system: You are a helpful assistant ## -------------------------------------------------------------- ## user: Please summarize the key points from the provided PDF document. ##  -> Attached Media Files:  die_verwandlung.pdf  ## -------------------------------------------------------------- ## assistant: Here are the key points from the provided PDF document 'Die Verwandlung' by Franz Kafka: ##  ## 1. The story centers around Gregor Samsa, who wakes up one morning to find that he has been transformed into a giant insect-like creature. ##  ## 2. Gregor's transformation causes distress and disruption for his family. They struggle to come to terms with the situation and how to deal with Gregor in his new state. ##  ## 3. Gregor's family, especially his sister Grete, initially tries to care for him, but eventually decides they need to get rid of him. They lock him in his room and discuss finding a way to remove him. ##  ## 4. Gregor becomes increasingly isolated and neglected by his family. He becomes weaker and less mobile due to his injuries and lack of proper care. ##  ## 5. Eventually, Gregor dies, and his family is relieved. They then begin to make plans to move to a smaller, more affordable apartment and start looking for new jobs and opportunities. ## -------------------------------------------------------------- Please summarize the key points from the provided PDF document.  <pdf filename=\"example_document.pdf\"> Extracted text from the PDF file... <\/pdf>"},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm.html","id":"sending-r-outputs-to-language-models","dir":"Articles","previous_headings":"Introduction to tidyllm > Installation","what":"Sending R Outputs to Language Models","title":"Get Started","text":"can automatically include R code outputs prompts. llm_message() optional argument .f can specify (anonymous) function, run console output captured appended message run . addition can use .caputre_plot send last plot pane model.  Now can send plot data summary language model:","code":"library(tidyverse) ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr     1.1.4     ✔ readr     2.1.5 ## ✔ forcats   1.0.0     ✔ stringr   1.5.1 ## ✔ ggplot2   3.5.1     ✔ tibble    3.2.1 ## ✔ lubridate 1.9.3     ✔ tidyr     1.3.1 ## ✔ purrr     1.0.2      ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag()    masks stats::lag() ## ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors # Create a plot for the mtcars example data ggplot(mtcars, aes(wt, mpg)) +   geom_point() +   geom_smooth(method = \"lm\", formula = 'y ~ x') +   labs(x=\"Weight\",y=\"Miles per gallon\") library(tidyverse) llm_message(\"Analyze this plot and data summary:\",                    .capture_plot = TRUE, #Send the plot pane to a model                   .f = ~{summary(mtcars)}) |> #Run summary(data) and send the output   claude() ## Message History: ## system: You are a helpful assistant ## -------------------------------------------------------------- ## user: Analyze this plot and data summary: ##  -> Attached Media Files:  file1568f6c1b4565.png, RConsole.txt  ## -------------------------------------------------------------- ## assistant: Based on the plot and data summary provided, here's an analysis: ##  ## 1. Relationship between Weight and MPG: ##    The scatter plot shows a clear negative correlation between weight (wt) and miles per gallon (mpg). As the weight of the car increases, the fuel efficiency (mpg) decreases. ##  ## 2. Linear Trend: ##    The blue line in the plot represents a linear regression fit. The downward slope confirms the negative relationship between weight and mpg. ##  ## 3. Data Distribution: ##    - The weight of cars in the dataset ranges from 1.513 to 5.424 (likely in thousands of pounds). ##    - The mpg values range from 10.40 to 33.90. ##  ## 4. Variability: ##    There's some scatter around the regression line, indicating that while weight is a strong predictor of mpg, other factors also influence fuel efficiency. ##  ## 5. Other Variables: ##    While not shown in the plot, the summary statistics provide information on other variables: ##    - Cylinder count (cyl) ranges from 4 to 8, with a median of 6. ##    - Horsepower (hp) ranges from 52 to 335, with a mean of 146.7. ##    - Transmission type (am) is binary (0 or 1), likely indicating automatic vs. manual. ##  ## 6. Model Fit: ##    The grey shaded area around the regression line represents the confidence interval. It widens at the extremes of the weight range, indicating less certainty in predictions for very light or very heavy vehicles. ##  ## 7. Outliers: ##    There are a few potential outliers, particularly at the lower and higher ends of the weight spectrum, that deviate from the general trend. ##  ## In conclusion, this analysis strongly suggests that weight is a significant factor in determining a car's fuel efficiency, with heavier cars generally having lower mpg. However, the presence of scatter in the data indicates that other factors (possibly related to engine characteristics, transmission type, or aerodynamics) also play a role in determining fuel efficiency. ## --------------------------------------------------------------"},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm.html","id":"getting-the-last-reply-as-raw-text-or-structured-data","dir":"Articles","previous_headings":"Introduction to tidyllm > Installation","what":"Getting the last reply (as raw text or structured data)","title":"Get Started","text":"can retrieve last assistant reply message history last_reply(). Typically, returns character vector text assistant’s reply. However, API functions requested replies JSON mode, can directly validate return structured output. function handles different response types automatically. JSON reply detected, returns list following fields: parsed_content: parsed JSON content (NULL case parsing errors). raw_response: direct string format reply. is_parsed: flag set TRUE JSON parsing successful, FALSE otherwise. can also force standard raw text replies, even JSON mode detected, using .raw argument.","code":""},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm.html","id":"example-1-getting-standard-text-replies","dir":"Articles","previous_headings":"Introduction to tidyllm > Installation > Getting the last reply (as raw text or structured data)","what":"Example 1: Getting standard text replies","title":"Get Started","text":"","code":"reply_text <- llm_message(\"Imagine a German adress.\") |>      groq() |>      last_reply() ## Message History: ## system: You are a helpful assistant ## -------------------------------------------------------------- ## user: Imagine a German adress. ## -------------------------------------------------------------- ## assistant: Let's imagine a German address.  ##  ## Herr Müller ## Musterstraße 12 ## 53111 Bonn ##  ## This address is formatted according to German conventions: ##  ## - 'Herr Müller' is the recipient's name (Mr. Müller). ## - 'Musterstraße 12' is the street name and number. ## - '53111 Bonn' is the postal code and city (Bonn). ## --------------------------------------------------------------"},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm.html","id":"example-2-getting-structured-replies-from-apis-in-json-mode","dir":"Articles","previous_headings":"Introduction to tidyllm > Installation > Getting the last reply (as raw text or structured data)","what":"Example 2: Getting structured replies from APIs in JSON mode","title":"Get Started","text":"API functions .json-argument enables JSON-mode. Note claude() explicit JSON-mode API-request need specify want JSON-output ideally shema prompt assistant.","code":"address <- llm_message('Imagine a German adress in JSON format. Reply only with JSON.')   address|>   ollama(.json = TRUE) |>  # API is asked to return JSON   last_reply()  str(address) ## List of 3 ##  $ raw_response  : chr \"{\\n   \\\"street\\\": \\\"Kurfürstenstraße\\\",\\n   \\\"houseNumber\\\": 23,\\n   \\\"postcode\\\": \\\"10785\\\",\\n   \\\"city\\\": \\\"B\"| __truncated__ ##  $ parsed_content:List of 6 ##   ..$ street     : chr \"Kurfürstenstraße\" ##   ..$ houseNumber: int 23 ##   ..$ postcode   : chr \"10785\" ##   ..$ city       : chr \"Berlin\" ##   ..$ region     : chr \"Berlin\" ##   ..$ country    : chr \"Deutschland\" ##  $ is_parsed     : logi TRUE"},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm.html","id":"api-parameters","dir":"Articles","previous_headings":"Introduction to tidyllm > Installation","what":"API parameters","title":"Get Started","text":"Different API functions support different model parameters like deterministic response via parameters like temperature. Please read API-documentation documentation model functions specific examples.","code":"temp_example <- llm_message(\"Explain how temperature parameters work in large language models  and why temperature 0 gives you deterministic outputs in one sentence.\")      #per default it is non-zero   temp_example |> ollama(.temperature=0) ## Message History: ## system: You are a helpful assistant ## -------------------------------------------------------------- ## user: Explain how temperature parameters work in large language models  and why temperature 0 gives you deterministic outputs in one sentence. ## -------------------------------------------------------------- ## assistant: In large language models, temperature parameters control the randomness of generated text by scaling the output probabilities, with higher temperatures introducing more uncertainty and lower temperatures favoring more likely outcomes; specifically, setting temperature to 0 effectively eliminates all randomness, resulting in deterministic outputs because it sets the probability of each token to its maximum likelihood value. ## -------------------------------------------------------------- #Retrying with .temperature=0   temp_example |> ollama(.temperature=0) ## Message History: ## system: You are a helpful assistant ## -------------------------------------------------------------- ## user: Explain how temperature parameters work in large language models  and why temperature 0 gives you deterministic outputs in one sentence. ## -------------------------------------------------------------- ## assistant: In large language models, temperature parameters control the randomness of generated text by scaling the output probabilities, with higher temperatures introducing more uncertainty and lower temperatures favoring more likely outcomes; specifically, setting temperature to 0 effectively eliminates all randomness, resulting in deterministic outputs because it sets the probability of each token to its maximum likelihood value. ## --------------------------------------------------------------"},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm.html","id":"streamning-back-responses-experimental","dir":"Articles","previous_headings":"Introduction to tidyllm > Installation","what":"Streamning back responses (Experimental)","title":"Get Started","text":"moment ollama(), chatgpt(), mistral() claude() support real-time streaming reply tokens console model works .stream=TRUE argument. feature offers slightly better feedback model behavior real-time, ’s particularly useful data-analysis workflows. consider feature experimental recommend using non-streaming responses production tasks. Note error handling streaming callbacks varies API differs quality time.","code":""},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm.html","id":"choosing-the-right-model-and-api","dir":"Articles","previous_headings":"Introduction to tidyllm > Installation","what":"Choosing the Right Model and API","title":"Get Started","text":"tidyllm supports multiple APIs, offering distinct large language models varying strengths. choice model API use often depends specific task, cost considerations, data privacy concerns. Claude (Anthropic API): Claude known generating thoughtful, nuanced responses, making ideal tasks require human-like reasoning, summarization creative writing. Calude Sonnet 3.5 currently one top-performing models many benchmarks. However, can sometimes verbose necessary, lacks direct JSON support, requires additional prompting validation ensure structured output. ChatGPT (OpenAI API): ChatGPT, particularly GPT-4o model, extremely versatile performs well across wide range tasks, including text generation, code completion, multimodal analysis. good cases, tends expensive alternatives, especially large-scale usage. Mistral (EU-based): Mistral offers lighter-weight, open-source models developed hosted EU, making particularly appealing data protection (e.g., GDPR compliance) concern. models may powerful GPT-4o Claude Sonnet, Mistral offers good performance standard text generation tasks. Groq (Fast): Groq offers unique advantage custom AI accelerator hardware, get fastest output available API. delivers high performance low costs, especially tasks require fast execution. hosts many strong open-source models, like lamma3:70b. ollama (Local Models): data privacy priority, running open-source models like gemma2::9B locally via ollama() gives full control model execution data. However, trade-local models require significant computational resources, often quite powerful large API-providers. ollama blog regularly posts new models advantages can download via ollama_download_model().","code":""},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm_classifiers.html","id":"a-common-classification-task","dir":"Articles","previous_headings":"","what":"A Common Classification Task","title":"Classifying Texts with tidyllm","text":"Imagine ’ve just collected thousands survey responses people describe jobs words. responses detailed, others vague, ’s plenty variation . Now, need categorize standardized occupation codes, like SOC classification system Bureau Labor Statistics. Manually sorting response take days, weeks, inconsistencies coders almost guaranteed. instance, dataset might look something like : 7,000 rows occupation descriptions, ranging “Librarian” “Making sure everything runs” goal classify messy responses one 22 2-digit occupation codes: article, take structured approach tackle classification task efficiently. ’s step--step workflow kind task: Pick Sample:Start filtering dataset retain distinct occupation descriptions. , randomly select sample distinct responses work . example, let’s select 20% dataset manual classification correction. Initial Classification: Use simple prompt categorize responses occupation codes. Manual Correction: Review correct classifications create reliable ground truth. Training/Test Split: split ground truth dataset using rsample training test sets. Experimentation: Test different prompts, models, parameters training set, comparing one-shot multi-shot approaches. Model Evaluation: Use yardstick find best-performing combination training data. Testing: Apply best-performing model test set evaluate well performs unseen occupation descriptions. Full Classification: Use validated model setup classify entire dataset efficiently.","code":"library(tidyllm) library(tidyverse) library(glue) occ_data <- read_rds(\"occupation_data.rds\") occ_data #> # A tibble: 7,000 × 2 #>    respondent occupation_open                   #>         <int> <chr>                             #>  1     100019 Ops oversight and strategy        #>  2     100266 Coordinating operations           #>  3     100453 Making sure everything runs       #>  4     100532 Building and demolition           #>  5     100736 Help lawyers with cases           #>  6     100910 I sell mechanical parts           #>  7     101202 Librarian                         #>  8     101325 Operations planning and execution #>  9     101329 Bookkeeper                        #> 10     101367 Kitchen staff                     #> # ℹ 6,990 more rows occ_codes <- read_rds(\"occ_codes_2digits.rds\") |>   print(n=Inf) #> # A tibble: 22 × 2 #>     occ2 occ_title                                                  #>    <dbl> <chr>                                                      #>  1    11 Management Occupations                                     #>  2    13 Business and Financial Operations Occupations              #>  3    15 Computer and Mathematical Occupations                      #>  4    17 Architecture and Engineering Occupations                   #>  5    19 Life, Physical, and Social Science Occupations             #>  6    21 Community and Social Service Occupations                   #>  7    23 Legal Occupations                                          #>  8    25 Educational Instruction and Library Occupations            #>  9    27 Arts, Design, Entertainment, Sports, and Media Occupations #> 10    29 Healthcare Practitioners and Technical Occupations         #> 11    31 Healthcare Support Occupations                             #> 12    33 Protective Service Occupations                             #> 13    35 Food Preparation and Serving Related Occupations           #> 14    37 Building and Grounds Cleaning and Maintenance Occupations  #> 15    39 Personal Care and Service Occupations                      #> 16    41 Sales and Related Occupations                              #> 17    43 Office and Administrative Support Occupations              #> 18    45 Farming, Fishing, and Forestry Occupations                 #> 19    47 Construction and Extraction Occupations                    #> 20    49 Installation, Maintenance, and Repair Occupations          #> 21    51 Production Occupations                                     #> 22    53 Transportation and Material Moving Occupations"},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm_classifiers.html","id":"classifying-a-sub-sample","dir":"Articles","previous_headings":"","what":"Classifying a Sub-Sample","title":"Classifying Texts with tidyllm","text":"start ensuring classify distinct responses. eliminates duplicates ensures efficient reliable classification process: help us focus variations across distinct occupations, avoiding repeated classification efforts identical responses. Next, divide distinct occupations sub-sample manual classification remaining portion used later. use initial_split() function rsample package, splitting 10% data smaller test set manual correction model training: splitting data, now smaller sub-sample 422 observations work initial classification stage.","code":"# Pick only distinct occupations from the dataset distinct_occupations <- occ_data |>    distinct(occupation = occupation_open)  print(distinct_occupations, n = 5) #> # A tibble: 2,209 × 1 #>   occupation                  #>   <chr>                       #> 1 Ops oversight and strategy  #> 2 Coordinating operations     #> 3 Making sure everything runs #> 4 Building and demolition     #> 5 Help lawyers with cases     #> # ℹ 2,204 more rows #Set a seed for reproducability set.seed(123)  # Split the distinct occupations into a sub-sample (10%) and the rest (90%) library(rsample) occ_split <- initial_split(distinct_occupations, prop = 0.8)  # Retrieve the sub-sample and the remaining data rest_of_data <- training(occ_split) sub_sample <- testing(occ_split)  print(sub_sample, n = 5)  #> # A tibble: 442 × 1 #>   occupation                      #>   <chr>                           #> 1 Making sure everything runs     #> 2 Bartender                       #> 3 Post-secondary health education #> 4 Food servin                     #> 5 Exectutive assistant            #> # ℹ 437 more rows"},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm_classifiers.html","id":"writing-an-initial-classifier-function","dir":"Articles","previous_headings":"Classifying a Sub-Sample","what":"Writing an initial classifier function","title":"Classifying Texts with tidyllm","text":"classify sub-sample occupation descriptions, use first function wraps llm_message(). function sends occupation description large language model (LLM) prompts assign one pre-defined occupation codes. first step, choose reliable commercial model make lives easier, requires fewer manual corrections due high-quality output, helping us build accurate ground truth: ’s Works: Prompt Generation: function creates detailed prompt instructs model classify occupation one 22 SOC codes. structured prompt ensures model provides clear accurate outputs. use glue() add function input (.e. response open occupation question) prompt {occupation}. Calling LLM: model (case, Claude-3.5-Sonnet) called using claude() function .temperature parameter set 0, ensuring deterministic (non-random) output. model’s reply retrieved cleaned using str_squish() remove unnecessary spaces: Error Handling: function uses tryCatch handle errors classification process. API call fails, function returns occupation code 97 flag issue. Validation Model Output: function checks whether model’s response valid SOC code. response invalid, assigns code 98. validation ensures unexpected outputs handled appropriately. Returning Result: function outputs tibble containing original occupation description model’s classification error code, providing clean structured result. typical function outputs look like : Next, apply classifier entire sub-sample one go using map_dfr() purrr package, efficiently generating tibble contains classification results occupation description. Running classifier sub-sample costs roughly 50 cents takes 12 minutes. ensure output correct, can export results Excel write_xlsx writexl package manually fix miss-classifications: manual review classifier’s output showed highly promising results. 443 classifications, 9 needed corrections, indicating error rate just 2% claude()-based classifier initial prompt. issues arose correctly identifying unclear responses missing, “Doin’ numbers” “Barster” (potential mix Barrister Barkeeper). Interestingly, 5 9 error cases, model returned invalid response (code 98) instead missing (code 99), suggests correctly avoided making incorrect guess, rather misclassifying occupations outright. likely proceed classifying rest dataset using claude()-based classifier, given strong performance, now solid ground truth work . allows us experiment different models prompts determine simpler alternatives small local models perform just well.","code":"classify_occupation <- function(occupation){   # Output what the model is currently doing to the console   glue(\"Classifying: {occupation}\\n\") |> cat(\"\\n\")      # Generate the prompt   prompt <- glue('       Classify this occupation response from a survey: {occupation}              Pick one of the following numerical codes from this list.        Respond only with the code!       11 = Management Occupations                                           13 = Business and Financial Operations Occupations                    15 = Computer and Mathematical Occupations                            17 = Architecture and Engineering Occupations                         19 = Life, Physical, and Social Science Occupations                   21 = Community and Social Service Occupations                         23 = Legal Occupations                                                25 = Educational Instruction and Library Occupations                  27 = Arts, Design, Entertainment, Sports, and Media Occupations       29 = Healthcare Practitioners and Technical Occupations               31 = Healthcare Support Occupations                                   33 = Protective Service Occupations                                   35 = Food Preparation and Serving Related Occupations                 37 = Building and Grounds Cleaning and Maintenance Occupations        39 = Personal Care and Service Occupations                            41 = Sales and Related Occupations                                    43 = Office and Administrative Support Occupations                    45 = Farming, Fishing, and Forestry Occupations                       47 = Construction and Extraction Occupations                          49 = Installation, Maintenance, and Repair Occupations                51 = Production Occupations                                           53 = Transportation and Material Moving Occupations       99 = Missing Occupation (No clear occupation)'   )      # List of valid codes as strings   valid_codes <- as.character(occ_codes$occ2)      # Initialize classification_raw   classification <- tryCatch({     # Get the assistant's reply     assistant_reply <- llm_message(prompt) |>       claude(.model = \"claude-3-5-sonnet-20240620\", .temperature = 0) |>       last_reply() |>       str_squish()          # Validate the assistant's reply     if (assistant_reply %in% valid_codes) {       as.integer(assistant_reply)     } else {       # If the reply is not a valid code, set code 98       98L     }   }, error = function(e){     # If there's an error with the model, set code 97     97L   })      # Output a tibble   tibble(     occupation_open = occupation,     occ2            = classification   ) } llm_message(prompt) |>       claude(.model = \"claude-3-5-sonnet-20240620\", .temperature = 0) |>       last_reply() |>       str_squish() classify_occupation(\"Software Engineer\") #> # A tibble: 1 × 2 #>   occupation_open    occ2 #>   <chr>             <dbl> #> 1 Software Engineer    15 sub_sample_classified <- sub_sample$occupation |>   map_dfr(classify_occupation) #> Classifying: Making sure everything runs #> Classifying: Bartender #> Classifying: Post-secondary health education #> Classifying: Food servin #> Classifying: Exectutive assistant #> Classifying: ... # Step 1: Bind the sub-sample classifications with descriptive occupation titles # We also include special codes for errors and invalid responses classified_with_titles <- sub_sample_classified |>   left_join(     occ_codes |>        # Add error codes and titles to the occupation code table       bind_rows(         tribble(           ~occ2, ~occ_title,           97, \"API-Connection Failure\",           98, \"Invalid Response\",           99, \"Missing (No Clear Occupation)\"         )       ),      by = \"occ2\"   )  # Step 2: Save the results to an Excel file for manual review # This allows us to inspect the classifications and any potential errors. writexl::write_xlsx(classified_with_titles, \"ground_truth_excel.xlsx\")"},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm_classifiers.html","id":"optimizing-and-testing-the-classifiers","dir":"Articles","previous_headings":"","what":"Optimizing and Testing the Classifiers","title":"Classifying Texts with tidyllm","text":"Now reliable ground-truth dataset, ’s time optimize experiment different model configurations. ’ll split dataset training test sets using rsample ensure can experiment different prompts setups training data evaluate final model performance unseen data.","code":"ground_truth <- readxl:::read_xlsx(\"ground_truth_corrected.xlsx\")  # Split the ground-truth into training and testing sets set.seed(123) gt_split <- initial_split(ground_truth, prop = 0.7)  # Retrieve training and testing data train_data <- training(gt_split) test_data  <- testing(gt_split)  print(train_data,n=5) #> # A tibble: 309 × 3 #>   occupation_open              occ2 occ_title                                    #>   <chr>                       <dbl> <chr>                                        #> 1 Computer network technician    15 Computer and Mathematical Occupations        #> 2 Educational support            25 Educational Instruction and Library Occupat… #> 3 Fine Carpentry                 47 Construction and Extraction Occupations      #> 4 Keep things organized          43 Office and Administrative Support Occupatio… #> 5 Group fitness instructor       39 Personal Care and Service Occupations        #> # ℹ 304 more rows"},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm_classifiers.html","id":"modifying-the-classifier-function","dir":"Articles","previous_headings":"Optimizing and Testing the Classifiers","what":"Modifying the Classifier Function","title":"Classifying Texts with tidyllm","text":"test different prompts models systematically need allow flexible classifier function can handle different prompts models. take prompt-building logic function allow different api-functions models function arguments:","code":"# External numerical code list for reusability numerical_code_list <- c('       11 = Management Occupations                                           13 = Business and Financial Operations Occupations                    15 = Computer and Mathematical Occupations                            ... abreviated ...       51 = Production Occupations                                           53 = Transportation and Material Moving Occupations       99 = Missing Occupation')  # Classification function that accepts prompt, api_function, and model  # as well as the ground truth to pass through as arguments classify_occupation_grid <- function(occupation,                                 occ2,                                 prompt,                                 prompt_id,                                 api_function,                                 model){   # Output what the model is currently doing to the console   glue(\"Classifying: {model} - {prompt_id} - {occupation}\\n\") |> cat(\"\\n\")      # List of valid codes as strings   valid_codes <- as.character(occ_codes$occ2)      # Initialize classification_raw   classification <- tryCatch({     # Get the assistant's reply using the dynamically provided API function and model     assistant_reply <- llm_message(prompt) |>       api_function(.model = model, .temperature = 0) |>       last_reply() |>       str_squish()          # Validate the assistant's reply     if (assistant_reply %in% valid_codes) {       as.integer(assistant_reply)     } else {       98L  # Return 98 for invalid responses     }   }, error = function(e){     97L  # Return 97 in case of an error (e.g., API failure)   })      # Return a tibble containing the original occupation description and classification result   tibble(     occupation_open = occupation,     occ2_predict    = classification,     occ2_truth      = occ2,     model           = model,     prompt_id       = prompt_id   ) }"},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm_classifiers.html","id":"defining-the-prompt-and-model-grid","dir":"Articles","previous_headings":"Optimizing and Testing the Classifiers","what":"Defining the Prompt and Model Grid","title":"Classifying Texts with tidyllm","text":"’ll define set prompts models want test. allow us apply classifier across different configurations compare results. ’s prompts models set : Prompts: Prompt 1: detailed prompt, asking model classify occupations warning make guesses. Prompt 2: Explicitly ask handle invalid responses returning special code (99) input resemble valid occupation. Prompt 3: shorter, concise version test whether model performs similarly less detailed instructions. Models: Llama3.2:3B: opensource large language model just 3 billion parameters fast, run locally via ollama() Gemma2:9B: Another candidate model, performs well classification tasks, double size Lama3.2 therefore somewhat slower. set grid combining prompts models. expand_grid() function tidyverse useful tool create every possible combination prompts models, use evaluate classifier: run classification across entire grid, use pmap_dfr() purrr package, allows us iterate multiple arguments simultaneously. combination occupation response, prompt, model passed classify_occupation() function, results concatenated single tibble: ⚠️ Note: Running extensive classification grid like , especially large datasets slow models, can take significant amount time. Therefore, ’s often reasonable save intermediate results periodically, don’t lose progress something goes wrong (e.g., crash network issue). combining pwalk() save_rds(), can run combination grid independently store results incrementally. run grid get insights well models prompts work train data. can experiment different models parameters much want see works .","code":"prompts <- tibble(prompt =           c( #Original prompt             'Classify this occupation response from a survey: {occupation}             Pick one of the following numerical codes from this list.              Respond only with the code!             {numerical_code_list}',             #Explicit instruction to avoid classifying something wrong            'Classify this occupation response from a survey: {occupation}             Pick one of the following numerical codes from this list.              Respond only with the code!             {numerical_code_list}                         If this does not look like a valid occupation response reply with              just 99            ',            #Shorter prompt            'Classify this occupation: {occupation}.             Respond only with one of the following codes:             {numerical_code_list}'          ),          prompt_id = 1:3)   grid <- expand_grid(train_data,                      prompts,                      model = c(\"llama3.2\", \"gemma2\")) |>   arrange(model) %>% # Arrange by model so ollama does not reload them often   rename(occupation = occupation_open) |>   rowwise() %>%  # Glue together prompts and occupation row-by-row   mutate(prompt = glue(prompt)) |>   ungroup() |> # Ungroup after the rowwise operation   select(model,occupation,occ2,prompt_id,prompt)  nrow(grid) #> [1] 1854 grid_results <- grid %>%   pmap_dfr(classify_occupation_grid,            api_function = ollama) #> Classifying: gemma2 - 1 - Computer network technician #> Classifying: gemma2 - 2 - Computer network technician #> Classifying: gemma2 - 3 - Computer network technician #> Classifying: ..."},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm_classifiers.html","id":"accuracy-estimates","dir":"Articles","previous_headings":"Optimizing and Testing the Classifiers","what":"Accuracy estimates","title":"Classifying Texts with tidyllm","text":"create overview prediction accuracy overview using yardstick package. many functions use, also need encode ground truth model predictions factors: First just just calculate plot model accuracy:  main insights classification experiment: Gemma2 consistently outperforms llama3.2, even gemma2’s top performance Prompt 1 (79.1) falls far short Claude Sonnet’s 98% accuracy initial run. simplified prompt clearly introduces challenges, especially smaller models like llama3.2, shows particularly poor performance (low 20.2% Prompt 3). suggests models might generalize well slightly varied less explicit prompts, whereas Claude able handle variations far greater ease. Let’s look confusion matrix top gemma2 performance see whether specific occupation category causes problems : lot miss-classifications management occupations office services, missing occupations classified well, classified occupations. nothing seems like immediate easy fix. Another thing worth trying cheap, simple fast chatgpt-4o mini models. Let’s try initial prompt Still worse accuracy Claude Sonnet much better gemma2. another trick goes beyond simple one-shot classification. common idea use multi-shot classifications (give model classification examples prompt, even guide output letting complete conversations answers right way). function df_llm_message() let’s easily built message histories put words mouth model. ways can help increase accuracy. popular one first let model reason detail best candidates classification build chain thought gives final answer.","code":"library(yardstick) #>  #> Attaching package: 'yardstick' #> The following object is masked from 'package:readr': #>  #>     spec  gr_factors <- grid_results |>   mutate(across(starts_with(\"occ2_\"),                 ~factor(.x,                         levels=c(occ_codes$occ2,97, 98,99),                         labels=c(occ_codes$occ_title,\"APIFAIL\",\"INVALID\",\"MISSING\")                 ))) accuracy <- gr_factors  |>   group_by(prompt_id, model) |>   accuracy(truth = occ2_truth, estimate = occ2_predict)  accuracy %>%   ggplot(aes(x = as.factor(prompt_id), y = .estimate, fill = model)) +   geom_bar(stat = \"identity\", position = \"dodge\") +   labs(title = \"Accuracy by Prompt and Model\", x = \"Prompt ID\", y = \"Accuracy\", fill=\"\") +   theme_bw(22) +   scale_y_continuous(labels = scales::label_percent(),limits = c(0,1)) +   scale_fill_brewer(palette = \"Set1\") conf_mat <- gr_factors |>   filter(prompt_id == 1, model == \"gemma2\") |>   conf_mat(truth = occ2_truth, estimate = occ2_predict)  # Autoplot the confusion matrix autoplot(conf_mat, type = \"heatmap\") +   scale_fill_gradient(low = \"white\", high = \"#E41A1C\") +   ggtitle(\"Confusion Matrix for Model gemma2, Prompt 1\") +   theme_bw(22) +   theme(axis.text.x = element_text(angle = 45, hjust = 1)) #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. chatgpt_grid <- grid |>   filter(model==\"gemma2\", prompt_id ==1) |>   mutate(model=\"gpt-4o-mini\") |>   pmap_dfr(classify_occupation_grid,            api_function = chatgpt)   chatgpt_grid %>%   mutate(across(starts_with(\"occ2_\"),                 ~factor(.x,                         levels=c(occ_codes$occ2,97, 98,99),                         labels=c(occ_codes$occ_title,\"APIFAIL\",\"INVALID\",\"MISSING\")                 )))  |>   accuracy(truth = occ2_truth, estimate = occ2_predict) #> # A tibble: 1 × 3 #>   .metric  .estimator .estimate #>   <chr>    <chr>          <dbl> #> 1 accuracy multiclass     0.767"},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm_classifiers.html","id":"multistep-chain-of-thought-prompting","dir":"Articles","previous_headings":"Optimizing and Testing the Classifiers","what":"Multistep Chain-of-Thought Prompting","title":"Classifying Texts with tidyllm","text":"need major change classification function, send two messages model. first one elicits reasoning step, second one asks final code, based answer first message. easily doable message-chaining abilities tidyllm modified chain thought prompting function reasoning output step: Note increases compute time gemma2 massively now produces multiple output . run produces roughly page reasoning, second question generates number, original prompting strategy just gave us one number. run-times increase strongly. Note: test complicated prompting strategies like realtime, another feature tidyllm comes handy. can stream back responses just like online chatbot interfaces first test run .stream-argument api functions immediately see whether made strange errors prompting. Therefore, pass stream arguement trough classify_occupation_cot() first test function prompting single inputs scale whole data. Let’s run function gemma2: work. turns case ! accuracy even worse without reasoning step! One interesting thing though model right classification one candidates reasoning step 94% cases. probably, still way get better final result gemma2: Yet likely worth invest time trying get good results local model use-case. Thus, looks like still clear favorite final choice without even needing testing. already know test performance Claude Sonnet 3.5 original split, since manually checked . Paying roughly 2,50$ Anthropic API use Claude Sonnet classify whole sample 98% accuracy probably worth case. final step use initial classify_occupation() based claude() data.","code":"classify_occupation_cot <- function(occupation,                                     occ2,                                     api_function,                                     model,                                     stream = FALSE){   # Output what the model is currently doing to the console   glue(\"Classifying with CoT: {model} - {occupation}\\n\") |> cat(\"\\n\")      # Step 1: Ask the model to think through the problem   prompt_reasoning <- glue('     Think about which of the following occupation codes would best describe this occupation description from a survey respondent: \"{occupation}\"          {numerical_code_list}          Explain your reasoning for the 3 top candidate codes step by step. Then evaluate which seems best.   ')      reasoning_response <- tryCatch({     conversation <<- llm_message(prompt_reasoning) |>       api_function(.model = model, .temperature = 0, .stream=stream)           conversation |>       last_reply()   }, error = function(e){     conversation <<- llm_message(\"Please classify this occupation response: {occupation}\")     \"Error in reasoning step.\"   })      # Step 2: Ask the model to provide the final answer   prompt_final <- glue('     Based on your reasoning, which code do you pick? Answer only with a numerical code!    ')      final_response <- tryCatch({     conversation |>     llm_message(prompt_final) |>       api_function(.model = model, .temperature = 0, .stream=stream) |>       last_reply() |>       str_squish()   }, error = function(e){     \"97\"   })      # Validate the model's final response   valid_codes <- as.character(occ_codes$occ2)      classification <- if (final_response %in% valid_codes) {     as.integer(final_response)   } else {     98L  # Return 98 for invalid responses   }      # Return a tibble containing the original occupation description and classification result   tibble(     occupation_open = occupation,     occ2_predict    = classification,     occ2_truth      = occ2,     model           = glue(\"{model}_cot\"),     reasoning       = reasoning_response,     final_response  = final_response   ) } results_cot <- grid |>   filter(model==\"gemma2\", prompt_id ==1) |>   select(-prompt,-prompt_id) |>   pmap_dfr(classify_occupation_cot,api_function = ollama, stream=FALSE) #> Classifying with CoT: gemma2 - 1 - Computer network technician #> Classifying with CoT: gemma2 - 2 - Educational support #> Classifying with CoT: gemma2 - 3 - Fine Carpentry #> Classifying with CoT: ... results_cot %>%   mutate(across(starts_with(\"occ2_\"),                 ~factor(.x,                         levels=c(occ_codes$occ2,97, 98,99),                         labels=c(occ_codes$occ_title,\"APIFAIL\",\"INVALID\",\"MISSING\")                 )))  %>%   accuracy(truth = occ2_truth, estimate = occ2_predict) #> # A tibble: 1 × 3 #>   .metric  .estimator .estimate #>   <chr>    <chr>          <dbl> #> 1 accuracy multiclass     0.754 results_cot %>%   rowwise() %>%   mutate(reasoning_classifications = str_extract_all(reasoning,\"\\\\d{2}\"),          reasoning_classifications = list(map_int(reasoning_classifications, as.integer)),          right_in_reasoning = occ2_truth %in% reasoning_classifications          ) %>%   ungroup() %>%   count(right_in_reasoning) %>%   mutate(pct=n/sum(n)) #> # A tibble: 2 × 3 #>   right_in_reasoning     n    pct #>   <lgl>              <int>  <dbl> #> 1 FALSE                 19 0.0615 #> 2 TRUE                 290 0.939"},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm_classifiers.html","id":"scaling-up-to-the-full-dataset","dir":"Articles","previous_headings":"","what":"Scaling Up to the Full Dataset","title":"Classifying Texts with tidyllm","text":"choose Claude 3.5 sonnet, original prompt apply data-set:","code":"full_occupations_classified <- distinct_occupations$occupation |>   map_dfr(classify_occupation)  occ_data |> left_join(full_occupations_classified, by=\"occupation_open\")"},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm_classifiers.html","id":"conclusion","dir":"Articles","previous_headings":"","what":"Conclusion","title":"Classifying Texts with tidyllm","text":"article, ’ve demonstrated tidyllm can effectively used tackle complex text classification tasks. walking process step--step, sample selection prompt design model testing optimization, ’ve highlighted flexibility package dealing real-world data.","code":""},{"path":"https://edubruell.github.io/tidyllm/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Eduard Brüll. Author, maintainer.","code":""},{"path":"https://edubruell.github.io/tidyllm/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Brüll E (2024). tidyllm: Tidy Integration Large Language Models. https://github.com/edubruell/tidyllm, https://edubruell.github.io/tidyllm/.","code":"@Manual{,   title = {tidyllm: Tidy Integration of Large Language Models},   author = {Eduard Brüll},   year = {2024},   note = {https://github.com/edubruell/tidyllm, https://edubruell.github.io/tidyllm/}, }"},{"path":"https://edubruell.github.io/tidyllm/index.html","id":"tidyllm-","dir":"","previous_headings":"","what":"Tidy Integration of Large Language Models","title":"Tidy Integration of Large Language Models","text":"tidyllm R package designed access various large language model APIs, including Claude, ChatGPT, Groq, Mistral, local models via Ollama. Built simplicity functionality, helps generate text, analyze media, integrate model feedback data workflows ease.","code":""},{"path":"https://edubruell.github.io/tidyllm/index.html","id":"features","dir":"","previous_headings":"","what":"Features","title":"Tidy Integration of Large Language Models","text":"Multiple Model Support: Seamlessly switch various model providers like Claude, ChatGPT, Groq, Mistral Ollama using best offer. Media Handling: Extract process text PDFs capture console outputs messaging. Upload imagefiles last plotpane multimodal models. Interactive Messaging History: Manage ongoing conversation models, maintaining structured history messages media interactions, automatically formatted API Stateful handling rate limits: API rate limits handled statefully within R Session API functions can wait automatically rate limits reset Tidy Workflow: Use R’s functional programming features side-effect-free, pipeline-oriented operation style.","code":""},{"path":"https://edubruell.github.io/tidyllm/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Tidy Integration of Large Language Models","text":"install tidyllm CRAN, use: development version GitHub:","code":"install.packages(\"tidyllm\") # Install devtools if not already installed if (!requireNamespace(\"devtools\", quietly = TRUE)) {   install.packages(\"devtools\") } devtools::install_github(\"edubruell/tidyllm\")"},{"path":"https://edubruell.github.io/tidyllm/index.html","id":"basic-example","dir":"","previous_headings":"","what":"Basic Example","title":"Tidy Integration of Large Language Models","text":"’s quick example using tidyllm describe image using Claude model follow local open-source models: examples advanced usage, check Get Started vignette. Please note: use tidyllm, need either installation ollama active API key one supported providers (e.g., Claude, ChatGPT). See Get Started vignette setup instructions.","code":"library(\"tidyllm\")  # Describe an image with  claude conversation <- llm_message(\"Describe this image\",                                .imagefile = here(\"image.png\")) |>   claude()  # Use the description to query further with groq conversation |>   llm_message(\"Based on the previous description,   what could the research in the figure be about?\") |>   ollama(.model = \"gemma2\")"},{"path":"https://edubruell.github.io/tidyllm/index.html","id":"learn-more","dir":"","previous_headings":"","what":"Learn More","title":"Tidy Integration of Large Language Models","text":"detailed instructions advanced features, see: Get Started tidyllm Changelog Documentation Classifying Texts tidyllm Structured Question Answering PDFs Generate Synthetic Survey Data","code":""},{"path":"https://edubruell.github.io/tidyllm/index.html","id":"contributing","dir":"","previous_headings":"","what":"Contributing","title":"Tidy Integration of Large Language Models","text":"welcome contributions! Feel free open issues submit pull requests GitHub.","code":""},{"path":"https://edubruell.github.io/tidyllm/index.html","id":"license","dir":"","previous_headings":"","what":"License","title":"Tidy Integration of Large Language Models","text":"project licensed MIT License - see LICENSE file details.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":null,"dir":"Reference","previous_headings":"","what":"Large Language Model Message Class — LLMMessage","title":"Large Language Model Message Class — LLMMessage","text":"Large Language Model Message Class Large Language Model Message Class","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Large Language Model Message Class — LLMMessage","text":"class manages history messages media interactions intended use large language  models. allows adding messages, converting messages API usage, printing history structured format.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"Large Language Model Message Class — LLMMessage","text":"message_history List store message interactions. system_prompt system prompt used conversation","code":""},{"path":[]},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Large Language Model Message Class — LLMMessage","text":"LLMMessage$new() LLMMessage$clone_deep() LLMMessage$add_message() LLMMessage$to_api_format() LLMMessage$has_image() LLMMessage$remove_message() LLMMessage$print() LLMMessage$clone()","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Large Language Model Message Class — LLMMessage","text":"Initializes LLMMessage object optional system prompt.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Large Language Model Message Class — LLMMessage","text":"","code":"LLMMessage$new(system_prompt = \"You are a helpful assistant\")"},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Large Language Model Message Class — LLMMessage","text":"system_prompt string sets initial system prompt.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Large Language Model Message Class — LLMMessage","text":"new LLMMessage object. Deep Clone LLMMessage Object method creates deep copy LLMMessage object. ensures internal states, including message histories settings, copied original object remains unchanged mutations applied copy. particularly useful maintaining immutability tidyverse-like functional programming context functions side effects inputs.","code":""},{"path":[]},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Large Language Model Message Class — LLMMessage","text":"","code":"LLMMessage$clone_deep()"},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"Large Language Model Message Class — LLMMessage","text":"new LLMMessage object deep copy original. Add message Adds message history. Optionally includes media.","code":""},{"path":[]},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Large Language Model Message Class — LLMMessage","text":"","code":"LLMMessage$add_message(role, content, media = NULL, json = FALSE)"},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Large Language Model Message Class — LLMMessage","text":"role role message sender (e.g., \"user\", \"assistant\"). content textual content message. media Optional; media content attach message. json message raw string contains json response? Convert API format Converts message history format suitable various API calls.","code":""},{"path":[]},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Large Language Model Message Class — LLMMessage","text":"","code":"LLMMessage$to_api_format(api_type, cgpt_image_detail = \"auto\")"},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Large Language Model Message Class — LLMMessage","text":"api_type type API (e.g., \"claude\",\"groq\",\"chatgpt\"). cgpt_image_detail Specific option ChatGPT API (imagedetail - set auto)","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"returns-2","dir":"Reference","previous_headings":"","what":"Returns","title":"Large Language Model Message Class — LLMMessage","text":"message history target API format Simple helper function determine whether message history contains image check function whenever call models support images can post warning user images found sent model","code":""},{"path":[]},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"usage-4","dir":"Reference","previous_headings":"","what":"Usage","title":"Large Language Model Message Class — LLMMessage","text":"","code":"LLMMessage$has_image()"},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"returns-3","dir":"Reference","previous_headings":"","what":"Returns","title":"Large Language Model Message Class — LLMMessage","text":"Returns TRUE message hisotry contains images Remove Message Index Removes message message history specified index.","code":""},{"path":[]},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"usage-5","dir":"Reference","previous_headings":"","what":"Usage","title":"Large Language Model Message Class — LLMMessage","text":"","code":"LLMMessage$remove_message(index)"},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"Large Language Model Message Class — LLMMessage","text":"index positive integer indicating position message remove.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"returns-4","dir":"Reference","previous_headings":"","what":"Returns","title":"Large Language Model Message Class — LLMMessage","text":"LLMMessage object, invisibly.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"method-print-","dir":"Reference","previous_headings":"","what":"Method print()","title":"Large Language Model Message Class — LLMMessage","text":"Prints current message history structured format.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"usage-6","dir":"Reference","previous_headings":"","what":"Usage","title":"Large Language Model Message Class — LLMMessage","text":"","code":"LLMMessage$print()"},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Large Language Model Message Class — LLMMessage","text":"objects class cloneable method.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"usage-7","dir":"Reference","previous_headings":"","what":"Usage","title":"Large Language Model Message Class — LLMMessage","text":"","code":"LLMMessage$clone(deep = FALSE)"},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"arguments-4","dir":"Reference","previous_headings":"","what":"Arguments","title":"Large Language Model Message Class — LLMMessage","text":"deep Whether make deep clone.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/chatgpt.html","id":null,"dir":"Reference","previous_headings":"","what":"Call the OpenAI API to interact with ChatGPT or o-reasoning models — chatgpt","title":"Call the OpenAI API to interact with ChatGPT or o-reasoning models — chatgpt","text":"Call OpenAI API interact ChatGPT o-reasoning models","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/chatgpt.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Call the OpenAI API to interact with ChatGPT or o-reasoning models — chatgpt","text":"","code":"chatgpt(   .llm,   .model = \"gpt-4o\",   .max_tokens = 1024,   .temperature = NULL,   .top_p = NULL,   .top_k = NULL,   .frequency_penalty = NULL,   .presence_penalty = NULL,   .api_url = \"https://api.openai.com/\",   .timeout = 60,   .verbose = FALSE,   .wait = TRUE,   .json = FALSE,   .min_tokens_reset = 0L,   .stream = FALSE,   .dry_run = FALSE )"},{"path":"https://edubruell.github.io/tidyllm/reference/chatgpt.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Call the OpenAI API to interact with ChatGPT or o-reasoning models — chatgpt","text":".llm  LLMMessage object. .model model identifier (default: \"gpt-4o\"). .max_tokens maximum number tokens generate (default: 1024). .temperature Control randomness response generation (optional). .top_p Nucleus sampling parameter (optional). .top_k Top k sampling parameter (optional). .frequency_penalty Controls repetition frequency (optional). .presence_penalty Controls much penalize repeating content (optional) .api_url Base URL API (default: https://api.openai.com/v1/completions). .timeout Request timeout seconds (default: 60). .verbose additional information shown API call .wait wait rate limits necessary? .json output JSON  mode (default: FALSE). .min_tokens_reset many tokens remaining wait wait token reset? .stream Stream back response piece piece (default: FALSE). .dry_run TRUE, perform dry run return request object.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/chatgpt.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Call the OpenAI API to interact with ChatGPT or o-reasoning models — chatgpt","text":"Returns updated LLMMessage object.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/claude.html","id":null,"dir":"Reference","previous_headings":"","what":"Call the Anthropic API to interact with Claude models — claude","title":"Call the Anthropic API to interact with Claude models — claude","text":"Call Anthropic API interact Claude models","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/claude.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Call the Anthropic API to interact with Claude models — claude","text":"","code":"claude(   .llm,   .model = \"claude-3-5-sonnet-20240620\",   .max_tokens = 1024,   .temperature = NULL,   .top_k = NULL,   .top_p = NULL,   .metadata = NULL,   .stop_sequences = NULL,   .tools = NULL,   .api_url = \"https://api.anthropic.com/\",   .verbose = FALSE,   .wait = TRUE,   .min_tokens_reset = 0L,   .timeout = 60,   .json = FALSE,   .stream = FALSE,   .dry_run = FALSE )"},{"path":"https://edubruell.github.io/tidyllm/reference/claude.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Call the Anthropic API to interact with Claude models — claude","text":".llm LLMMessage object. .model model identifier (default: \"claude-3-5-sonnet-20240620\"). .max_tokens maximum number tokens generate (default: 1024). .temperature Control randomness response generation (optional). .top_k Top k sampling parameter (optional). .top_p Nucleus sampling parameter (optional). .metadata Additional metadata request (optional). .stop_sequences Sequences stop generation (optional). .tools Additional tools used model (optional). .api_url Base URL API (default: \"https://api.anthropic.com/v1/messages\"). .verbose additional information shown API call .wait wait rate limits necessary? .min_tokens_reset many tokens remaining wait wait token reset? .timeout Request timeout seconds (default: 60). .json output JSON  (default: FALSE). .stream Stream back response piece piece (default: FALSE). .dry_run TRUE, perform dry run return request object.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/claude.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Call the Anthropic API to interact with Claude models — claude","text":"Returns updated LLMMessage object.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/df_llm_message.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert a Data Frame to an LLMMessage Object — df_llm_message","title":"Convert a Data Frame to an LLMMessage Object — df_llm_message","text":"function takes data frame converts LLMMessage object representing conversation history. data frame contain specific columns (role content) row representing message conversation.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/df_llm_message.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert a Data Frame to an LLMMessage Object — df_llm_message","text":"","code":"df_llm_message(.df)"},{"path":"https://edubruell.github.io/tidyllm/reference/df_llm_message.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert a Data Frame to an LLMMessage Object — df_llm_message","text":".df data frame least two rows columns role content. column role contain values \"user\", \"assistant\", \"system\", content type character.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/df_llm_message.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert a Data Frame to an LLMMessage Object — df_llm_message","text":"LLMMessage object containing structured messages per input data frame.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/generate_callback_function.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate API-Specific Callback Function for Streaming Responses — generate_callback_function","title":"Generate API-Specific Callback Function for Streaming Responses — generate_callback_function","text":"function generates callback function processes streaming responses different language model APIs. callback function specific API provided (claude, ollama, \"mistral\", chatgpt) processes incoming data streams, printing content console updating global environment use.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/generate_callback_function.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate API-Specific Callback Function for Streaming Responses — generate_callback_function","text":"","code":"generate_callback_function(.api)"},{"path":"https://edubruell.github.io/tidyllm/reference/generate_callback_function.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate API-Specific Callback Function for Streaming Responses — generate_callback_function","text":".api character string indicating API type. Supported values \"claude\", \"ollama\", \"mistral\", \"chatgpt\".","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/generate_callback_function.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate API-Specific Callback Function for Streaming Responses — generate_callback_function","text":"function serves callback handle streaming responses specified API. callback function processes raw data, updates .tidyllm_stream_env$stream object, prints streamed content console. function returns TRUE streaming continue, FALSE streaming finished.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/generate_callback_function.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generate API-Specific Callback Function for Streaming Responses — generate_callback_function","text":"Claude API: function processes event data lines, handles message_start message_stop events control streaming flow. Ollama API: function directly parses stream content JSON extracts message$content field. ChatGPT API: function handles JSON data streams processes content deltas. stops processing [DONE] message encountered. Mistral API: function similar ChatGPT callback function. stops processing [DONE] message encountered.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/get_reply.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve an Assistant Reply by Index — get_reply","title":"Retrieve an Assistant Reply by Index — get_reply","text":"Extracts content assistant's reply LLMMessage object specific index. function can handle replies expected JSON format attempting parse . parsing fails user opts raw text, function gracefully return original content.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/get_reply.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve an Assistant Reply by Index — get_reply","text":"","code":"get_reply(.llm, .index = NULL, .raw = FALSE)"},{"path":"https://edubruell.github.io/tidyllm/reference/get_reply.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve an Assistant Reply by Index — get_reply","text":".llm LLMMessage object containing history messages exchanged assistant. parameter must valid LLMMessage object; otherwise, function throw error. .index positive integer indicating assistant reply retrieve. Defaults NULL, retrieves last reply. .raw logical value indicating whether return raw text even message marked JSON. Defaults FALSE, meaning function attempt parse JSON.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/get_reply.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieve an Assistant Reply by Index — get_reply","text":"Returns content assistant's reply specified index, based following conditions: assistant replies, NULL returned. reply marked JSON parsing successful, list containing: parsed_content: parsed JSON content. raw_response: original raw content. json: flag indicating successful JSON parsing (TRUE). JSON parsing fails, list containing: parsed_content: NULL. raw_response: original raw content. json: FALSE. .raw TRUE message marked JSON, returns raw text content directly.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/get_user_message.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve a User Message by Index — get_user_message","title":"Retrieve a User Message by Index — get_user_message","text":"Extracts content user's message LLMMessage object specific index.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/get_user_message.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve a User Message by Index — get_user_message","text":"","code":"get_user_message(.llm, .index = NULL)"},{"path":"https://edubruell.github.io/tidyllm/reference/get_user_message.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve a User Message by Index — get_user_message","text":".llm LLMMessage object. .index positive integer indicating user message retrieve. Defaults NULL, retrieves last message.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/get_user_message.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieve a User Message by Index — get_user_message","text":"Returns content user's message specified index. messages found, returns NULL.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/groq.html","id":null,"dir":"Reference","previous_headings":"","what":"Call the Groq API to interact with fast opensource models on Groq — groq","title":"Call the Groq API to interact with fast opensource models on Groq — groq","text":"Call Groq API interact fast opensource models Groq","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/groq.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Call the Groq API to interact with fast opensource models on Groq — groq","text":"","code":"groq(   .llm,   .model = \"llama-3.2-11b-vision-preview\",   .max_tokens = 1024,   .temperature = NULL,   .top_p = NULL,   .frequency_penalty = NULL,   .presence_penalty = NULL,   .api_url = \"https://api.groq.com/\",   .json = FALSE,   .timeout = 60,   .verbose = FALSE,   .wait = TRUE,   .min_tokens_reset = 0L,   .dry_run = FALSE )"},{"path":"https://edubruell.github.io/tidyllm/reference/groq.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Call the Groq API to interact with fast opensource models on Groq — groq","text":".llm existing LLMMessage object. .model model identifier (default: \"llama-3.2-11b-vision-preview\"). .max_tokens maximum number tokens generate (default: 1024). .temperature Control randomness response generation (optional). .top_p Nucleus sampling parameter (optional). .frequency_penalty Controls repetition frequency (optional). .presence_penalty Controls much penalize repeating content (optional) .api_url Base URL API (default: \"https://api.anthropic.com/v1/messages\"). .json output structured JSON  (default: FALSE). .timeout Request timeout seconds (default: 60). .verbose additional information shown API call .wait wait rate limits necessary? .min_tokens_reset many tokens remaining wait wait token reset? .dry_run TRUE, perform dry run return request object.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/groq.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Call the Groq API to interact with fast opensource models on Groq — groq","text":"Returns updated LLMMessage object.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/initialize_api_env.html","id":null,"dir":"Reference","previous_headings":"","what":"Initialize or Retrieve API-specific Environment — initialize_api_env","title":"Initialize or Retrieve API-specific Environment — initialize_api_env","text":"function initializes named environment storing rate limit information specific API. ensures API's rate limit data stored separately.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/initialize_api_env.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Initialize or Retrieve API-specific Environment — initialize_api_env","text":"","code":"initialize_api_env(.api_name)"},{"path":"https://edubruell.github.io/tidyllm/reference/initialize_api_env.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Initialize or Retrieve API-specific Environment — initialize_api_env","text":".api_name name API initialize retrieve environment","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/last_reply.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve the Last Assistant Reply — last_reply","title":"Retrieve the Last Assistant Reply — last_reply","text":"wrapper around get_reply() retrieve recent assistant reply.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/last_reply.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve the Last Assistant Reply — last_reply","text":"","code":"last_reply(.llm, .raw = FALSE)"},{"path":"https://edubruell.github.io/tidyllm/reference/last_reply.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve the Last Assistant Reply — last_reply","text":".llm LLMMessage object. .raw logical value indicating whether return raw text even message marked JSON. Defaults FALSE, meaning function attempt parse JSON.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/last_reply.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieve the Last Assistant Reply — last_reply","text":"Returns content assistant's reply specified index, based following conditions: assistant replies, NULL returned. reply marked JSON parsing successful, list containing: parsed_content: parsed JSON content. raw_response: original raw content. json: flag indicating successful JSON parsing (TRUE). JSON parsing fails, list containing: parsed_content: NULL. raw_response: original raw content. json: FALSE. .raw TRUE message marked JSON, returns raw text content directly.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/last_user_message.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve the Last User Message — last_user_message","title":"Retrieve the Last User Message — last_user_message","text":"wrapper around get_user_message() retrieve recent user message.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/last_user_message.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve the Last User Message — last_user_message","text":"","code":"last_user_message(.llm)"},{"path":"https://edubruell.github.io/tidyllm/reference/last_user_message.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve the Last User Message — last_user_message","text":".llm LLMMessage object.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/last_user_message.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieve the Last User Message — last_user_message","text":"content last user message.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/llm_message.html","id":null,"dir":"Reference","previous_headings":"","what":"Create or Update Large Language Model Message Object — llm_message","title":"Create or Update Large Language Model Message Object — llm_message","text":"function allows creation new LLMMessage object updating existing one. can handle addition text prompts various media types images, PDFs, text files, plots. function includes input validation ensure provided parameters correct format.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/llm_message.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create or Update Large Language Model Message Object — llm_message","text":"","code":"llm_message(   .llm = NULL,   .prompt = NULL,   .role = \"user\",   .system_prompt = \"You are a helpful assistant\",   .imagefile = NULL,   .pdf = NULL,   .textfile = NULL,   .capture_plot = FALSE,   .f = NULL )"},{"path":"https://edubruell.github.io/tidyllm/reference/llm_message.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create or Update Large Language Model Message Object — llm_message","text":".llm existing LLMMessage object initial text prompt. .prompt Text prompt add message history. .role role message sender, typically \"user\" \"assistant\". .system_prompt Default system prompt new LLMMessage needs created. .imagefile Path image file attached (optional). .pdf Path PDF file attached (optional). Can character vector length one (file path), list filename, start_page, end_page. .textfile Path text file read attached (optional). .capture_plot Boolean indicate whether plot captured attached image (optional). .f R function object coercible function via rlang::as_function, whose output captured attached (optional).","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/llm_message.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create or Update Large Language Model Message Object — llm_message","text":"Returns updated new LLMMessage object.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/mistral.html","id":null,"dir":"Reference","previous_headings":"","what":"Send LLMMessage to Mistral API — mistral","title":"Send LLMMessage to Mistral API — mistral","text":"Send LLMMessage Mistral API","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/mistral.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Send LLMMessage to Mistral API — mistral","text":"","code":"mistral(   .llm,   .model = \"mistral-large-latest\",   .stream = FALSE,   .seed = NULL,   .json = FALSE,   .temperature = NULL,   .timeout = 120,   .wait = TRUE,   .min_tokens_reset = 0L,   .max_tokens = 1024,   .min_tokens = NULL,   .dry_run = FALSE,   .verbose = FALSE )"},{"path":"https://edubruell.github.io/tidyllm/reference/mistral.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Send LLMMessage to Mistral API — mistral","text":".llm LLMMessage object. .model model identifier (default: \"mistral-large-latest\"). .stream answer streamed console comes (optional) .seed seed used random numbers  (optional). .json output structured JSON  (default: FALSE). .temperature Control randomness response generation (optional). .timeout connection time (default: 120 seconds). .wait wait rate limits necessary? .min_tokens_reset many tokens remaining wait wait token reset? .max_tokens Maximum number tokens response (default: 1024). .min_tokens Minimum number tokens response (optional). .dry_run TRUE, perform dry run return request object. .verbose additional information shown API call","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/mistral.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Send LLMMessage to Mistral API — mistral","text":"Returns updated LLMMessage object.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/mistral_embedding.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Embeddings Using Mistral API — mistral_embedding","title":"Generate Embeddings Using Mistral API — mistral_embedding","text":"Generate Embeddings Using Mistral API","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/mistral_embedding.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Embeddings Using Mistral API — mistral_embedding","text":"","code":"mistral_embedding(   .llm,   .model = \"mistral-7b\",   .mistral_api_key = Sys.getenv(\"MISTRAL_API_KEY\"),   .timeout = 120,   .dry_run = FALSE )"},{"path":"https://edubruell.github.io/tidyllm/reference/mistral_embedding.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Embeddings Using Mistral API — mistral_embedding","text":".llm existing LLMMessage object (character vector texts embed) .model embedding model identifier (default: \"mistral-7b\"). .mistral_api_key Mistral API key authentication. .timeout Timeout API request seconds (default: 120). .dry_run TRUE, perform dry run return request object.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/mistral_embedding.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Embeddings Using Mistral API — mistral_embedding","text":"matrix column corresponds embedding message message history.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/ollama.html","id":null,"dir":"Reference","previous_headings":"","what":"Send LLMMessage to ollama API — ollama","title":"Send LLMMessage to ollama API — ollama","text":"Send LLMMessage ollama API","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/ollama.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Send LLMMessage to ollama API — ollama","text":"","code":"ollama(   .llm,   .model = \"llama3\",   .stream = FALSE,   .seed = NULL,   .json = FALSE,   .temperature = NULL,   .num_ctx = 2048,   .ollama_server = \"http://localhost:11434\",   .timeout = 120,   .dry_run = FALSE )"},{"path":"https://edubruell.github.io/tidyllm/reference/ollama.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Send LLMMessage to ollama API — ollama","text":".llm existing LLMMessage object. .model model identifier (default: \"llama3\"). .stream answer streamed console comes (optional) .seed seed used random numbers  (optional). .json output structured JSON  (default: FALSE). .temperature Control randomness response generation (optional). .num_ctx size context window tokens (optional) .ollama_server URL ollama server used .timeout connection time .dry_run TRUE, perform dry run return request object.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/ollama.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Send LLMMessage to ollama API — ollama","text":"Returns updated LLMMessage object.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/ollama_download_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Download a model from the Ollama API — ollama_download_model","title":"Download a model from the Ollama API — ollama_download_model","text":"function sends request Ollama API download specified model. can operate streaming mode provides live updates download status progress, single response mode.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/ollama_download_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Download a model from the Ollama API — ollama_download_model","text":"","code":"ollama_download_model(.model, .ollama_server = \"http://localhost:11434\")"},{"path":"https://edubruell.github.io/tidyllm/reference/ollama_download_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Download a model from the Ollama API — ollama_download_model","text":".model name model download. .ollama_server base URL Ollama API (default \"http://localhost:11434\").","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/ollama_embedding.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Embeddings Using Ollama API — ollama_embedding","title":"Generate Embeddings Using Ollama API — ollama_embedding","text":"Generate Embeddings Using Ollama API","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/ollama_embedding.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Embeddings Using Ollama API — ollama_embedding","text":"","code":"ollama_embedding(   .llm,   .model = \"all-minilm\",   .truncate = TRUE,   .ollama_server = \"http://localhost:11434\",   .timeout = 120,   .dry_run = FALSE )"},{"path":"https://edubruell.github.io/tidyllm/reference/ollama_embedding.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Embeddings Using Ollama API — ollama_embedding","text":".llm existing LLMMessage object (charachter vector texts embed) .model embedding model identifier (default: \"-minilm\"). .truncate Whether truncate inputs fit model's context length (default: TRUE). .ollama_server URL Ollama server used (default: \"http://localhost:11434\"). .timeout Timeout API request seconds (default: 120). .dry_run TRUE, perform dry run return request object.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/ollama_embedding.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Embeddings Using Ollama API — ollama_embedding","text":"matrix column corresponds embedding message message history.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/ollama_list_models.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve and return model information from the Ollama API — ollama_list_models","title":"Retrieve and return model information from the Ollama API — ollama_list_models","text":"function connects Ollama API retrieves information available models, returning tibble.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/ollama_list_models.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve and return model information from the Ollama API — ollama_list_models","text":"","code":"ollama_list_models(.ollama_server = \"http://localhost:11434\")"},{"path":"https://edubruell.github.io/tidyllm/reference/ollama_list_models.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve and return model information from the Ollama API — ollama_list_models","text":".ollama_server URL ollama server used","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/ollama_list_models.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieve and return model information from the Ollama API — ollama_list_models","text":"tibble containing model information, NULL models found.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/openai_embedding.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Embeddings Using OpenAI API — openai_embedding","title":"Generate Embeddings Using OpenAI API — openai_embedding","text":"Generate Embeddings Using OpenAI API","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/openai_embedding.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Embeddings Using OpenAI API — openai_embedding","text":"","code":"openai_embedding(   .llm,   .model = \"text-embedding-3-small\",   .truncate = TRUE,   .openai_api_key = Sys.getenv(\"OPENAI_API_KEY\"),   .timeout = 120,   .dry_run = FALSE )"},{"path":"https://edubruell.github.io/tidyllm/reference/openai_embedding.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Embeddings Using OpenAI API — openai_embedding","text":".llm existing LLMMessage object (character vector texts embed) .model embedding model identifier (default: \"text-embedding-3-small\"). .truncate Whether truncate inputs fit model's context length (default: TRUE). .openai_api_key OpenAI API key authentication. .timeout Timeout API request seconds (default: 120). .dry_run TRUE, perform dry run return request object.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/openai_embedding.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Embeddings Using OpenAI API — openai_embedding","text":"matrix column corresponds embedding message message history.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/parse_duration_to_seconds.html","id":null,"dir":"Reference","previous_headings":"","what":"An internal function to parse the duration strings that OpenAI APIs return for ratelimit resets — parse_duration_to_seconds","title":"An internal function to parse the duration strings that OpenAI APIs return for ratelimit resets — parse_duration_to_seconds","text":"internal function parses duration strings returned OpenAI API","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/parse_duration_to_seconds.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"An internal function to parse the duration strings that OpenAI APIs return for ratelimit resets — parse_duration_to_seconds","text":"","code":"parse_duration_to_seconds(.duration_str)"},{"path":"https://edubruell.github.io/tidyllm/reference/parse_duration_to_seconds.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"An internal function to parse the duration strings that OpenAI APIs return for ratelimit resets — parse_duration_to_seconds","text":".duration_str duration string.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/parse_duration_to_seconds.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"An internal function to parse the duration strings that OpenAI APIs return for ratelimit resets — parse_duration_to_seconds","text":"numeric number seconds","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/pdf_page_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Batch Process PDF into LLM Messages — pdf_page_batch","title":"Batch Process PDF into LLM Messages — pdf_page_batch","text":"function processes PDF file page page. page, extracts text converts page image. creates list LLMMessage objects text image multimodal processing. Users can specify range pages process provide custom function generate prompts page.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/pdf_page_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Batch Process PDF into LLM Messages — pdf_page_batch","text":"","code":"pdf_page_batch(   .pdf,   .general_prompt,   .system_prompt = \"You are a helpful assistant\",   .page_range = NULL,   .prompt_fn = NULL )"},{"path":"https://edubruell.github.io/tidyllm/reference/pdf_page_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Batch Process PDF into LLM Messages — pdf_page_batch","text":".pdf Path PDF file. .general_prompt default prompt applied page .prompt_fn provided. .system_prompt Optional system prompt initialize LLMMessage (default \"helpful assistant\"). .page_range vector two integers specifying start end pages process. NULL, pages processed. .prompt_fn optional custom function generates prompt page. function takes page text input returns string. NULL, .general_prompt used pages.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/pdf_page_batch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Batch Process PDF into LLM Messages — pdf_page_batch","text":"list LLMMessage objects, containing text image page.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/perform_api_request.html","id":null,"dir":"Reference","previous_headings":"","what":"Perform an API request to interact with language models — perform_api_request","title":"Perform an API request to interact with language models — perform_api_request","text":"Perform API request interact language models","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/perform_api_request.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Perform an API request to interact with language models — perform_api_request","text":"","code":"perform_api_request(   .request,   .api,   .stream = FALSE,   .timeout = 60,   .parse_response_fn = NULL,   .dry_run = FALSE )"},{"path":"https://edubruell.github.io/tidyllm/reference/perform_api_request.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Perform an API request to interact with language models — perform_api_request","text":".request httr2 request object. .api API identifier (e.g., \"claude\", \"openai\"). .stream Stream response TRUE. .timeout Request timeout seconds. .parse_response_fn function parse assistant's reply. .dry_run TRUE, perform dry run return request object.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/perform_api_request.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Perform an API request to interact with language models — perform_api_request","text":"list containing assistant's reply response headers.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/rate_limit_info.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the current rate limit information for all or a specific API — rate_limit_info","title":"Get the current rate limit information for all or a specific API — rate_limit_info","text":"function retrieves rate limit details specified API, APIs stored .tidyllm_rate_limit_env API specified.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/rate_limit_info.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the current rate limit information for all or a specific API — rate_limit_info","text":"","code":"rate_limit_info(.api_name = NULL)"},{"path":"https://edubruell.github.io/tidyllm/reference/rate_limit_info.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the current rate limit information for all or a specific API — rate_limit_info","text":".api_name (Optional) name API whose rate limit info want print. provided, rate limit info APIs environment returned","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/rate_limit_info.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the current rate limit information for all or a specific API — rate_limit_info","text":"tibble containing rate limit information.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/tidyllm-package.html","id":null,"dir":"Reference","previous_headings":"","what":"tidyllm: Tidy Integration of Large Language Models — tidyllm-package","title":"tidyllm: Tidy Integration of Large Language Models — tidyllm-package","text":"tidy interface integrating large language model (LLM) APIs 'Claude', 'ChatGPT', 'Groq','Mistral' local models via 'Ollama' R workflows. package supports text media-based interactions, interactive message history, stateful rate limit handling, tidy, pipeline-oriented interface streamlined integration data workflows. Web services available https://www.anthropic.com, https://openai.com, https://groq.com, https://mistral.ai/ https://ollama.com.","code":""},{"path":[]},{"path":"https://edubruell.github.io/tidyllm/reference/tidyllm-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"tidyllm: Tidy Integration of Large Language Models — tidyllm-package","text":"Maintainer: Eduard Brüll eduard.bruell@zew.de","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/update_rate_limit.html","id":null,"dir":"Reference","previous_headings":"","what":"Update the standard API rate limit info in the hidden .tidyllm_rate_limit_env environment — update_rate_limit","title":"Update the standard API rate limit info in the hidden .tidyllm_rate_limit_env environment — update_rate_limit","text":"function initializes stores ratelimit information API functions future use","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/update_rate_limit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update the standard API rate limit info in the hidden .tidyllm_rate_limit_env environment — update_rate_limit","text":"","code":"update_rate_limit(.api_name, .response_object)"},{"path":"https://edubruell.github.io/tidyllm/reference/update_rate_limit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update the standard API rate limit info in the hidden .tidyllm_rate_limit_env environment — update_rate_limit","text":".api_name name API initialize retrieve environment. .response_object preparsed response object cotaining info remaining requests, tokens rest times","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/wait_rate_limit.html","id":null,"dir":"Reference","previous_headings":"","what":"Wait for ratelimit restore times to ellapse if necessary — wait_rate_limit","title":"Wait for ratelimit restore times to ellapse if necessary — wait_rate_limit","text":"function implements standardized wait rate limit resets","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/wait_rate_limit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Wait for ratelimit restore times to ellapse if necessary — wait_rate_limit","text":"","code":"wait_rate_limit(.api_name, .min_tokens_reset)"},{"path":"https://edubruell.github.io/tidyllm/reference/wait_rate_limit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Wait for ratelimit restore times to ellapse if necessary — wait_rate_limit","text":".api_name name API rate limit want wait .min_tokens_reset token boundary wish reset (Typically larger size message)","code":""},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"changelog-for-development-version-018","dir":"Changelog","previous_headings":"","what":"Changelog for Development Version 0.1.8","title":"Changelog for Development Version 0.1.8","text":"Changes since last CRAN Release 0.1.0","code":""},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"breaking-changes-compared-to-cran-release-0-1-8","dir":"Changelog","previous_headings":"","what":"Breaking Changes (Compared to CRAN Release 0.1.0)","title":"Changelog for Development Version 0.1.8","text":"last_reply() Changes: .json argument longer used, JSON replies automatically parsed. Use .raw raw text. Groq Models: System prompts longer sent Groq models, since many models groq support multimodal models groq allow .","code":""},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"new-features-0-1-8","dir":"Changelog","previous_headings":"","what":"New Features","title":"Changelog for Development Version 0.1.8","text":"Embedding Models Support: Embedding model support three APIs: Embedding functions process message histories combines text message content media attachements embedding models ollama_embedding() generate embeddings using Ollama API. openai_embedding() generate embeddings using OpenAI API. mistral_embedding() generate embeddings using Mistral API. Message Retrieval Functions: Added functions retrieve single messages conversations: last_user_message() pulls last message user sent get_reply() gets assistant reply given index assistant messages get_user_message() gets user message given index user messages Updated last_reply(): Now wrapper around get_reply() consistent behavior. New Ollama functions: Model Download: Introduced ollama_download_model() function download models Ollama API. supports streaming mode provides live progress bar updates download progress. PDF Page Batch Processing: Introduced pdf_page_batch() function, processes PDF files page page, extracting text converting page image allows general prompt page specific prompts. function generates list LLMMessage objects can sent API Support Mistral API: New mistral() function use Mistral Models Le Platforme servers hosted EU. rate-limiting streaming-support. PDF Page Support llm_message(): llm_message() function now supports specifying range pages PDF passing list filename, start_page, end_page. allows users extract process specific pages PDF, shown example :","code":"llm_message(     .prompt = \"Please summarize pages 2 to 5 of the attached document.\",     .pdf = list(       filename = \"path/to/your/document.pdf\",       start_page = 2,       end_page = 5     )   )"},{"path":[]},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"groq-support-for-vision-0-1-8","dir":"Changelog","previous_headings":"","what":"Groq support for vision","title":"Changelog for Development Version 0.1.8","text":"groq() function now supports images. Since modern models groq, especially ones multimodal abilities support system prompts, system role deleted groq api calls.","code":""},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"json-mode-improvements-0-1-8","dir":"Changelog","previous_headings":"","what":"JSON Mode Improvements","title":"Changelog for Development Version 0.1.8","text":"Since version 0.1.1, JSON mode now widely supported across API functions, allowing structured outputs APIs support . .json argument now passed API functions, specifying API respond, needed anymore last_reply(). Additionally, behavior reply functions changed. now automatically handle JSON replies parsing structured data falling back raw text case errors. can still force raw text replies even JSON output using .raw argument.","code":""},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"new-tests-for-api-functions-0-1-8","dir":"Changelog","previous_headings":"","what":"New tests for API functions","title":"Changelog for Development Version 0.1.8","text":"Easier Troubleshooting API-function: API functions now support .dry_run argument, allowing users generate httr2-request easier debugging inspection. API Function Tests: Implemented httptest2-based tests mock responses API functions, covering basic functionality rate-limiting.","code":""}]
