[{"path":"https://edubruell.github.io/tidyllm/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2024 Eduard Brüll Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm-pdfquestions.html","id":"what-is-json-and-json-mode","dir":"Articles","previous_headings":"","what":"What is JSON and JSON Mode?","title":"Structured Question Answering from PDFs","text":"JSON (JavaScript Object Notation) lightweight, text-based format representing structured data. ’s commonly used transmitting data server web application different parts system. JSON object consists key-value pairs, making ideal capturing structured data like title paper, authors, answers specific research questions. tidyllm, can use ability many large language models write answers JSON ensure AI model returns answers structured format directly R. particularly useful automating processes, extracting information multiple documents, allows easy conversion tables data manipulation.","code":""},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm-pdfquestions.html","id":"context-length","dir":"Articles","previous_headings":"","what":"Context length","title":"Structured Question Answering from PDFs","text":"working long documents research papers, reports, even entire books, one challenge arises much document model can handle . Large language models limitation known context length, refers maximum amount text can process single query. important document exceeds limit, model won’t able process entire content —leading missing sections incomplete answers. models, context length measured tokens (basic units text). example, maximum context length many smaller models 8192 tokens typically covers 30-35 pages text. means longer documents, like academic paper exceeds length, first portion document seen model, potentially omitting key sections like bibliography appendices, important citations results may reside. mitigate , common approach limit number pages sent model processing. example, workflow, restrict input first 35 pages, typically includes abstract, introduction, methodology, results, discussion—sections relevant summarizing paper. approach ensures model can process core content academic papers, mean information typically found later document missed. Alternatively, large documents, split smaller chunks process chunk separately. way, can cover entire content without exceeding model’s context window. However, keep mind might disrupt flow text make harder model retain overall context across chunks.","code":""},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm-pdfquestions.html","id":"example-workflow","dir":"Articles","previous_headings":"","what":"Example Workflow","title":"Structured Question Answering from PDFs","text":"Imagine folder looks something like —many downloaded papers, structure yet: can write simple function processes document passes model. demonstration purposes, send first 35 pages pdf model, even though gpt-4o can handle 128,000 tokens. feature primarily needed, large reports whole books larger typical models context window. Usually papers 35 pages provide model enough information reasonably answer main summarisation queries. want upload complete files though, possible larger models, just need specify file name .pdf argument string variable. example, use chatgpt() default gpt-4o model answer questions. 35-page paper, API cost approximately $0.08, making affordable process multiple papers different queries. See details pricing . also try open-source local models, paid remote models like gpt-4o typically handle longer documents much effectively. Smaller local models, gemma2:9B ollama(), often struggle extensive content multiple questions structured response even though context window large. use , might need reduce number processed pages simplify queries. addition, ollama() need increase context length ollama-default 2,048 tokens higher number ollama()-option .num_ctx. Since context length directly related memory usage, likely slower small context query. Alternatively, even larger local models like llama3::70B work complicated queries smaller models, require significant hardware resources run effectively. ⚠️ Note: using paid remote models, ’s important consider data privacy security, especially ’re processing sensitive documents, uploading data external servers may introduce risks — local models provide control cases. enabling .json = TRUE, instruct model return response JSON format. effectively extract key information multiple academic papers, also need consistent way prompt large language model. prompt use specifies details want document, title, authors, type paper, answers specific questions. example questions cover empirical methods, theoretical framework, main point, paper’s key contribution. can course ask specific questions can extract information even relevant use case. nice start. also give model exact schema need answers order make easier work results R. example output lengthy prompt one paper: output function seems quite daunting. close prompt, specified schema. structured three key components: parsed_content, raw_response, is_parsed. structure ensures always access model’s reply raw form structured R list: raw_response: Even response successfully parsed, ’s useful keep original raw output model. field stores unprocessed response returned LLM, can valuable debugging comparison purposes. models explicitly check whether return valid JSON. However, APIs without explicit JSON support like Anthropic,often add sentences like “Based document, requested information JSON format:” actual json. cases can parsed easy fix raw_response, stringr jsonlite::fromJSON(). However, printing raw response example gives us JSON structure asked : parsed_content: field contains structured response successfully parsed list R. allows directly access specific elements reply, title, authors, answers questions, making easy integrate results analysis save human readable document. example can directly get suggested file name response: is_parsed: logical flag indicating whether parsing successful. TRUE, means parsed_content available output conforms expected JSON structure. FALSE, parsed_content returned, can rely raw_response inspection manual handling. can now run function generate kind response papers folder: Let’s look files successfully parsed: Luckily, parse successfully. Now can proceed getting structured output response saving easily readable format. example name, author names,paper type answers questions together row tibble model output: can save summaries excel table can format manually better readability: Similarly can extract key references add separate Excel file. next step work output, go parsed content always pick key_citations map. example, encouter error : Looking cause see common problem JSON-based workflows. can see problem raw responses second paper parsed: key_citations put questions instead separate field. model completely adhere asked produce. quick fix problem check key_citations put :","code":"library(tidyverse) library(tidyllm) dir(\"aipapers\") #>  [1] \"2017_Brynjolfsson_etal_AI_Productivity_Paradox.pdf\"                                                #>  [2] \"2018_Autor_etal_AutomationLaborDisplacing.pdf\"                                                     #>  [3] \"2018_Felten_etal_AILinkOccupations.pdf\"                                                            #>  [4] \"2019_Acemoglu_etal_AutomationTasks.pdf\"                                                            #>  [5] \"2022_Arntz_etal_AutomationAngst.pdf\"                                                               #>  [6] \"2023_Brynjolfsson_etal_GenerativeAIWork.pdf\"                                                       #>  [7] \"2023_DellAcqua_etal_NavigatingTechnologicalFrontier.pdf\"                                           #>  [8] \"2023_Eloundou_etal_Labor_Impact_LLMs.pdf\"                                                          #>  [9] \"2023_Horton_etal_LLMs_as_Simulated_Agents.pdf\"                                                     #> [10] \"2023_Korinek_etal_GenerativeAIEconomicResearch.pdf\"                                                #> [11] \"2023_Lergetporer_etal_Automatability_of_Occupations.pdf\"                                           #> [12] \"2023_Noy_etal_ProductivityEffectsAI.pdf\"                                                           #> [13] \"2023_Peng_etal_AI_Productivity.pdf\"                                                                #> [14] \"2023_Svanberg_etal_BeyondAIExposure.pdf\"                                                           #> [15] \"2024_Acemoglu_etal_SimpleMacroeconomicsAI.pdf\"                                                     #> [16] \"2024_Bick_etal_RapidAdoption.pdf\"                                                                  #> [17] \"2024_Bloom_etal_AI_Skill_Premium.pdf\"                                                              #> [18] \"2024_Caplin_etal_ABCsofAI.pdf\"                                                                     #> [19] \"2024_Deming_etal_TechnologicalDisruption.pdf\"                                                      #> [20] \"2024_Falck_etal_KI_Nutzung_Unternehmen.pdf\"                                                        #> [21] \"2024_Falck_etal_KI_Nutzung.pdf\"                                                                    #> [22] \"2024_Falck_etal_KI_Usage_Companies.pdf\"                                                            #> [23] \"2024_Green_etal_AI_and_Skills_Demand.pdf\"                                                          #> [24] \"2301.07543v1.pdf\"                                                                                  #> [25] \"2302.06590v1.pdf\"                                                                                  #> [26] \"2303.10130v5.pdf\"                                                                                  #> [27] \"488.pdf\"                                                                                           #> [28] \"88684e36-en.pdf\"                                                                                   #> [29] \"ABCs_AI_Oct2024.pdf\"                                                                               #> [30] \"acemoglu-restrepo-2019-automation-and-new-tasks-how-technology-displaces-and-reinstates-labor.pdf\" #> [31] \"BBD_GenAI_NBER_Sept2024.pdf\"                                                                       #> [32] \"Deming-Ong-Summers-AESG-2024.pdf\"                                                                  #> [33] \"dp22036.pdf\"                                                                                       #> [34] \"FeltenRajSeamans_AIAbilities_AEA.pdf\"                                                              #> [35] \"JEL-2023-1736_published_version.pdf\"                                                               #> [36] \"Noy_Zhang_1.pdf\"                                                                                   #> [37] \"sd-2024-09-falck-etal-kuenstliche-intelligenz-unternehmen-1.pdf\"                                   #> [38] \"sd-2024-09-falck-etal-kuenstliche-intelligenz-unternehmen-2.pdf\"                                   #> [39] \"sd-2024-09-falck-etal-kuenstliche-intelligenz-unternehmen.pdf\"                                     #> [40] \"ssrn-4700751.pdf\"                                                                                  #> [41] \"SSRN-id4573321.pdf\"                                                                                #> [42] \"The Simple Macroeconomics of AI.pdf\"                                                               #> [43] \"w24001.pdf\"                                                                                        #> [44] \"w24871.pdf\"                                                                                        #> [45] \"w31161.pdf\"                                                                                        #> [46] \"w32430.pdf\" pdf_summary <- function(file, document_prompt){   summary <- llm_message(document_prompt,                .pdf = list(                 filename = file,                 start_page = 1,                 end_page = 35)                ) |>     openai(.json=TRUE,.stream=TRUE) |>     last_reply()      summary } files <- paste0(\"aipapers/\",dir(\"aipapers\"))  example_output <- pdf_summary(files[8],document_prompt = ' Please analyze this document and provide the following details:     1. Title: The full title of the paper.     2. Authors: List of all authors.     3. Suggested new filename: A filename for the document in the format \"ReleaseYear_Author_etal_ShortTitle.pdf\"     4. Type: Is this a (brief) policy report or a research paper? Answer with either \"Policy\" or \"Research\"     5. Answer these four  questions based on the document. Each answer should be roughly one 100 words long:         Q1. What empirical methods are used in this work?         Q2. What theoretical framework is applied or discussed?         Q3. What is the main point or argument presented?         Q4. What is the key contribution of this work?     6. Key citations: List the four most important references that the document uses in describing its own contribution.      Please  answers only with a json output in the following format:  {   \"title\": \"\",   \"authors\": [],   \"suggested_new_filename\": \"\",   \"type\": \"\",   \"questions\": {     \"empirical_methods\": \"\",     \"theory\": \"\",     \"main_point\": \"\",     \"contribution\": \"\"   },   \"key_citations\": [] } ')  example_output #> $raw_response #> [1] \"{\\n  \\\"title\\\": \\\"The Rapid Adoption of Generative AI\\\",\\n  \\\"authors\\\": [\\\"Alexander Bick\\\", \\\"Adam Blandin\\\", \\\"David J. Deming\\\"],\\n  \\\"suggested_new_filename\\\": \\\"2024_Bick_etal_RapidAdoptionGenerativeAI.pdf\\\",\\n  \\\"type\\\": \\\"Research\\\",\\n  \\\"questions\\\": {\\n    \\\"empirical_methods\\\": \\\"The paper employs a nationally representative survey method, the Real-Time Population Survey (RPS), to collect data on generative AI usage among the U.S. population. The RPS methodology aligns with the structure of the Current Population Survey (CPS) and ensures representativeness by benchmarking against national employment and earnings statistics. The survey allows flexibility in adding and modifying questions, enabling tracking of generative AI usage over time. This empirical approach provides comprehensive data on the adoption rates and usage patterns of generative AI in the United States, allowing the authors to analyze trends and draw comparisons with historical technological adoption metrics like PCs and the internet.\\\",\\n    \\\"theory\\\": \\\"The theoretical framework revolves around the concept of technology adoption and its economic impact. The paper discusses generative AI as a general-purpose technology (GPT) that can be used across various occupations and tasks. It leverages existing theories on the diffusion of innovations and the role of technology in economic growth and inequality. References to historical analyses of previous GPTs, like personal computers and the internet, provide a backdrop for understanding the implications of generative AI. The authors discuss the potential for generative AI to affect productivity and labor market dynamics, drawing comparisons with past technological shifts.\\\",\\n    \\\"main_point\\\": \\\"The main argument of the paper is that the adoption of generative AI in the United States is occurring at a faster pace than previous technological advances, such as personal computers and the internet. The authors highlight the broad utilization of generative AI across different demographics and occupations. They present evidence showing high usage rates both at work and home, with significant implications for productivity and potentially influencing workplace inequalities. They suggest that, although its rapid adoption could transform economic activities and job tasks, generative AI may amplify existing disparities due to unequal access among demographics.\\\",\\n    \\\"contribution\\\": \\\"The key contribution of this work is the provision of first-of-its-kind empirical data on the adoption and use of generative AI among a representative sample of the U.S. population. By detailing generative AI's higher adoption rates compared to past technologies, the study offers insights into current trends and potential future impacts on labor markets and productivity. Moreover, it sheds light on demographic and occupational patterns in AI usage, informing debates on inequality and economic transformation. The paper's unique survey-based approach serves as a valuable resource for policymakers, researchers, and stakeholders interested in understanding and maximizing the potential benefits of generative AI adoption.\\\"\\n  },\\n  \\\"key_citations\\\": [\\n    \\\"Brynjolfsson, Li, and Raymond, 2023\\\",\\n    \\\"Acemoglu, Autor, Hazell, and Restrepo, 2022\\\",\\n    \\\"Autor, Levy, and Murnane, 2003\\\",\\n    \\\"Comin and Hobijn, 2010\\\"\\n  ]\\n}\" #>  #> $parsed_content #> $parsed_content$title #> [1] \"The Rapid Adoption of Generative AI\" #>  #> $parsed_content$authors #> [1] \"Alexander Bick\"  \"Adam Blandin\"    \"David J. Deming\" #>  #> $parsed_content$suggested_new_filename #> [1] \"2024_Bick_etal_RapidAdoptionGenerativeAI.pdf\" #>  #> $parsed_content$type #> [1] \"Research\" #>  #> $parsed_content$questions #> $parsed_content$questions$empirical_methods #> [1] \"The paper employs a nationally representative survey method, the Real-Time Population Survey (RPS), to collect data on generative AI usage among the U.S. population. The RPS methodology aligns with the structure of the Current Population Survey (CPS) and ensures representativeness by benchmarking against national employment and earnings statistics. The survey allows flexibility in adding and modifying questions, enabling tracking of generative AI usage over time. This empirical approach provides comprehensive data on the adoption rates and usage patterns of generative AI in the United States, allowing the authors to analyze trends and draw comparisons with historical technological adoption metrics like PCs and the internet.\" #>  #> $parsed_content$questions$theory #> [1] \"The theoretical framework revolves around the concept of technology adoption and its economic impact. The paper discusses generative AI as a general-purpose technology (GPT) that can be used across various occupations and tasks. It leverages existing theories on the diffusion of innovations and the role of technology in economic growth and inequality. References to historical analyses of previous GPTs, like personal computers and the internet, provide a backdrop for understanding the implications of generative AI. The authors discuss the potential for generative AI to affect productivity and labor market dynamics, drawing comparisons with past technological shifts.\" #>  #> $parsed_content$questions$main_point #> [1] \"The main argument of the paper is that the adoption of generative AI in the United States is occurring at a faster pace than previous technological advances, such as personal computers and the internet. The authors highlight the broad utilization of generative AI across different demographics and occupations. They present evidence showing high usage rates both at work and home, with significant implications for productivity and potentially influencing workplace inequalities. They suggest that, although its rapid adoption could transform economic activities and job tasks, generative AI may amplify existing disparities due to unequal access among demographics.\" #>  #> $parsed_content$questions$contribution #> [1] \"The key contribution of this work is the provision of first-of-its-kind empirical data on the adoption and use of generative AI among a representative sample of the U.S. population. By detailing generative AI's higher adoption rates compared to past technologies, the study offers insights into current trends and potential future impacts on labor markets and productivity. Moreover, it sheds light on demographic and occupational patterns in AI usage, informing debates on inequality and economic transformation. The paper's unique survey-based approach serves as a valuable resource for policymakers, researchers, and stakeholders interested in understanding and maximizing the potential benefits of generative AI adoption.\" #>  #>  #> $parsed_content$key_citations #> [1] \"Brynjolfsson, Li, and Raymond, 2023\"         #> [2] \"Acemoglu, Autor, Hazell, and Restrepo, 2022\" #> [3] \"Autor, Levy, and Murnane, 2003\"              #> [4] \"Comin and Hobijn, 2010\"                      #>  #>  #> $is_parsed #> [1] TRUE example_output$raw_response |> cat() #> { #>   \"title\": \"The Rapid Adoption of Generative AI\", #>   \"authors\": [\"Alexander Bick\", \"Adam Blandin\", \"David J. Deming\"], #>   \"suggested_new_filename\": \"2024_Bick_etal_RapidAdoptionGenerativeAI.pdf\", #>   \"type\": \"Research\", #>   \"questions\": { #>     \"empirical_methods\": \"The paper employs a nationally representative survey method, the Real-Time Population Survey (RPS), to collect data on generative AI usage among the U.S. population. The RPS methodology aligns with the structure of the Current Population Survey (CPS) and ensures representativeness by benchmarking against national employment and earnings statistics. The survey allows flexibility in adding and modifying questions, enabling tracking of generative AI usage over time. This empirical approach provides comprehensive data on the adoption rates and usage patterns of generative AI in the United States, allowing the authors to analyze trends and draw comparisons with historical technological adoption metrics like PCs and the internet.\", #>     \"theory\": \"The theoretical framework revolves around the concept of technology adoption and its economic impact. The paper discusses generative AI as a general-purpose technology (GPT) that can be used across various occupations and tasks. It leverages existing theories on the diffusion of innovations and the role of technology in economic growth and inequality. References to historical analyses of previous GPTs, like personal computers and the internet, provide a backdrop for understanding the implications of generative AI. The authors discuss the potential for generative AI to affect productivity and labor market dynamics, drawing comparisons with past technological shifts.\", #>     \"main_point\": \"The main argument of the paper is that the adoption of generative AI in the United States is occurring at a faster pace than previous technological advances, such as personal computers and the internet. The authors highlight the broad utilization of generative AI across different demographics and occupations. They present evidence showing high usage rates both at work and home, with significant implications for productivity and potentially influencing workplace inequalities. They suggest that, although its rapid adoption could transform economic activities and job tasks, generative AI may amplify existing disparities due to unequal access among demographics.\", #>     \"contribution\": \"The key contribution of this work is the provision of first-of-its-kind empirical data on the adoption and use of generative AI among a representative sample of the U.S. population. By detailing generative AI's higher adoption rates compared to past technologies, the study offers insights into current trends and potential future impacts on labor markets and productivity. Moreover, it sheds light on demographic and occupational patterns in AI usage, informing debates on inequality and economic transformation. The paper's unique survey-based approach serves as a valuable resource for policymakers, researchers, and stakeholders interested in understanding and maximizing the potential benefits of generative AI adoption.\" #>   }, #>   \"key_citations\": [ #>     \"Brynjolfsson, Li, and Raymond, 2023\", #>     \"Acemoglu, Autor, Hazell, and Restrepo, 2022\", #>     \"Autor, Levy, and Murnane, 2003\", #>     \"Comin and Hobijn, 2010\" #>   ] #> } example_output$parsed_content$suggested_new_filename #> [1] \"2024_Bick_etal_RapidAdoptionGenerativeAI.pdf\" pdf_responses <- map(files,pdf_summary) pdf_responses |>   map_lgl(\"is_parsed\")  #>  [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE #> [16] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE table1 <- pdf_responses |>   map(\"parsed_content\") |>   map_dfr(~{     #Collapse authors into a single string     base_info <- tibble(       authors = str_c(.x$authors, collapse = \"; \"),       title   = .x$title,       type    = .x$type     )           answers <- tibble(      main_point   = .x$questions$main_point,      contribution =.x$questions$contribution,      theory       = .x$questions$theory,      methods      = .x$questions$empirical_methods     )     bind_cols(base_info,answers)   })  table1  #> # A tibble: 23 × 7 #>    authors                    title type  main_point contribution theory methods #>    <chr>                      <chr> <chr> <chr>      <chr>        <chr>  <chr>   #>  1 John J. Horton             Larg… Rese… The main … The key con… The t… The pa… #>  2 Sida Peng; Eirini Kalliam… The … Rese… The main … The key con… The t… The em… #>  3 Tyna Eloundou; Sam Mannin… GPTs… Rese… The main … The key con… The t… The pa… #>  4 Philipp Lergetporer; Kath… Auto… Rese… The prima… The paper's… The t… The wo… #>  5 Andrew Green               Arti… Rese… The main … This work's… The t… The do… #>  6 Andrew Caplin; David J. D… The … Rese… The main … This work's… The t… The em… #>  7 Daron Acemoglu; Pascual R… Auto… Rese… The main … The key con… The t… The au… #>  8 Alexander Bick; Adam Blan… The … Rese… The main … The key con… The t… The em… #>  9 David Deming; Christopher… Tech… Poli… The main … The key con… The t… The pa… #> 10 Melanie Arntz; Sebastian … The … Rese… The main … The key con… The p… The au… #> # ℹ 13 more rows table1 |> writexl::write_xlsx(\"papers_summaries.xlsx\") pdf_responses |>   map(\"parsed_content\") |>   map(\"key_citations\") |>   map_dfr(~as_tibble) #> Error in `dplyr::bind_rows()`: #> ! Argument 1 must be a data frame or a named atomic vector. #> { #>   \"title\": [\"The Impact of AI on Developer Productivity: Evidence from GitHub Copilot\"], #>   \"authors\": [\"Sida Peng\", \"Eirini Kalliamvakou\", \"Peter Cihon\", \"Mert Demirer\"], #>   \"suggested_new_filename\": [\"2023_Peng_etal_AI_Productivity.pdf\"], #>   \"type\": [\"Research\"], #>   \"questions\": { #>     \"empirical_methods\": [\".. abbreviated ..\"], #>     \"theory\": [\".. abbreviated ..\"], #>     \"main_point\": [\".. abbreviated ..\"], #>     \"contribution\": [\".. abbreviated ..\"], #>     \"key_citations\": [\"Zhang et al., 2022\", \"Nguyen and Nadi, 2022\", \"Barke et al., 2022\", \"Mozannar et al., 2022\"] #>   } #> } pdf_responses |>   map(\"parsed_content\") |>   map_dfr(~{     # Check if key_citations is nested under questions or at the top level     citations <- if (!is.null(.x$key_citations)) {       as_tibble(.x$key_citations)     } else if (!is.null(.x$questions$key_citations)) {       as_tibble(.x$questions$key_citations)     }     citations   }) #> # A tibble: 91 × 1 #>    value                                                                         #>    <chr>                                                                         #>  1 Charness, G., & Rabin, M. (2002). Understanding social preferences with simp… #>  2 Kahneman, D., Knetsch, J. L., & Thaler, R. H. (1986). Fairness as a constrai… #>  3 Samuelson, W., & Zeckhauser, R. (1988). Status quo bias in decision making.   #>  4 Aher, M., Arriaga, X., & Kalai, A. (2022). Large language models as social a… #>  5 Zhang et al., 2022                                                            #>  6 Nguyen and Nadi, 2022                                                         #>  7 Barke et al., 2022                                                            #>  8 Mozannar et al., 2022                                                         #>  9 Brynjolfsson et al., 2018                                                     #> 10 Felten et al., 2018                                                           #> # ℹ 81 more rows"},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm-pdfquestions.html","id":"schema-support","dir":"Articles","previous_headings":"","what":"Schema Support","title":"Structured Question Answering from PDFs","text":"better solution check start whether model adheres schema requested. Many newer APIs, recent OpenAI API, now offer ability pre-specify valid output schema. means can enforce strict formatting rules model generates response, reducing need post-processing corrections. can use functionality via .json_schema-arguement openai(). Instead write complete schema complex list structure os JSON tidyllm includes tidyllm_schema() function, allows define rules fields: example tidyllm_schema(), begin naming schema character vector (required API), list field along data type. Supported types include character (string also allowed synonym), logical, numeric, option define factor() categorical fields want limit options text answers. Vector outputs can indicated appending [] type. example, factor(Policy, Research) creates categorical field two options, Authors = character[] defines Authors formatted character vector multiple entries. tidyllm_schema() tries prevent much nesting. fact best option use singular output fields without [] can easily converted tibbles:","code":"# Creating an llm_message with the document analysis task msg <- llm_message('Please analyze this document and provide the following details:     1. Title: The full title of the paper.     2. Authors: List of all authors.     3. Suggested new filename: A filename for the document in the format \"ReleaseYear_Author_etal_ShortTitle.pdf\"     4. Type: Is this a (brief) policy report or a research paper? Answer with either \"Policy\" or \"Research\"     5. Answer these four  questions based on the document. Each answer should be roughly one 100 words long:         Q1. What empirical methods are used in this work?         Q2. What theoretical framework is applied or discussed?         Q3. What is the main point or argument presented?         Q4. What is the key contribution of this work?     6. Key citations: List the four most important references that the document uses in describing its own contribution.  Please provide the answers only as JSON in the following format:', .pdf = files[[8]])  #Use the helper function tidyllm_schema to define a schema  schema <- tidyllm_schema(   name = \"DocumentAnalysisSchema\",   Title = \"character\",   Authors = \"character[]\",   SuggestedFilename = \"character\",   Type = \"factor(Policy, Research)\",   Answer_Q1 = \"character\",   Answer_Q2 = \"character\",   Answer_Q3 = \"character\",   Answer_Q4 = \"character\",   KeyCitations = \"character[]\" ) # Call the OpenAI function with the message and JSON schema result <- openai(   .llm = msg,   .json_schema = schema ) #Use the helper function tidyllm_schema to define a schema  schema <- tidyllm_schema(   name = \"DocumentAnalysisSchema\",   Title = \"character\",   Authors = \"character\",   SuggestedFilename = \"character\",   Type = \"factor(Policy, Research)\",   Answer_Q1 = \"character\",   Answer_Q2 = \"character\",   Answer_Q3 = \"character\",   Answer_Q4 = \"character\",   KeyCitations = \"character\" ) # Call the OpenAI function with the message and JSON schema result <- openai(   .llm = msg,   .json_schema = schema )  reply <- results |>    last_reply()  as_tibble(reply$parsed_content) #> # A tibble: 1 × 9 #>   Title  Authors SuggestedFilename Type  Answer_Q1 Answer_Q2 Answer_Q3 Answer_Q4 #>   <chr>  <chr>   <chr>             <chr> <chr>     <chr>     <chr>     <chr>     #> 1 GPTs … Tyna E… 2023_Eloundou_et… Rese… The auth… The pape… The main… The key … #> # ℹ 1 more variable: KeyCitations <chr>"},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm-pdfquestions.html","id":"final-notes","dir":"Articles","previous_headings":"","what":"Final Notes","title":"Structured Question Answering from PDFs","text":"workflows like , ’s common need several iterations refine structure output, especially working complex queries. example, case, defining citations formatted might also improve consistency, since key_citation field varies responses. Additionally, may beneficial run separate pass just citations, perhaps asking model output BibTeX keys four important references together reason citation important note-field bibtex. now, processing steps wanted answers citations can proceed file names got structure folder:","code":"tibble(old_name  = files,        new_name  = pdf_responses |>          map(\"parsed_content\") |>          map_chr(\"suggested_new_filename\")         ) |>   mutate(     success = file.rename(old_name, file.path(\"aipapers\", new_name))   )"},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm-pdfquestions.html","id":"outlook","dir":"Articles","previous_headings":"","what":"Outlook","title":"Structured Question Answering from PDFs","text":"structured question-answering workflow streamlines extraction key insights academic papers, can also adapted document-heavy tasks. Whether ’re working reports, policy documents, news articles approach can quickly help summarize categorize information analysis. Additionally, refining prompts leveraging schema validation, can expand use workflow handle various types structured data, extracting key insights legal documents, patents, even pictures historical documents— anywhere need structured information unstructured text.","code":""},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm-synthetic-data.html","id":"implementing-an-information-treatment","dir":"Articles","previous_headings":"","what":"Implementing an information treatment","title":"Generate Synthetic Data with tidyllm","text":"next step implement generate_synthetic_infotreatment() function designed simulate respondents might update answers receiving new information, known information treatment. example, treatment based study reports high exposure legal professionals automation generative AI. function takes two arguments: conversation, represents ongoing interaction LLM questionaire based responses generate_synthetic_answers(), treated, boolean flag indicating whether respondent receives information treatment. function starts retrieving synthetic respondent’s previous answers key questions output. provides AI automation-related prompt gauge respondent’s initial perception occupation’s automatable potential (“prior” belief). respondent treated group, receive additional information legal professionals’ AI exposure. presenting information, function prompts respondent reconsider initial answer, thus capturing “posterior” belief. Finally, function returns prior posterior beliefs along respondent’s demographic information, offering insights information treatment affects perceptions AI automation. simplified version function (without error-handling cleanup logic) might look like: lawyer familiar AI, low prior automatibility occupation, choose read info material update posterior downwards. Ironically, now lawyer ’ve fully “replaced” generative AI synthetic survey respondent seems believe job safe automation. now loop lawyer profile, make answer survey, add questions. basic setup generate synthetic data tidyllm","code":"generate_synthetic_infotreatment <- function(conversation, treated) {      # Extract key initial answers (gender, birth year, familiarity with AI)   answers_opener <- tibble(     gender      = get_reply(conversation, 1),     birth_year  = get_reply(conversation, 2),     ai_familiar = get_reply(conversation, 3)   )      # Ask the prior belief question (before treatment)   prior <- conversation |>     llm_message(\"Among all occupations, how automatable do you think is your occupation?                                      0 = Not Automatable                    1 = Among the 10 lowest percent                   2 = Among the 20 lowest percent                   3 = Among the 30 lowest percent                   4 = Among the 40 lowest percent                   5 = Right in the middle                   6 = Among the top 40 percent                   7 = Among the top 30 percent                   8 = Among the top 20 percent                   9 = Among the top 10 percent                   10 = At the very top                   99 = Prefer not to say                   \") |>     ollama(.model = \"gemma2\")      # Extract the prior answer (belief before the treatment)   prior_answer <- prior |> last_reply() |> str_squish()      # Default to use the conversation state of the prior answer for the untreated group   post_treatment <- prior      # Initialize the info-updating variable (0 means no treatment)   info_updating <- \"0\"      # Apply the information treatment if the treated flag is TRUE   if (treated) {      post_treatment <- prior |>       llm_message(\"A recent study titled *Occupational, Industry, and Geographic Exposure to Artificial Intelligence* by Ed Felten (Princeton), Manav Raj (University of Pennsylvania), and Robert Seamans (New York University) identified legal professionals, including lawyers and judges, as some of the occupations with the highest exposure to AI technologies.   According to the study, legal professionals are among the top 20 among 774 occupations most exposed to generative AI, suggesting that tasks traditionally performed by lawyers, such as legal research and document review, could be increasingly automated in the coming years.  Have you read this information? 1 = YES 2 = NO 99 = Prefer not to say \") |>       ollama(.model = \"gemma2\")          # Update info-updating based on whether the participant confirms reading the information     info_updating <- last_reply(post_treatment)   }      # Ask the posterior belief question (after treatment)   # Untreated ar also asked if they want to update   post_treatment |>     llm_message(\"Do you want to correct your previous answer? Which of these do you pick?                                      0 = Not Automatable                    1 = Among the 10 lowest percent                   2 = Among the 20 lowest percent                   3 = Among the 30 lowest percent                   4 = Among the 40 lowest percent                   5 = Right in the middle                   6 = Among the top 40 percent                   7 = Among the top 30 percent                   8 = Among the top 20 percent                   9 = Among the top 10 percent                   10 = At the very top                   99 = Prefer not to say                   \") |>     ollama(.model = \"gemma2\")      # Extract the posterior answer (belief after the treatment)   posterior_answer <- last_reply(post_treatment)      # Combine demographic data, prior and posterior beliefs, and info-updating status   answers_opener |>     mutate(prior = prior_answer,            info_updating = info_updating,            posterior = posterior_answer) }  #Let's generate this treatment under the assumption that our first example lawyer was treated profile1_info_treatment <- profile1_questionaire |>    generate_synthetic_infotreatment(treated = TRUE)  #Print the result tibble profile1_info_treatment #> # A tibble: 1 × 6 #>   gender birth_year ai_familiar prior info_updating posterior #>   <chr>  <chr>      <chr>       <chr> <chr>         <chr>     #> 1 \"1 \\n\" 1991       4           3     2             2"},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm-synthetic-data.html","id":"validity-and-limitations","dir":"Articles","previous_headings":"","what":"Validity and Limitations","title":"Generate Synthetic Data with tidyllm","text":"synthetic data LLMs offers valuable insights pretesting surveys, ’s important recognize limitations approach. LLM-generated responses approximations might miss nuances come real human respondents. instance, model might accurately reflect personal biases, experiences, diverse legal practices influence real lawyers’ perspectives automation. Additionally, AI models trained vast datasets, might overgeneralization, especially niche professions data (.e. divorce lawyers). Therefore, synthetic data can streamline early iterations survey design, complement, replace, actual human feedback later stages research.","code":""},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm-synthetic-data.html","id":"outlook","dir":"Articles","previous_headings":"","what":"Outlook","title":"Generate Synthetic Data with tidyllm","text":"Looking ahead, integration synthetic data generation tools like tidyllm traditional survey workflows offers exciting possibilities researchers. LLMs become advanced capable simulating nuanced human behaviors, accuracy synthetic responses likely improve. lead faster, efficient iterations survey design, enabling researchers refine questions test hypotheses diverse, simulated populations real-world deployment. Moreover, future advancements may allow greater customization synthetic respondents, capturing complex demographic variables behavioral patterns. instance, enhancing ability simulate specific professions, backgrounds, even emotional states, synthetic data evolve robust tool experimental pretesting fields beyond survey research, behavioral economics, political polling, educational assessment.","code":""},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm.html","id":"introduction-to-tidyllm","dir":"Articles","previous_headings":"","what":"Introduction to tidyllm","title":"Get Started","text":"tidyllm R package designed provide unified interface interacting various large language model APIs. vignette guide basic setup usage tidyllm.","code":""},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm.html","id":"installation","dir":"Articles","previous_headings":"Introduction to tidyllm","what":"Installation","title":"Get Started","text":"install tidyllm CRAN , use: install current development version directly GitHub using devtools:","code":"install.packages(\"tidyllm\") # Install devtools if not already installed if (!requireNamespace(\"devtools\", quietly = TRUE)) {   install.packages(\"devtools\") }  # Install TidyLLM from GitHub devtools::install_github(\"edubruell/tidyllm\")"},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm.html","id":"setting-up-api-keys-or-ollama","dir":"Articles","previous_headings":"Introduction to tidyllm > Installation","what":"Setting up API Keys or ollama","title":"Get Started","text":"using tidyllm, need set API keys services plan use. ’s set different providers: Claude models can get API key Anthropic Console: ChatGPT can obtain API key signing OpenAI set : Mistral can set API key Mistral console page set groq (confused grok) can setup API keys Groq Console: Alternatively, can set keys .Renviron file persistent storage. , execute usethis::edit_r_environ(), add line API key file, example: want work local large lange models via ollama need install official project website. Ollama sets local large language model server can use run open-source models devices.","code":"Sys.setenv(ANTHROPIC_API_KEY = \"YOUR-ANTHROPIC-API-KEY\") Sys.setenv(OPENAI_API_KEY = \"YOUR-OPENAI-API-KEY\") Sys.setenv(MISTRAL_API_KEY = \"MISTRAL-API-KEY-GOES-HERE\") Sys.setenv(GROQ_API_KEY = \"YOUR-GROQ-API-KEY\") ANTHROPIC_API_KEY=\"YOUR-ANTHROPIC-API-KEY\""},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm.html","id":"basic-usage","dir":"Articles","previous_headings":"Introduction to tidyllm > Installation","what":"Basic Usage","title":"Get Started","text":"Let’s start simple example using tidyllm interact different language models:","code":"library(tidyllm)  # Start a conversation with Claude conversation <- llm_message(\"What is the capital of France?\") |>   claude()  #Standard way that llm_messages are printed conversation ## Message History: ## system: You are a helpful assistant ## -------------------------------------------------------------- ## user: What is the capital of France? ## -------------------------------------------------------------- ## assistant: The capital of France is Paris. ## -------------------------------------------------------------- # Continue the conversation with ChatGPT conversation <- conversation |>   llm_message(\"What's a famous landmark in this city?\") |>   openai()  get_reply(conversation) ## [1] \"A famous landmark in Paris is the Eiffel Tower.\""},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm.html","id":"sending-images-to-models","dir":"Articles","previous_headings":"Introduction to tidyllm > Installation","what":"Sending Images to Models","title":"Get Started","text":"tidyllm also supports sending images multimodal models. Let’s send picture : let ChatGPT guess picture made:","code":"# Describe an image using a llava model on ollama image_description <- llm_message(\"Describe this picture? Can you guess where it was made?\",                                  .imagefile = \"picture.jpeg\") |>   openai(.model = \"gpt-4o\")  # Get the last reply get_reply(image_description) ## [1] \"The picture shows a beautiful landscape with a lake, mountains, and a town nestled below. The sun is shining brightly, casting a serene glow over the water. The area appears lush and green, with agricultural fields visible. \\n\\nThis type of scenery is reminiscent of northern Italy, particularly around Lake Garda, which features similar large mountains, picturesque water, and charming towns.\""},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm.html","id":"adding-pdfs-to-messages","dir":"Articles","previous_headings":"Introduction to tidyllm > Installation","what":"Adding PDFs to messages","title":"Get Started","text":"llm_message() function also supports extracting text PDFs including message. allows easily provide context PDF document interacting AI assistant. use feature, need pdftools package installed. already installed, can install : include text PDF prompt, simply pass file path .pdf argument chat function: package automatically extract text PDF include prompt sent API. text wrapped <pdf> tags clearly indicate content PDF:","code":"install.packages(\"pdftools\") llm_message(\"Please summarize the key points from the provided PDF document.\",       .pdf = \"die_verwandlung.pdf\") |>      openai(.model = \"gpt-4o-mini\") ## Message History: ## system: You are a helpful assistant ## -------------------------------------------------------------- ## user: Please summarize the key points from the provided PDF document. ##  -> Attached Media Files:  die_verwandlung.pdf  ## -------------------------------------------------------------- ## assistant: Here are the key points from the provided PDF document 'Die Verwandlung' by Franz Kafka: ##  ## 1. The story centers around Gregor Samsa, who wakes up one morning to find that he has been transformed into a giant insect-like creature. ##  ## 2. Gregor's transformation causes distress and disruption for his family. They struggle to come to terms with the situation and how to deal with Gregor in his new state. ##  ## 3. Gregor's family, especially his sister Grete, initially tries to care for him, but eventually decides they need to get rid of him. They lock him in his room and discuss finding a way to remove him. ##  ## 4. Gregor becomes increasingly isolated and neglected by his family. He becomes weaker and less mobile due to his injuries and lack of proper care. ##  ## 5. Eventually, Gregor dies, and his family is relieved. They then begin to make plans to move to a smaller, more affordable apartment and start looking for new jobs and opportunities. ## -------------------------------------------------------------- Please summarize the key points from the provided PDF document.  <pdf filename=\"example_document.pdf\"> Extracted text from the PDF file... <\/pdf>"},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm.html","id":"sending-r-outputs-to-language-models","dir":"Articles","previous_headings":"Introduction to tidyllm > Installation","what":"Sending R Outputs to Language Models","title":"Get Started","text":"can automatically include R code outputs prompts. llm_message() optional argument .f can specify (anonymous) function, run console output captured appended message run . addition can use .caputre_plot send last plot pane model.  Now can send plot data summary language model:","code":"library(tidyverse)  # Create a plot for the mtcars example data ggplot(mtcars, aes(wt, mpg)) +   geom_point() +   geom_smooth(method = \"lm\", formula = 'y ~ x') +   labs(x=\"Weight\",y=\"Miles per gallon\") library(tidyverse) llm_message(\"Analyze this plot and data summary:\",                    .capture_plot = TRUE, #Send the plot pane to a model                   .f = ~{summary(mtcars)}) |> #Run summary(data) and send the output   claude() ## Message History: ## system: You are a helpful assistant ## -------------------------------------------------------------- ## user: Analyze this plot and data summary: ##  -> Attached Media Files:  file1568f6c1b4565.png, RConsole.txt  ## -------------------------------------------------------------- ## assistant: Based on the plot and data summary provided, here's an analysis: ##  ## 1. Relationship between Weight and MPG: ##    The scatter plot shows a clear negative correlation between weight (wt) and miles per gallon (mpg). As the weight of the car increases, the fuel efficiency (mpg) decreases. ##  ## 2. Linear Trend: ##    The blue line in the plot represents a linear regression fit. The downward slope confirms the negative relationship between weight and mpg. ##  ## 3. Data Distribution: ##    - The weight of cars in the dataset ranges from 1.513 to 5.424 (likely in thousands of pounds). ##    - The mpg values range from 10.40 to 33.90. ##  ## 4. Variability: ##    There's some scatter around the regression line, indicating that while weight is a strong predictor of mpg, other factors also influence fuel efficiency. ##  ## 5. Other Variables: ##    While not shown in the plot, the summary statistics provide information on other variables: ##    - Cylinder count (cyl) ranges from 4 to 8, with a median of 6. ##    - Horsepower (hp) ranges from 52 to 335, with a mean of 146.7. ##    - Transmission type (am) is binary (0 or 1), likely indicating automatic vs. manual. ##  ## 6. Model Fit: ##    The grey shaded area around the regression line represents the confidence interval. It widens at the extremes of the weight range, indicating less certainty in predictions for very light or very heavy vehicles. ##  ## 7. Outliers: ##    There are a few potential outliers, particularly at the lower and higher ends of the weight spectrum, that deviate from the general trend. ##  ## In conclusion, this analysis strongly suggests that weight is a significant factor in determining a car's fuel efficiency, with heavier cars generally having lower mpg. However, the presence of scatter in the data indicates that other factors (possibly related to engine characteristics, transmission type, or aerodynamics) also play a role in determining fuel efficiency. ## --------------------------------------------------------------"},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm.html","id":"getting-replies-from-the-api","dir":"Articles","previous_headings":"Introduction to tidyllm > Installation","what":"Getting replies from the API","title":"Get Started","text":"can retrieve assistant reply text message history get_reply(). specifying index get_reply() can decide assistant message get list. defaults last available reply. However, requested structured data API also function called get_reply_data(), directly gives structured output format asked . can get assistant messages conversation get_reply() last_reply():","code":"conversation <- llm_message(\"Imagine a German adress.\") |>      groq() |>      llm_message(\"Imagine another address\") |>      groq()  conversation ## Message History: ## system: You are a helpful assistant ## -------------------------------------------------------------- ## user: Imagine a German adress. ## -------------------------------------------------------------- ## assistant: Let's imagine a German address:  ##  ## Herr Müller ## Musterstraße 12 ## 53111 Bonn ## -------------------------------------------------------------- ## user: Imagine another address ## -------------------------------------------------------------- ## assistant: Let's imagine another German address: ##  ## Frau Schmidt ## Fichtenweg 78 ## 42103 Wuppertal ## -------------------------------------------------------------- #Getting the first  conversation |> get_reply(1) ## [1] \"Let's imagine a German address: \\n\\nHerr Müller\\nMusterstraße 12\\n53111 Bonn\" #By default it gets the last reply conversation |> get_reply() ## [1] \"Let's imagine another German address:\\n\\nFrau Schmidt\\nFichtenweg 78\\n42103 Wuppertal\" #Or if you can more easily remember last_reply this works too conversation |> last_reply() ## [1] \"Let's imagine another German address:\\n\\nFrau Schmidt\\nFichtenweg 78\\n42103 Wuppertal\""},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm.html","id":"working-with-structured-model-outputs","dir":"Articles","previous_headings":"Introduction to tidyllm > Installation","what":"Working with structured model outputs","title":"Get Started","text":"Currently, openai() API function tidyllm supports enforcing pre-specified schema replies. specifying schema .json_schema argument openai(), ensure structured responses defined format. tidyllm_schema() function helper defining schemas translating JSON-schema format API needs. disallows nested structures ensures flat, predictable JSON outputs. works following syntax: name: name identifier schema (needed API). \"character\" \"string\": Text fields. \"factor(...)\": Enumerations allowable values, like factor(Germany, France). \"logical\": TRUE FALSE \"numeric\": Numbers. \"type[]\": Lists given type, \"character[]\". Note model answered text JSON (JavaScript Object Notation) format. lightweight, text-based format commonly used transmitting data server web application. JSON response consists key-value pairs, can easily converted R list get_reply_data() function: ollama(), groq(), mistral() functions also support structured outputs via simpler JSON-mode, accessible .json argument. Since APIs currently lack native schema enforcement, ’ll need specify desired JSON format directly prompt. Although get_reply_data() can extract structured data replies, model may fully adhere specified output format. tidyllm integrate schema functionality APIs support natively.","code":"address_schema <- tidyllm_schema(   name = \"AddressSchema\",   street = \"character\",   houseNumber = \"numeric\",   postcode = \"character\",   city = \"character\",   region = \"character\",   country = \"factor(Germany,France)\" ) address <- llm_message(\"Imagine an address in JSON format that matches the schema.\") |>         openai(.json_schema = address_schema) address ## Message History: ## system: You are a helpful assistant ## -------------------------------------------------------------- ## user: Imagine an address in JSON format that matches the schema. ## -------------------------------------------------------------- ## assistant: {\"street\":\"Hauptstraße\",\"houseNumber\":123,\"postcode\":\"10115\",\"city\":\"Berlin\",\"region\":\"Berlin\",\"country\":\"Germany\"} ## -------------------------------------------------------------- address |> get_reply_data() |> str() ## List of 6 ##  $ street     : chr \"Hauptstraße\" ##  $ houseNumber: int 123 ##  $ postcode   : chr \"10115\" ##  $ city       : chr \"Berlin\" ##  $ region     : chr \"Berlin\" ##  $ country    : chr \"Germany\""},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm.html","id":"api-parameters","dir":"Articles","previous_headings":"Introduction to tidyllm > Installation","what":"API parameters","title":"Get Started","text":"Different API functions support different model parameters like deterministic response via parameters like temperature. Please read API-documentation documentation model functions specific examples.","code":"temp_example <- llm_message(\"Explain how temperature parameters work in large language models  and why temperature 0 gives you deterministic outputs in one sentence.\")      #per default it is non-zero   temp_example |> ollama(.temperature=0) ## Message History: ## system: You are a helpful assistant ## -------------------------------------------------------------- ## user: Explain how temperature parameters work in large language models  and why temperature 0 gives you deterministic outputs in one sentence. ## -------------------------------------------------------------- ## assistant: In large language models, temperature parameters control the randomness of generated text by scaling the output probabilities, with higher temperatures introducing more uncertainty and lower temperatures favoring more likely outcomes; specifically, setting temperature to 0 effectively eliminates all randomness, resulting in deterministic outputs because it sets the probability of each token to its maximum likelihood value. ## -------------------------------------------------------------- #Retrying with .temperature=0   temp_example |> ollama(.temperature=0) ## Message History: ## system: You are a helpful assistant ## -------------------------------------------------------------- ## user: Explain how temperature parameters work in large language models  and why temperature 0 gives you deterministic outputs in one sentence. ## -------------------------------------------------------------- ## assistant: In large language models, temperature parameters control the randomness of generated text by scaling the output probabilities, with higher temperatures introducing more uncertainty and lower temperatures favoring more likely outcomes; specifically, setting temperature to 0 effectively eliminates all randomness, resulting in deterministic outputs because it sets the probability of each token to its maximum likelihood value. ## --------------------------------------------------------------"},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm.html","id":"embeddings","dir":"Articles","previous_headings":"Introduction to tidyllm > Installation","what":"Embeddings","title":"Get Started","text":"Embedding models tidyllm transform textual inputs vector representations, capturing semantic information can enhance similarity comparisons, clustering, retrieval tasks. can generate embeddings using functions like openai_embedding(), mistral_embedding(), ollama_embedding() interface respective APIs. functions create vector representations texts. functions return vector representations either message message history , typically application, entry character vector.","code":""},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm.html","id":"batch-requests","dir":"Articles","previous_headings":"Introduction to tidyllm > Installation","what":"Batch requests","title":"Get Started","text":"⚠️ Note: batch functions OpenAI API work progress. fetch_openai_batch() yet ready current version. Anthropic OpenAI, offer batch request options around 50% cheaper standard single-interaction APIs. Batch processing allows submit multiple message histories , processed together, usually within 24-hour period. tidyllm, can use send_claude_batch() send_openai_batch() functions submit batch requests. ’s example send batch request Claude’s batch API: send_claude_batch() function returns list message histories input, marked attribute contains batch-id Claude API well unique names list element can used stitch together messages replies, ready. provide named list messages, tidyllm use names identifiers batch, names unique. Tip: Saving batch requests file allows persist across R sessions, making easier manage large jobs access results later. sending batch request, can check status check_openai_batch() check_claude_batch() . example: status output shows details number successful, errored, expired, canceled requests batch, well current status. processing batch completed can fetch results fetch_claude_batch() fetch_openai_batch(): output list message histories, now updated new assistant replies. can process responses tidyllm’s standard tools.launching large batch operation, ’s good practice run test requests review outputs. approach helps confirm prompt settings model configurations produce desired responses, minimizing potential errors resource waste.","code":"#Create a message batch and save it to disk to fetch it later glue(\"Write a poem about {x}\", x=c(\"cats\",\"dogs\",\"hamsters\")) |>   purrr::map(llm_message) |>   send_claude_batch() |>   saveRDS(\"claude_batch.rds\") #Create a message batch and save it to disk to fetch it later glue(\"Write a poem about {x}\", x=c(\"cats\",\"dogs\",\"hamsters\")) |>   purrr::map(llm_message) |>   send_claude_batch() |>   saveRDS(\"claude_batch.rds\") #Check the status of the batch readRDS(\"claude_batch.rds\") |>    check_claude_batch() ## # A tibble: 1 × 8 ##   batch_id          status created_at          expires_at          req_succeeded ##   <chr>             <chr>  <dttm>              <dttm>                      <dbl> ## 1 msgbatch_02A1B2C… ended  2024-11-01 10:30:00 2024-11-02 10:30:00             3 ## # ℹ 3 more variables: req_errored <dbl>, req_expired <dbl>, req_canceled <dbl> conversations <- readRDS(\"claude_batch.rds\") |>   fetch_claude_batch()  poems <- map_chr(conversations, get_reply)"},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm.html","id":"streaming-back-responses-experimental","dir":"Articles","previous_headings":"Introduction to tidyllm > Installation","what":"Streaming back responses (Experimental)","title":"Get Started","text":"chat API-functions support real-time streaming reply tokens console model works .stream=TRUE argument. feature offers slightly better feedback model behavior real-time, ’s particularly useful data-analysis workflows. consider feature experimental recommend using non-streaming responses production tasks. Note error handling streaming callbacks varies API differs quality time.","code":""},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm.html","id":"choosing-the-right-model-and-api","dir":"Articles","previous_headings":"Introduction to tidyllm > Installation","what":"Choosing the Right Model and API","title":"Get Started","text":"tidyllm supports multiple APIs, offering distinct large language models varying strengths. choice model API use often depends specific task, cost considerations, data privacy concerns. claude() - Anthropic API: Claude known generating thoughtful, nuanced responses, making ideal tasks require human-like reasoning, summarization creative writing. Claude Sonnet 3.5 currently one top-performing models many benchmarks. However, can sometimes verbose necessary, lacks direct JSON support, requires additional prompting validation ensure structured output. openai() (OpenAI API): Models OpenAI API, particularly GPT-4o model, extremely versatile perform well across wide range tasks, including text generation, code completion, multimodal analysis. addition o1-reasoning models offer good performance set specific task (relatively high price). also azure_openai() function prefer use OpenAI API Microsoft Azure. mistral() (EU-based): Mistral offers lighter-weight, open-source models developed hosted EU, making particularly appealing data protection (e.g., GDPR compliance) concern. models may powerful GPT-4o Claude Sonnet, Mistral offers good performance standard text generation tasks. groq() (Fast): Groq offers unique advantage custom AI accelerator hardware, get fastest output available API. delivers high performance low costs, especially tasks require fast execution. hosts many strong open-source models, like lamma3:70b. also groq_transcribe() function available allows transcribe audio files Whipser-Large model Groq API. ollama() (Local Models): data privacy priority, running open-source models like gemma2::9B locally via ollama gives full control model execution data. However, trade-local models require significant computational resources, often quite powerful large API-providers. ollama blog regularly posts new models advantages can download via ollama_download_model().","code":""},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm_classifiers.html","id":"a-common-classification-task","dir":"Articles","previous_headings":"","what":"A Common Classification Task","title":"Classifying Texts with tidyllm","text":"Imagine ’ve just collected thousands survey responses people describe jobs words. responses detailed, others vague, ’s plenty variation . Now, need categorize standardized occupation codes, like SOC classification system Bureau Labor Statistics. Manually sorting response take days, weeks, inconsistencies coders almost guaranteed. instance, dataset might look something like : 7,000 rows occupation descriptions, ranging “Librarian” “Making sure everything runs” goal classify messy responses one 22 2-digit occupation codes: article, take structured approach tackle classification task efficiently. ’s step--step workflow kind task: Pick Sample:Start filtering dataset retain distinct occupation descriptions. , randomly select sample distinct responses work . Initial Classification: Use simple prompt categorize responses occupation codes. Manual Correction: Review correct classifications create reliable ground truth. Training/Test Split: split ground truth dataset using rsample training test sets. Experimentation: Test different prompts, models, parameters training set, comparing one-shot multi-shot approaches. Model Evaluation: Use yardstick find best-performing combination training data. Testing: Apply best-performing model test set evaluate well performs unseen occupation descriptions. Full Classification: Use validated model setup classify entire dataset efficiently.","code":"library(tidyllm) library(tidyverse) library(glue) occ_data <- read_rds(\"occupation_data.rds\") occ_data #> # A tibble: 7,000 × 2 #>    respondent occupation_open                   #>         <int> <chr>                             #>  1     100019 Ops oversight and strategy        #>  2     100266 Coordinating operations           #>  3     100453 Making sure everything runs       #>  4     100532 Building and demolition           #>  5     100736 Help lawyers with cases           #>  6     100910 I sell mechanical parts           #>  7     101202 Librarian                         #>  8     101325 Operations planning and execution #>  9     101329 Bookkeeper                        #> 10     101367 Kitchen staff                     #> # ℹ 6,990 more rows occ_codes <- read_rds(\"occ_codes_2digits.rds\") |>   print(n=Inf) #> # A tibble: 22 × 2 #>     occ2 occ_title                                                  #>    <dbl> <chr>                                                      #>  1    11 Management Occupations                                     #>  2    13 Business and Financial Operations Occupations              #>  3    15 Computer and Mathematical Occupations                      #>  4    17 Architecture and Engineering Occupations                   #>  5    19 Life, Physical, and Social Science Occupations             #>  6    21 Community and Social Service Occupations                   #>  7    23 Legal Occupations                                          #>  8    25 Educational Instruction and Library Occupations            #>  9    27 Arts, Design, Entertainment, Sports, and Media Occupations #> 10    29 Healthcare Practitioners and Technical Occupations         #> 11    31 Healthcare Support Occupations                             #> 12    33 Protective Service Occupations                             #> 13    35 Food Preparation and Serving Related Occupations           #> 14    37 Building and Grounds Cleaning and Maintenance Occupations  #> 15    39 Personal Care and Service Occupations                      #> 16    41 Sales and Related Occupations                              #> 17    43 Office and Administrative Support Occupations              #> 18    45 Farming, Fishing, and Forestry Occupations                 #> 19    47 Construction and Extraction Occupations                    #> 20    49 Installation, Maintenance, and Repair Occupations          #> 21    51 Production Occupations                                     #> 22    53 Transportation and Material Moving Occupations"},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm_classifiers.html","id":"classifying-a-sub-sample","dir":"Articles","previous_headings":"","what":"Classifying a Sub-Sample","title":"Classifying Texts with tidyllm","text":"start ensuring classify distinct responses. eliminates duplicates ensures efficient reliable classification process: help us focus variations across distinct occupations, avoiding repeated classification efforts identical responses. Next, divide distinct occupations sub-sample manual classification remaining portion used later. use initial_split() function rsample package, splitting 10% data smaller test set manual correction model training: splitting data, now smaller sub-sample 422 observations work initial classification stage.","code":"# Pick only distinct occupations from the dataset distinct_occupations <- occ_data |>    distinct(occupation = occupation_open)  print(distinct_occupations, n = 5) #> # A tibble: 2,209 × 1 #>   occupation                  #>   <chr>                       #> 1 Ops oversight and strategy  #> 2 Coordinating operations     #> 3 Making sure everything runs #> 4 Building and demolition     #> 5 Help lawyers with cases     #> # ℹ 2,204 more rows #Set a seed for reproducability set.seed(123)  # Split the distinct occupations into a sub-sample (10%) and the rest (90%) library(rsample) occ_split <- initial_split(distinct_occupations, prop = 0.8)  # Retrieve the sub-sample and the remaining data rest_of_data <- training(occ_split) sub_sample <- testing(occ_split)  print(sub_sample, n = 5)  #> # A tibble: 442 × 1 #>   occupation                      #>   <chr>                           #> 1 Making sure everything runs     #> 2 Bartender                       #> 3 Post-secondary health education #> 4 Food servin                     #> 5 Exectutive assistant            #> # ℹ 437 more rows"},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm_classifiers.html","id":"classifying-a-sub-sample-1","dir":"Articles","previous_headings":"Classifying a Sub-Sample","what":"Classifying a sub-sample","title":"Classifying Texts with tidyllm","text":"classify sub-sample occupation descriptions, first generate prompts specific classification task occupation. prompt directs model classify occupation based numerical code, ensuring output aligns coding system. use glue() create prompts occupation description prompt {occupation} purrr::map() pass prompt llm_message(). output list messages ready submission large language model. typical message list classification tasks looks like : classify initial subset, need manually verified later, beneficial use reliable commercial model, output typically requires fewer adjustments. , choose Claude-3.5-Sonnet, setting .temperature 0 ensure deterministic (non-random) responses. Initially, test prompt standard sequential claude() function. preliminary step allows us verify model interprets prompt accurately produces output required format running full batch classification tasks. end, create tibble containing occupation descriptions sub-sample alongside prepared classification tasks. randomly select 10 rows tibble. selected occupation passed sequentially classify_sequential() function passes classification task claude(), fetches reply get_reply() converts text output model numeric code parse_number(). Finally, results joined occ_codes table map numeric occupation codes corresponding occupation labels, providing interpretable output: output demonstrates model accurately interprets prompts, assigning occupation description suitable category. confirmation, can confidently leverage tidyllm’s built-batch-processing functions classify entire set messages classification_tasks using Anthropic’s Claude-3.5-Sonnet model. Batch-processing offers significant cost advantage sequential classification. Anthropic charges half price per token batch requests compared single requests, making efficient choice large-scale tasks. batch requests return results instantly, typically processed within 24 hours, often much faster. Message Batch limited either 10,000 requests 32 MB size, whichever reached first. initiate batch request, use send_claude_batch() function. function returns list classification tasks sent, marked attribute contains batch-id Claude API well unique names list element can used stitch together messages replies. supply named list messages, names used identifiers (ensuring unique submission). convenient access later, save batch output locally, allowing us resume processing batch even closing R session: can check status batch : now see batch created status ended. 442 requests succeeded. Batch results can downloaded within 29 days creation. can see available batches API list_claude_batches(). download results batch use fetch_claude_batch() function: get list messages assistant replies fetch_claude_batch() pass map_chr(get_reply) get assistant replies message character vector. parse numeric code merge occupation titles. ensure output correct, can export results Excel write_xlsx writexl package manually fix miss-classifications. Running initial batch sub-sample cost us less 30 cents. manual review classifier’s output showed highly promising results. 443 classifications, 9 needed corrections, indicating error rate just 2% claude()-based classifier initial prompt. issues arose correctly identifying unclear responses missing, “Doin’ numbers” “Barster” (potential mix Barrister Barkeeper). point stop proceed classify entire data send_claude_batch(), given strong performance manual validation unseen data scaling data cost less 1.50$. However, illustrate general principle find good llm classifiers less sure, now use initial ground truth built base experiment different models prompts. example, try determine simpler alternatives small local models perform just well.","code":"prompts <- glue('       Classify this occupation response from a survey: {occupation}              Pick one of the following numerical codes from this list.        Respond only with the code!       11 = Management Occupations                                           13 = Business and Financial Operations Occupations                    15 = Computer and Mathematical Occupations                            17 = Architecture and Engineering Occupations                         19 = Life, Physical, and Social Science Occupations                   21 = Community and Social Service Occupations                         23 = Legal Occupations                                                25 = Educational Instruction and Library Occupations                  27 = Arts, Design, Entertainment, Sports, and Media Occupations       29 = Healthcare Practitioners and Technical Occupations               31 = Healthcare Support Occupations                                   33 = Protective Service Occupations                                   35 = Food Preparation and Serving Related Occupations                 37 = Building and Grounds Cleaning and Maintenance Occupations        39 = Personal Care and Service Occupations                            41 = Sales and Related Occupations                                    43 = Office and Administrative Support Occupations                    45 = Farming, Fishing, and Forestry Occupations                       47 = Construction and Extraction Occupations                          49 = Installation, Maintenance, and Repair Occupations                51 = Production Occupations                                           53 = Transportation and Material Moving Occupations       99 = Missing Occupation (No clear occupation)', occupation = sub_sample$occupation)  classification_tasks <- map(prompts,llm_message) classification_tasks[[1]] #> Message History: #> system: You are a helpful assistant #> -------------------------------------------------------------- #> user: Classify this occupation response from a survey: Making sure everything runs #>  #> Pick one of the following numerical codes from this list.  #> Respond only with the code! #> 11 = Management Occupations                                     #> 13 = Business and Financial Operations Occupations              #> 15 = Computer and Mathematical Occupations                      #> 17 = Architecture and Engineering Occupations                   #> 19 = Life, Physical, and Social Science Occupations             #> 21 = Community and Social Service Occupations                   #> 23 = Legal Occupations                                          #> 25 = Educational Instruction and Library Occupations            #> 27 = Arts, Design, Entertainment, Sports, and Media Occupations #> 29 = Healthcare Practitioners and Technical Occupations         #> 31 = Healthcare Support Occupations                             #> 33 = Protective Service Occupations                             #> 35 = Food Preparation and Serving Related Occupations           #> 37 = Building and Grounds Cleaning and Maintenance Occupations  #> 39 = Personal Care and Service Occupations                      #> 41 = Sales and Related Occupations                              #> 43 = Office and Administrative Support Occupations              #> 45 = Farming, Fishing, and Forestry Occupations                 #> 47 = Construction and Extraction Occupations                    #> 49 = Installation, Maintenance, and Repair Occupations          #> 51 = Production Occupations                                     #> 53 = Transportation and Material Moving Occupations #> 99 = Missing Occupation (No clear occupation) #> -------------------------------------------------------------- classify_sequential <- function(occupation_open,message){     raw_code <- message |>         claude(.temperature = 0) |>         get_reply() |>         parse_number()          tibble(occupation_open=occupation_open, occ2=raw_code) }  tibble(occupation_open = sub_sample$occupation, message = classification_tasks) %>%   slice_sample(n=10) |>   pmap_dfr(classify_sequential) |>   left_join(occ_codes, by=\"occ2\") #> # A tibble: 10 × 3 #>    occupation_open                  occ2 occ_title                               #>    <chr>                           <dbl> <chr>                                   #>  1 Fine Carpentry                     47 Construction and Extraction Occupations #>  2 Eyeglass makin'                    51 Production Occupations                  #>  3 Layin' down shingles               47 Construction and Extraction Occupations #>  4 Handle construction budgets        13 Business and Financial Operations Occu… #>  5 Bodyshop guy                       49 Installation, Maintenance, and Repair … #>  6 Hair dresser                       39 Personal Care and Service Occupations   #>  7 Sort and deliver mail              43 Office and Administrative Support Occu… #>  8 Ops oversight                      11 Management Occupations                  #>  9 Oversee all cleaning operations    11 Management Occupations                  #> 10 Systems administrator              15 Computer and Mathematical Occupations classification_tasks |>   send_claude_batch(.temperature = 0) |>   write_rds(\"sub_sample_batch.rds\") read_rds(\"sub_sample_batch.rds\") |>   check_claude_batch() #> # A tibble: 1 × 8 #>   batch_id          status created_at          expires_at          req_succeeded #>   <chr>             <chr>  <dttm>              <dttm>                      <dbl> #> 1 msgbatch_015JWDH… ended  2024-10-31 16:33:35 2024-11-01 16:33:35           442 #> # ℹ 3 more variables: req_errored <dbl>, req_expired <dbl>, req_canceled <dbl> occ2_codes <- read_rds(\"sub_sample_batch.rds\")) |>   fetch_claude_batch() |>   map_chr(get_reply) |>   parse_number()  tibble(occupation_open = sub_sample$occupation,        occ2 = occ2_codes) |>   left_join(occ_codes, by=\"occ2\") |>   writexl::write_xlsx(\"ground_truth_excel.xlsx\")"},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm_classifiers.html","id":"optimizing-and-testing-classifiers","dir":"Articles","previous_headings":"","what":"Optimizing and Testing Classifiers","title":"Classifying Texts with tidyllm","text":"Next, present general approach optimize test different LLM-based classifiers. First, split dataset training test sets using rsample ensure can experiment different prompts setups training data evaluate final model performance unseen data.","code":"ground_truth <- readxl:::read_xlsx(\"ground_truth_corrected.xlsx\")  # Split the ground-truth into training and testing sets set.seed(123) gt_split <- initial_split(ground_truth, prop = 0.7)  # Retrieve training and testing data train_data <- training(gt_split) test_data  <- testing(gt_split)  print(train_data,n=5) #> # A tibble: 309 × 3 #>   occupation_open              occ2 occ_title                                    #>   <chr>                       <dbl> <chr>                                        #> 1 Computer network technician    15 Computer and Mathematical Occupations        #> 2 Educational support            25 Educational Instruction and Library Occupat… #> 3 Fine Carpentry                 47 Construction and Extraction Occupations      #> 4 Keep things organized          43 Office and Administrative Support Occupatio… #> 5 Group fitness instructor       39 Personal Care and Service Occupations        #> # ℹ 304 more rows"},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm_classifiers.html","id":"writing-a-flexible-classifier-function","dir":"Articles","previous_headings":"Optimizing and Testing Classifiers","what":"Writing a flexible classifier function","title":"Classifying Texts with tidyllm","text":"test different prompts models systematically need create flexible classifier function can handle different prompts models can work api-function including local models support batch requests. function can take open occupation, ground-truth occ2 prompt prompt_id, api_function model arguements allowing us test occupation classification across grid prompts models.","code":"# External numerical code list for reusability numerical_code_list <- c('       11 = Management Occupations                                           13 = Business and Financial Operations Occupations                    15 = Computer and Mathematical Occupations                            ... abreviated ...       51 = Production Occupations                                           53 = Transportation and Material Moving Occupations       99 = Missing Occupation')  # Classification function that accepts prompt, api_function, and model  # as well as the ground truth to pass through as arguments classify_occupation_grid <- function(occupation,                                 occ2,                                 prompt,                                 prompt_id,                                 api_function,                                 model){   # Output what the model is currently doing to the console   glue(\"Classifying: {model} - {prompt_id} - {occupation}\\n\") |> cat(\"\\n\")      # List of valid codes as strings   valid_codes <- as.character(occ_codes$occ2)      # Initialize classification_raw   classification <- tryCatch({     # Get the assistant's reply using the dynamically provided API function and model     assistant_reply <- llm_message(prompt) |>       api_function(.model = model, .temperature = 0) |>       get_reply() |>       str_squish()          # Validate the assistant's reply     if (assistant_reply %in% valid_codes) {       as.integer(assistant_reply)     } else {       98L  # Return 98 for invalid responses     }   }, error = function(e){     97L  # Return 97 in case of an error (e.g., API failure)   })      # Return a tibble containing the original occupation description and classification result   tibble(     occupation_open = occupation,     occ2_predict    = classification,     occ2_truth      = occ2,     model           = model,     prompt_id       = prompt_id   ) }"},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm_classifiers.html","id":"defining-the-prompt-and-model-grid","dir":"Articles","previous_headings":"Optimizing and Testing Classifiers","what":"Defining the Prompt and Model Grid","title":"Classifying Texts with tidyllm","text":"define set prompts models want test. allow us apply classifier across different configurations compare results. ’s prompts models set : Prompts: Prompt 1: detailed prompt, asking model classify occupations warning make guesses. Prompt 2: Explicitly ask handle invalid responses returning special code (99) input resemble valid occupation. Prompt 3: shorter, concise version test whether model performs similarly less detailed instructions. Models: Llama3.2:3B: opensource large language model just 3 billion parameters fast, run locally via ollama() Gemma2:9B: Another candidate model, performs well classification tasks, double size Lama3.2 therefore somewhat slower. set grid combining prompts models. expand_grid() function tidyverse useful tool create every possible combination prompts models, use evaluate classifier: run classification across entire grid, use pmap_dfr() purrr package, allows us iterate multiple arguments simultaneously. combination occupation response, prompt, model passed classify_occupation() function, results concatenated single tibble: ⚠️ Note: Running extensive classification grid like , especially large datasets slow models, can take significant amount time. Therefore, ’s often reasonable save intermediate results periodically, don’t lose progress something goes wrong (e.g., crash network issue). combining pwalk() save_rds(), can run combination grid independently store results incrementally. run grid get insights well models prompts work train data. can experiment different models parameters much want see works .","code":"prompts <- tibble(prompt =           c( #Original prompt             'Classify this occupation response from a survey: {occupation}             Pick one of the following numerical codes from this list.              Respond only with the code!             {numerical_code_list}',             #Explicit instruction to avoid classifying something wrong            'Classify this occupation response from a survey: {occupation}             Pick one of the following numerical codes from this list.              Respond only with the code!             {numerical_code_list}                         If this does not look like a valid occupation response reply with              just 99            ',            #Shorter prompt            'Classify this occupation: {occupation}.             Respond only with one of the following codes:             {numerical_code_list}'          ),          prompt_id = 1:3)   grid <- expand_grid(train_data,                      prompts,                      model = c(\"llama3.2\", \"gemma2\")) |>   arrange(model) %>% # Arrange by model so ollama does not reload them often   rename(occupation = occupation_open) |>   rowwise() |>  # Glue together prompts and occupation row-by-row   mutate(prompt = glue(prompt)) |>   ungroup() |> # Ungroup after the rowwise operation   select(model,occupation,occ2,prompt_id,prompt)  nrow(grid) #> [1] 1854 grid_results <- grid |>   pmap_dfr(classify_occupation_grid,            api_function = ollama) #> Classifying: gemma2 - 1 - Computer network technician #> Classifying: gemma2 - 2 - Computer network technician #> Classifying: gemma2 - 3 - Computer network technician #> Classifying: ..."},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm_classifiers.html","id":"accuracy-estimates","dir":"Articles","previous_headings":"Optimizing and Testing Classifiers","what":"Accuracy estimates","title":"Classifying Texts with tidyllm","text":"create overview prediction accuracy overview using yardstick package. many functions use, also need encode ground truth model predictions factors: First just just calculate plot model accuracy:  main insights classification experiment: Gemma2 consistently outperforms llama3.2, even gemma2’s top performance Prompt 1 (79.1) falls far short Claude Sonnet’s 98% accuracy initial run. simplified prompt clearly introduces challenges, especially smaller models like llama3.2, shows particularly poor performance (low 20.2% Prompt 3). suggests models might generalize well slightly varied less explicit prompts, whereas Claude able handle variations far greater ease. Let’s look confusion matrix top gemma2 performance see whether specific occupation category causes problems : lot miss-classifications management occupations office services, missing occupations classified well, classified occupations. nothing seems like immediate easy fix. tricks goes beyond simple one-shot classification. common idea use multi-shot classifications (give model classification examples prompt, even guide output letting complete conversations answers right way). function df_llm_message() let’s easily built message histories put words mouth model. ways can help increase accuracy. popular one first let model reason detail best candidates classification build chain thought gives final answer.","code":"library(yardstick) #>  #> Attaching package: 'yardstick' #> The following object is masked from 'package:readr': #>  #>     spec  gr_factors <- grid_results |>   mutate(across(starts_with(\"occ2_\"),                 ~factor(.x,                         levels=c(occ_codes$occ2,97, 98,99),                         labels=c(occ_codes$occ_title,\"APIFAIL\",\"INVALID\",\"MISSING\")                 ))) accuracy <- gr_factors  |>   group_by(prompt_id, model) |>   accuracy(truth = occ2_truth, estimate = occ2_predict)  accuracy |>   ggplot(aes(x = as.factor(prompt_id), y = .estimate, fill = model)) +   geom_bar(stat = \"identity\", position = \"dodge\") +   labs(title = \"Accuracy by Prompt and Model\", x = \"Prompt ID\", y = \"Accuracy\", fill=\"\") +   theme_bw(22) +   scale_y_continuous(labels = scales::label_percent(),limits = c(0,1)) +   scale_fill_brewer(palette = \"Set1\") conf_mat <- gr_factors |>   filter(prompt_id == 1, model == \"gemma2\") |>   conf_mat(truth = occ2_truth, estimate = occ2_predict)  # Autoplot the confusion matrix autoplot(conf_mat, type = \"heatmap\") +   scale_fill_gradient(low = \"white\", high = \"#E41A1C\") +   ggtitle(\"Confusion Matrix for Model gemma2, Prompt 1\") +   theme_bw(22) +   theme(axis.text.x = element_text(angle = 45, hjust = 1)) #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale."},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm_classifiers.html","id":"multistep-chain-of-thought-prompting","dir":"Articles","previous_headings":"Optimizing and Testing Classifiers","what":"Multistep Chain-of-Thought Prompting","title":"Classifying Texts with tidyllm","text":"need major change classification function, send two messages model. first one elicits reasoning step, second one asks final code, based answer first message. easily doable message-chaining abilities tidyllm modified chain thought prompting function reasoning output step: Note increases compute time gemma2 massively now produces multiple output . run produces roughly page reasoning, second question generates number, original prompting strategy just gave us one number. run-times increase strongly. Note: test complicated prompting strategies like realtime, another feature tidyllm comes handy. can stream back responses just like online chatbot interfaces first test run .stream-argument api functions immediately see whether made strange errors prompting. Therefore, pass stream arguement trough classify_occupation_cot() first test function prompting single inputs scale whole data. Let’s run function gemma2: work. turns case ! accuracy even worse without reasoning step! One interesting thing though model right classification one candidates reasoning step 94% cases. probably, still way get better final result gemma2: specific case already clear favorite final choice without even needing testing, already know test performance Claude Sonnet 3.5, since manually checked . check prompt models test data needed . can just run entire sample claude_send_batch()","code":"classify_occupation_cot <- function(occupation,                                     occ2,                                     api_function,                                     model,                                     stream = FALSE){   # Output what the model is currently doing to the console   glue(\"Classifying with CoT: {model} - {occupation}\\n\") |> cat(\"\\n\")      # Step 1: Ask the model to think through the problem   prompt_reasoning <- glue('     Think about which of the following occupation codes would best describe this occupation description from a survey respondent: \"{occupation}\"          {numerical_code_list}          Explain your reasoning for the 3 top candidate codes step by step. Then evaluate which seems best.   ')      reasoning_response <- tryCatch({     conversation <<- llm_message(prompt_reasoning) |>       api_function(.model = model, .temperature = 0, .stream=stream)           conversation |>       get_reply()   }, error = function(e){     conversation <<- llm_message(\"Please classify this occupation response: {occupation}\")     \"Error in reasoning step.\"   })      # Step 2: Ask the model to provide the final answer   prompt_final <- glue('     Based on your reasoning, which code do you pick? Answer only with a numerical code!    ')      final_response <- tryCatch({     conversation |>     llm_message(prompt_final) |>       api_function(.model = model, .temperature = 0, .stream=stream) |>       get_reply() |>       str_squish()   }, error = function(e){     \"97\"   })      # Validate the model's final response   valid_codes <- as.character(occ_codes$occ2)      classification <- if (final_response %in% valid_codes) {     as.integer(final_response)   } else {     98L  # Return 98 for invalid responses   }      # Return a tibble containing the original occupation description and classification result   tibble(     occupation_open = occupation,     occ2_predict    = classification,     occ2_truth      = occ2,     model           = glue(\"{model}_cot\"),     reasoning       = reasoning_response,     final_response  = final_response   ) } results_cot <- grid |>   filter(model==\"gemma2\", prompt_id ==1) |>   select(-prompt,-prompt_id) |>   pmap_dfr(classify_occupation_cot,api_function = ollama, stream=FALSE) #> Classifying with CoT: gemma2 - 1 - Computer network technician #> Classifying with CoT: gemma2 - 2 - Educational support #> Classifying with CoT: gemma2 - 3 - Fine Carpentry #> Classifying with CoT: ... results_cot |>   mutate(across(starts_with(\"occ2_\"),                 ~factor(.x,                         levels=c(occ_codes$occ2,97, 98,99),                         labels=c(occ_codes$occ_title,\"APIFAIL\",\"INVALID\",\"MISSING\")                 )))  |>   accuracy(truth = occ2_truth, estimate = occ2_predict) #> # A tibble: 1 × 3 #>   .metric  .estimator .estimate #>   <chr>    <chr>          <dbl> #> 1 accuracy multiclass     0.754 results_cot |>   rowwise() |>   mutate(reasoning_classifications = str_extract_all(reasoning,\"\\\\d{2}\"),          reasoning_classifications = list(map_int(reasoning_classifications, as.integer)),          right_in_reasoning = occ2_truth %in% reasoning_classifications          ) |>   ungroup() |>   count(right_in_reasoning) |>   mutate(pct=n/sum(n)) #> # A tibble: 2 × 3 #>   right_in_reasoning     n    pct #>   <lgl>              <int>  <dbl> #> 1 FALSE                 19 0.0615 #> 2 TRUE                 290 0.939"},{"path":"https://edubruell.github.io/tidyllm/articles/tidyllm_classifiers.html","id":"conclusion","dir":"Articles","previous_headings":"","what":"Conclusion","title":"Classifying Texts with tidyllm","text":"article, ’ve demonstrated tidyllm can effectively used tackle complex text classification tasks. walking process step--step, sample selection prompt design model testing optimization, ’ve highlighted flexibility package dealing real-world data.","code":""},{"path":"https://edubruell.github.io/tidyllm/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Eduard Brüll. Author, maintainer.","code":""},{"path":"https://edubruell.github.io/tidyllm/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Brüll E (2024). tidyllm: Tidy Integration Large Language Models. https://github.com/edubruell/tidyllm, https://edubruell.github.io/tidyllm/.","code":"@Manual{,   title = {tidyllm: Tidy Integration of Large Language Models},   author = {Eduard Brüll},   year = {2024},   note = {https://github.com/edubruell/tidyllm, https://edubruell.github.io/tidyllm/}, }"},{"path":"https://edubruell.github.io/tidyllm/index.html","id":"tidyllm-","dir":"","previous_headings":"","what":"Tidy Integration of Large Language Models","title":"Tidy Integration of Large Language Models","text":"tidyllm R package designed access various large language model APIs, including Claude, ChatGPT, Groq, Mistral, local models via Ollama. Built simplicity functionality, helps generate text, analyze media, integrate model feedback data workflows ease.","code":""},{"path":"https://edubruell.github.io/tidyllm/index.html","id":"features","dir":"","previous_headings":"","what":"Features","title":"Tidy Integration of Large Language Models","text":"Multiple Model Support: Seamlessly switch various model providers like Claude, ChatGPT, Groq, Mistral Ollama using best offer. Media Handling: Extract process text PDFs capture console outputs messaging. Upload imagefiles last plotpane multimodal models. Interactive Messaging History: Manage ongoing conversation models, maintaining structured history messages media interactions, automatically formatted API Batch processing: Efficiently handle large workloads Anthropic OpenAI batch processing APIs, reducing costs 50%. Tidy Workflow: Use R’s functional programming features side-effect-free, pipeline-oriented operation style.","code":""},{"path":"https://edubruell.github.io/tidyllm/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Tidy Integration of Large Language Models","text":"install tidyllm CRAN, use: development version GitHub:","code":"install.packages(\"tidyllm\") # Install devtools if not already installed if (!requireNamespace(\"devtools\", quietly = TRUE)) {   install.packages(\"devtools\") } devtools::install_github(\"edubruell/tidyllm\")"},{"path":"https://edubruell.github.io/tidyllm/index.html","id":"basic-example","dir":"","previous_headings":"","what":"Basic Example","title":"Tidy Integration of Large Language Models","text":"’s quick example using tidyllm describe image using Claude model follow local open-source models: examples advanced usage, check Get Started vignette. Please note: use tidyllm, need either installation ollama active API key one supported providers (e.g., Claude, ChatGPT). See Get Started vignette setup instructions.","code":"library(\"tidyllm\")  # Describe an image with  claude conversation <- llm_message(\"Describe this image\",                                .imagefile = here(\"image.png\")) |>   claude()  # Use the description to query further with groq conversation |>   llm_message(\"Based on the previous description,   what could the research in the figure be about?\") |>   ollama(.model = \"gemma2\")"},{"path":"https://edubruell.github.io/tidyllm/index.html","id":"learn-more","dir":"","previous_headings":"","what":"Learn More","title":"Tidy Integration of Large Language Models","text":"detailed instructions advanced features, see: Get Started tidyllm Changelog Documentation Classifying Texts tidyllm Generate Synthetic Survey Data","code":""},{"path":"https://edubruell.github.io/tidyllm/index.html","id":"contributing","dir":"","previous_headings":"","what":"Contributing","title":"Tidy Integration of Large Language Models","text":"welcome contributions! Feel free open issues submit pull requests GitHub.","code":""},{"path":"https://edubruell.github.io/tidyllm/index.html","id":"license","dir":"","previous_headings":"","what":"License","title":"Tidy Integration of Large Language Models","text":"project licensed MIT License - see LICENSE file details.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":null,"dir":"Reference","previous_headings":"","what":"Large Language Model Message Class — LLMMessage","title":"Large Language Model Message Class — LLMMessage","text":"Large Language Model Message Class Large Language Model Message Class","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Large Language Model Message Class — LLMMessage","text":"class manages history messages media interactions intended use large language  models. allows adding messages, converting messages API usage, printing history structured format.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"Large Language Model Message Class — LLMMessage","text":"message_history List store message interactions. system_prompt system prompt used conversation","code":""},{"path":[]},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Large Language Model Message Class — LLMMessage","text":"LLMMessage$new() LLMMessage$clone_deep() LLMMessage$add_message() LLMMessage$to_api_format() LLMMessage$has_image() LLMMessage$remove_message() LLMMessage$print() LLMMessage$clone()","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Large Language Model Message Class — LLMMessage","text":"Initializes LLMMessage object optional system prompt.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Large Language Model Message Class — LLMMessage","text":"","code":"LLMMessage$new(system_prompt = \"You are a helpful assistant\")"},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Large Language Model Message Class — LLMMessage","text":"system_prompt string sets initial system prompt.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Large Language Model Message Class — LLMMessage","text":"new LLMMessage object. Deep Clone LLMMessage Object method creates deep copy LLMMessage object. ensures internal states, including message histories settings, copied original object remains unchanged mutations applied copy. particularly useful maintaining immutability tidyverse-like functional programming context functions side effects inputs.","code":""},{"path":[]},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Large Language Model Message Class — LLMMessage","text":"","code":"LLMMessage$clone_deep()"},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"Large Language Model Message Class — LLMMessage","text":"new LLMMessage object deep copy original. Add message Adds message history. Optionally includes media.","code":""},{"path":[]},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Large Language Model Message Class — LLMMessage","text":"","code":"LLMMessage$add_message(role, content, media = NULL, json = FALSE)"},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Large Language Model Message Class — LLMMessage","text":"role role message sender (e.g., \"user\", \"assistant\"). content textual content message. media Optional; media content attach message. json message raw string contains json response? Convert API format Converts message history format suitable various API calls.","code":""},{"path":[]},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Large Language Model Message Class — LLMMessage","text":"","code":"LLMMessage$to_api_format(   api_type,   cgpt_image_detail = \"auto\",   no_system = FALSE )"},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Large Language Model Message Class — LLMMessage","text":"api_type type API (e.g., \"claude\",\"groq\",\"openai\"). cgpt_image_detail Specific option ChatGPT API (imagedetail - set auto) no_system Without system prompt (default: FALSE)","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"returns-2","dir":"Reference","previous_headings":"","what":"Returns","title":"Large Language Model Message Class — LLMMessage","text":"message history target API format Simple helper function determine whether message history contains image check function whenever call models support images can post warning user images found sent model","code":""},{"path":[]},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"usage-4","dir":"Reference","previous_headings":"","what":"Usage","title":"Large Language Model Message Class — LLMMessage","text":"","code":"LLMMessage$has_image()"},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"returns-3","dir":"Reference","previous_headings":"","what":"Returns","title":"Large Language Model Message Class — LLMMessage","text":"Returns TRUE message hisotry contains images Remove Message Index Removes message message history specified index.","code":""},{"path":[]},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"usage-5","dir":"Reference","previous_headings":"","what":"Usage","title":"Large Language Model Message Class — LLMMessage","text":"","code":"LLMMessage$remove_message(index)"},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"Large Language Model Message Class — LLMMessage","text":"index positive integer indicating position message remove.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"returns-4","dir":"Reference","previous_headings":"","what":"Returns","title":"Large Language Model Message Class — LLMMessage","text":"LLMMessage object, invisibly.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"method-print-","dir":"Reference","previous_headings":"","what":"Method print()","title":"Large Language Model Message Class — LLMMessage","text":"Prints current message history structured format.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"usage-6","dir":"Reference","previous_headings":"","what":"Usage","title":"Large Language Model Message Class — LLMMessage","text":"","code":"LLMMessage$print()"},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Large Language Model Message Class — LLMMessage","text":"objects class cloneable method.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"usage-7","dir":"Reference","previous_headings":"","what":"Usage","title":"Large Language Model Message Class — LLMMessage","text":"","code":"LLMMessage$clone(deep = FALSE)"},{"path":"https://edubruell.github.io/tidyllm/reference/LLMMessage.html","id":"arguments-4","dir":"Reference","previous_headings":"","what":"Arguments","title":"Large Language Model Message Class — LLMMessage","text":"deep Whether make deep clone.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/azure_openai.html","id":null,"dir":"Reference","previous_headings":"","what":"Send LLM Messages to an OpenAI Chat Completions endpoint on Azure — azure_openai","title":"Send LLM Messages to an OpenAI Chat Completions endpoint on Azure — azure_openai","text":"function sends message history Azure OpenAI Chat Completions API returns assistant's reply. function work progress fully tested","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/azure_openai.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Send LLM Messages to an OpenAI Chat Completions endpoint on Azure — azure_openai","text":"","code":"azure_openai(   .llm,   .endpoint_url = Sys.getenv(\"AZURE_ENDPOINT_URL\"),   .deployment = \"gpt-4o-mini\",   .api_version = \"2024-08-01-preview\",   .max_completion_tokens = NULL,   .frequency_penalty = NULL,   .logit_bias = NULL,   .logprobs = FALSE,   .top_logprobs = NULL,   .presence_penalty = NULL,   .seed = NULL,   .stop = NULL,   .stream = FALSE,   .temperature = NULL,   .top_p = NULL,   .timeout = 60,   .verbose = FALSE,   .json = FALSE,   .json_schema = NULL,   .dry_run = FALSE,   .max_tries = 3 )"},{"path":"https://edubruell.github.io/tidyllm/reference/azure_openai.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Send LLM Messages to an OpenAI Chat Completions endpoint on Azure — azure_openai","text":".llm LLMMessage object containing conversation history. .endpoint_url Base URL API (default:  Sys.getenv(\"AZURE_ENDPOINT_URL\")). .deployment identifier model deployed (default: \"gpt-4o-mini\"). .api_version version API deployed (default: \"2024-08-01-preview\") .max_completion_tokens upper bound number tokens can generated completion, including visible output tokens reasoning tokens. .frequency_penalty Number -2.0 2.0. Positive values penalize new tokens based existing frequency text far. .logit_bias named list modifying likelihood specified tokens appearing completion. .logprobs Whether return log probabilities output tokens (default: FALSE). .top_logprobs integer 0 20 specifying number likely tokens return token position. .presence_penalty Number -2.0 2.0. Positive values penalize new tokens based whether appear text far. .seed specified, system make best effort sample deterministically. .stop 4 sequences API stop generating tokens. .stream set TRUE, answer streamed console comes (default: FALSE). .temperature sampling temperature use, 0 2. Higher values make output random. .top_p alternative sampling temperature, called nucleus sampling. .timeout Request timeout seconds (default: 60). .verbose additional information shown API call (default: FALSE). .json output JSON mode (default: FALSE). .json_schema JSON schema object R list enforce output structure (defined precedence JSON mode). .dry_run TRUE, perform dry run return request object (default: FALSE). .max_tries Maximum retries perform request","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/azure_openai.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Send LLM Messages to an OpenAI Chat Completions endpoint on Azure — azure_openai","text":"new LLMMessage object containing original messages plus assistant's response.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/azure_openai.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Send LLM Messages to an OpenAI Chat Completions endpoint on Azure — azure_openai","text":"","code":"if (FALSE) { # \\dontrun{ # Basic usage msg <- llm_message(\"What is R programming?\") result <- azure_openai(msg)  # With custom parameters result2 <- azure_openai(msg,                   .deployment = \"gpt-4o-mini\",                  .temperature = 0.7,                   .max_tokens = 1000) } # }"},{"path":"https://edubruell.github.io/tidyllm/reference/chatgpt.html","id":null,"dir":"Reference","previous_headings":"","what":"ChatGPT Wrapper (Deprecated) — chatgpt","title":"ChatGPT Wrapper (Deprecated) — chatgpt","text":"Provides wrapper openai() function facilitate migration deprecated chatgpt() function. ensures backward compatibility allowing users transition updated features.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/chatgpt.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"ChatGPT Wrapper (Deprecated) — chatgpt","text":"","code":"chatgpt(   .llm,   .model = \"gpt-4o\",   .max_tokens = 1024,   .temperature = NULL,   .top_p = NULL,   .top_k = NULL,   .frequency_penalty = NULL,   .presence_penalty = NULL,   .api_url = \"https://api.openai.com/\",   .timeout = 60,   .verbose = FALSE,   .json = FALSE,   .stream = FALSE,   .dry_run = FALSE )"},{"path":"https://edubruell.github.io/tidyllm/reference/chatgpt.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"ChatGPT Wrapper (Deprecated) — chatgpt","text":".llm LLMMessage (passed directly openai() function) .model character string specifying model use. .max_tokens integer specifying maximum number tokens  (mapped .max_completion_tokens openai()) .temperature numeric value controlling randomness. .top_p numeric value nucleus sampling, indicating top .top_k Currently unused, supported openai(). .frequency_penalty numeric value penalizes new tokens based frequency far. .presence_penalty numeric value penalizes new tokens based whether appear text far. .api_url Character string specifying API URL. Defaults OpenAI API endpoint. .timeout integer specifying request timeout seconds. .verbose print additional information request (default: false) .json json-mode used? (detault: false) .stream response  processed stream (default: false) .dry_run Shouldthe request constructed actually sent. Useful debugging testing. (default: false)","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/chatgpt.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"ChatGPT Wrapper (Deprecated) — chatgpt","text":"LLMMessage object assistant's reply.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/chatgpt.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"ChatGPT Wrapper (Deprecated) — chatgpt","text":"function deprecated now wrapper around openai(). recommended switch using openai() directly future code. chatgpt() function remains available ensure backward compatibility existing projects.","code":""},{"path":[]},{"path":"https://edubruell.github.io/tidyllm/reference/chatgpt.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"ChatGPT Wrapper (Deprecated) — chatgpt","text":"","code":"if (FALSE) { # \\dontrun{ # Using the deprecated chatgpt() function result <- chatgpt(.llm = llm_message(), .prompt = \"Hello, how are you?\") } # }"},{"path":"https://edubruell.github.io/tidyllm/reference/check_claude_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Check Batch Processing Status for Claude API — check_claude_batch","title":"Check Batch Processing Status for Claude API — check_claude_batch","text":"function retrieves processing status details specified Claude batch ID Claude API.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/check_claude_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check Batch Processing Status for Claude API — check_claude_batch","text":"","code":"check_claude_batch(   .llms = NULL,   .batch_id = NULL,   .api_url = \"https://api.anthropic.com/\",   .dry_run = FALSE,   .max_tries = 3,   .timeout = 60 )"},{"path":"https://edubruell.github.io/tidyllm/reference/check_claude_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check Batch Processing Status for Claude API — check_claude_batch","text":".llms list LLMMessage objects .batch_id manually set batchid .api_url Character; base URL Claude API (default: \"https://api.anthropic.com/\"). .dry_run Logical; TRUE, returns prepared request object without executing (default: FALSE). .max_tries Maximum retries peform request .timeout Integer specifying request timeout seconds (default: 60).","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/check_claude_batch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check Batch Processing Status for Claude API — check_claude_batch","text":"tibble information status batch processing","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/check_openai_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Check Batch Processing Status for OpenAI Batch API — check_openai_batch","title":"Check Batch Processing Status for OpenAI Batch API — check_openai_batch","text":"function retrieves processing status details specified OpenAI batch ID OpenAI Batch API.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/check_openai_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check Batch Processing Status for OpenAI Batch API — check_openai_batch","text":"","code":"check_openai_batch(   .llms = NULL,   .batch_id = NULL,   .dry_run = FALSE,   .max_tries = 3,   .timeout = 60 )"},{"path":"https://edubruell.github.io/tidyllm/reference/check_openai_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check Batch Processing Status for OpenAI Batch API — check_openai_batch","text":".llms list LLMMessage objects. .batch_id manually set batch ID. .dry_run Logical; TRUE, returns prepared request object without executing (default: FALSE). .max_tries Maximum retries perform request (default: 3). .timeout Integer specifying request timeout seconds (default: 60).","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/check_openai_batch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check Batch Processing Status for OpenAI Batch API — check_openai_batch","text":"tibble information status batch processing.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/claude.html","id":null,"dir":"Reference","previous_headings":"","what":"Interact with Claude AI models via the Anthropic API — claude","title":"Interact with Claude AI models via the Anthropic API — claude","text":"Interact Claude AI models via Anthropic API","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/claude.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Interact with Claude AI models via the Anthropic API — claude","text":"","code":"claude(   .llm,   .model = \"claude-3-5-sonnet-20241022\",   .max_tokens = 1024,   .temperature = NULL,   .top_k = NULL,   .top_p = NULL,   .metadata = NULL,   .stop_sequences = NULL,   .tools = NULL,   .api_url = \"https://api.anthropic.com/\",   .verbose = FALSE,   .max_tries = 3,   .timeout = 60,   .stream = FALSE,   .dry_run = FALSE )"},{"path":"https://edubruell.github.io/tidyllm/reference/claude.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Interact with Claude AI models via the Anthropic API — claude","text":".llm LLMMessage object containing conversation history system prompt. .model Character string specifying Claude model version (default: \"claude-3-5-sonnet-20241022\"). .max_tokens Integer specifying maximum number tokens response (default: 1024). .temperature Numeric 0 1 controlling response randomness. .top_k Integer controlling diversity limiting top K tokens. .top_p Numeric 0 1 nucleus sampling. .metadata List additional metadata include request. .stop_sequences Character vector sequences halt response generation. .tools List additional tools functions model can use. .api_url Base URL Anthropic API (default: \"https://api.anthropic.com/\"). .verbose Logical; TRUE, displays additional information API call (default: FALSE). .max_tries Maximum retries peform request .timeout Integer specifying request timeout seconds (default: 60). .stream Logical; TRUE, streams response piece piece (default: FALSE). .dry_run Logical; TRUE, returns prepared request object without executing (default: FALSE).","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/claude.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Interact with Claude AI models via the Anthropic API — claude","text":"new LLMMessage object containing original messages plus Claude's response.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/claude.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Interact with Claude AI models via the Anthropic API — claude","text":"","code":"if (FALSE) { # \\dontrun{ # Basic usage msg <- llm_message(\"What is R programming?\") result <- claude(msg)  # With custom parameters result2 <- claude(msg,                   .temperature = 0.7,                   .max_tokens = 1000) } # }"},{"path":"https://edubruell.github.io/tidyllm/reference/df_llm_message.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert a Data Frame to an LLMMessage Object — df_llm_message","title":"Convert a Data Frame to an LLMMessage Object — df_llm_message","text":"function takes data frame converts LLMMessage object representing conversation history. data frame contain specific columns (role content) row representing message conversation.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/df_llm_message.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert a Data Frame to an LLMMessage Object — df_llm_message","text":"","code":"df_llm_message(.df)"},{"path":"https://edubruell.github.io/tidyllm/reference/df_llm_message.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert a Data Frame to an LLMMessage Object — df_llm_message","text":".df data frame least two rows columns role content. column role contain values \"user\", \"assistant\", \"system\", content type character.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/df_llm_message.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert a Data Frame to an LLMMessage Object — df_llm_message","text":"LLMMessage object containing structured messages per input data frame.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/fetch_claude_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Fetch Results for a Claude Batch — fetch_claude_batch","title":"Fetch Results for a Claude Batch — fetch_claude_batch","text":"function retrieves results completed Claude batch updates provided list LLMMessage objects responses. aligns response original request using custom_ids generated send_claude_batch().","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/fetch_claude_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fetch Results for a Claude Batch — fetch_claude_batch","text":"","code":"fetch_claude_batch(   .llms,   .batch_id = NULL,   .api_url = \"https://api.anthropic.com/\",   .dry_run = FALSE,   .max_tries = 3,   .timeout = 60 )"},{"path":"https://edubruell.github.io/tidyllm/reference/fetch_claude_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fetch Results for a Claude Batch — fetch_claude_batch","text":".llms list LLMMessage objects part batch. list names (custom IDs) set send_claude_batch() ensure correct alignment. .batch_id Character; unique identifier batch. default NULL function attempt use batch_id attribute .llms. .api_url Character; base URL Claude API (default: \"https://api.anthropic.com/\"). .dry_run Logical; TRUE, returns constructed request without executing (default: FALSE). .max_tries Integer; maximum number retries request fails (default: 3). .timeout Integer; request timeout seconds (default: 60).","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/fetch_claude_batch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fetch Results for a Claude Batch — fetch_claude_batch","text":"list updated LLMMessage objects, assistant's response added successful.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/generate_callback_function.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate API-Specific Callback Function for Streaming Responses — generate_callback_function","title":"Generate API-Specific Callback Function for Streaming Responses — generate_callback_function","text":"function generates callback function processes streaming responses different language model APIs. callback function specific API provided (claude, ollama, \"mistral\", openai) processes incoming data streams, printing content console updating global environment use.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/generate_callback_function.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate API-Specific Callback Function for Streaming Responses — generate_callback_function","text":"","code":"generate_callback_function(.api)"},{"path":"https://edubruell.github.io/tidyllm/reference/generate_callback_function.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate API-Specific Callback Function for Streaming Responses — generate_callback_function","text":".api character string indicating API type. Supported values \"claude\", \"ollama\", \"mistral\", \"groq\" \"openai\".","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/generate_callback_function.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate API-Specific Callback Function for Streaming Responses — generate_callback_function","text":"function serves callback handle streaming responses specified API. callback function processes raw data, updates .tidyllm_stream_env$stream object, prints streamed content console. function returns TRUE streaming continue, FALSE streaming finished.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/generate_callback_function.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generate API-Specific Callback Function for Streaming Responses — generate_callback_function","text":"Claude API: function processes event data lines, handles message_start message_stop events control streaming flow. Ollama API: function directly parses stream content JSON extracts message$content field. OpenAI, Mistral Groq: function handles JSON data streams processes content deltas. stops processing [DONE] message encountered.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/get_reply.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Assistant Reply as Text — get_reply","title":"Get Assistant Reply as Text — get_reply","text":"Retrieves assistant's reply plain text LLMMessage object specified index.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/get_reply.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Assistant Reply as Text — get_reply","text":"","code":"get_reply(.llm, .index = NULL)"},{"path":"https://edubruell.github.io/tidyllm/reference/get_reply.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Assistant Reply as Text — get_reply","text":".llm LLMMessage object containing message history. .index positive integer assistant reply index retrieve, defaulting last reply.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/get_reply.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Assistant Reply as Text — get_reply","text":"Plain text content assistant's reply, NA_character_ reply available.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/get_reply_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Data from an Assistant Reply by parsing structured JSON responses — get_reply_data","title":"Get Data from an Assistant Reply by parsing structured JSON responses — get_reply_data","text":"Retrieves parses assistant's reply JSON LLMMessage object specified index. attempts parse reply marked JSON; otherwise, returns NULL.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/get_reply_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Data from an Assistant Reply by parsing structured JSON responses — get_reply_data","text":"","code":"get_reply_data(.llm, .index = NULL)"},{"path":"https://edubruell.github.io/tidyllm/reference/get_reply_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Data from an Assistant Reply by parsing structured JSON responses — get_reply_data","text":".llm LLMMessage object containing message history. .index positive integer assistant reply index retrieve, defaulting last reply.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/get_reply_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Data from an Assistant Reply by parsing structured JSON responses — get_reply_data","text":"Parsed data content assistant's reply, NULL reply JSON parsing fails.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/get_user_message.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve a User Message by Index — get_user_message","title":"Retrieve a User Message by Index — get_user_message","text":"Extracts content user's message LLMMessage object specific index.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/get_user_message.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve a User Message by Index — get_user_message","text":"","code":"get_user_message(.llm, .index = NULL)"},{"path":"https://edubruell.github.io/tidyllm/reference/get_user_message.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve a User Message by Index — get_user_message","text":".llm LLMMessage object. .index positive integer indicating user message retrieve. Defaults NULL, retrieves last message.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/get_user_message.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieve a User Message by Index — get_user_message","text":"Returns content user's message specified index. messages found, returns NULL.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/groq.html","id":null,"dir":"Reference","previous_headings":"","what":"Send LLM Messages to the Groq Chat API — groq","title":"Send LLM Messages to the Groq Chat API — groq","text":"function sends message history Groq Chat API returns assistant's reply.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/groq.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Send LLM Messages to the Groq Chat API — groq","text":"","code":"groq(   .llm,   .model = \"llama-3.2-11b-vision-preview\",   .max_tokens = 1024,   .temperature = NULL,   .top_p = NULL,   .frequency_penalty = NULL,   .presence_penalty = NULL,   .stop = NULL,   .seed = NULL,   .api_url = \"https://api.groq.com/\",   .json = FALSE,   .timeout = 60,   .verbose = FALSE,   .stream = FALSE,   .dry_run = FALSE,   .max_tries = 3 )"},{"path":"https://edubruell.github.io/tidyllm/reference/groq.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Send LLM Messages to the Groq Chat API — groq","text":".llm LLMMessage object containing conversation history. .model identifier model use (default: \"llama-3.2-11b-vision-preview\"). .max_tokens maximum number tokens can generated response (default: 1024). .temperature Controls randomness model's response. Values 0 2 allowed, higher values increase randomness (optional). .top_p Nucleus sampling parameter controls proportion probability mass considered. Values 0 1 allowed (optional). .frequency_penalty Number -2.0 2.0. Positive values penalize repeated tokens, reducing likelihood repetition (optional). .presence_penalty Number -2.0 2.0. Positive values encourage new topics penalizing tokens appeared far (optional). .stop One sequences API stop generating tokens. Can string list strings (optional). .seed integer deterministic sampling. specified, attempts return result repeated requests identical parameters (optional). .api_url Base URL Groq API (default: \"https://api.groq.com/\"). .json Whether response structured JSON (default: FALSE). .timeout Request timeout seconds (default: 60). .verbose TRUE, displays additional information API call, including rate limit details (default: FALSE). .stream Logical; TRUE, streams response piece piece (default: FALSE). .dry_run TRUE, performs dry run returns constructed request object without executing (default: FALSE). .max_tries Maximum retries peform request","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/groq.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Send LLM Messages to the Groq Chat API — groq","text":"new LLMMessage object containing original messages plus assistant's response.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/groq.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Send LLM Messages to the Groq Chat API — groq","text":"","code":"if (FALSE) { # \\dontrun{ # Basic usage msg <- llm_message(\"What is Groq?\") result <- groq(msg)  # With custom parameters result2 <- groq(msg,                 .model = \"llama-3.2-vision\",                .temperature = 0.5,                 .max_tokens = 512) } # }"},{"path":"https://edubruell.github.io/tidyllm/reference/groq_transcribe.html","id":null,"dir":"Reference","previous_headings":"","what":"Transcribe an Audio File Using Groq transcription API — groq_transcribe","title":"Transcribe an Audio File Using Groq transcription API — groq_transcribe","text":"function reads audio file sends Groq transcription API transcription.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/groq_transcribe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Transcribe an Audio File Using Groq transcription API — groq_transcribe","text":"","code":"groq_transcribe(   .audio_file,   .model = \"whisper-large-v3\",   .language = NULL,   .prompt = NULL,   .temperature = 0,   .api_url = \"https://api.groq.com/openai/v1/audio/transcriptions\",   .dry_run = FALSE,   .verbose = FALSE,   .max_tries = 3 )"},{"path":"https://edubruell.github.io/tidyllm/reference/groq_transcribe.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Transcribe an Audio File Using Groq transcription API — groq_transcribe","text":".audio_file path audio file (required). Supported formats include flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, webm. .model model use transcription (default: \"whisper-large-v3\"). .language language input audio, ISO-639-1 format (optional). .prompt prompt guide transcription style. match audio language (optional). .temperature Sampling temperature, 0 1, higher values producing randomness (default: 0). .api_url Base URL API (default: \"https://api.groq.com/openai/v1/audio/transcriptions\"). .dry_run Logical; TRUE, performs dry run returns request object without making API call (default: FALSE). .verbose Logical; TRUE, rate limiting info displayed API request (default: FALSE). .max_tries Maximum retries peform request","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/groq_transcribe.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Transcribe an Audio File Using Groq transcription API — groq_transcribe","text":"character vector containing transcription.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/groq_transcribe.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Transcribe an Audio File Using Groq transcription API — groq_transcribe","text":"","code":"if (FALSE) { # \\dontrun{ # Basic usage groq_transcribe(.audio_file = \"example.mp3\") } # }"},{"path":"https://edubruell.github.io/tidyllm/reference/initialize_api_env.html","id":null,"dir":"Reference","previous_headings":"","what":"Initialize or Retrieve API-specific Environment — initialize_api_env","title":"Initialize or Retrieve API-specific Environment — initialize_api_env","text":"function initializes named environment storing rate limit information specific API. ensures API's rate limit data stored separately.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/initialize_api_env.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Initialize or Retrieve API-specific Environment — initialize_api_env","text":"","code":"initialize_api_env(.api_name)"},{"path":"https://edubruell.github.io/tidyllm/reference/initialize_api_env.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Initialize or Retrieve API-specific Environment — initialize_api_env","text":".api_name name API initialize retrieve environment","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/last_reply.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the Last Assistant Reply as Text — last_reply","title":"Get the Last Assistant Reply as Text — last_reply","text":"wrapper around get_reply() retrieve recent assistant text reply.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/last_reply.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the Last Assistant Reply as Text — last_reply","text":"","code":"last_reply(.llm)"},{"path":"https://edubruell.github.io/tidyllm/reference/last_reply.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the Last Assistant Reply as Text — last_reply","text":".llm LLMMessage object.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/last_reply.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the Last Assistant Reply as Text — last_reply","text":"Returns content assistant's reply specified index, based following conditions:","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/last_reply_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the Last Assistant Reply as Text — last_reply_data","title":"Get the Last Assistant Reply as Text — last_reply_data","text":"wrapper around get_reply_data() retrieve structured data recent assistant reply.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/last_reply_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the Last Assistant Reply as Text — last_reply_data","text":"","code":"last_reply_data(.llm)"},{"path":"https://edubruell.github.io/tidyllm/reference/last_reply_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the Last Assistant Reply as Text — last_reply_data","text":".llm LLMMessage object.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/last_reply_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the Last Assistant Reply as Text — last_reply_data","text":"Returns content assistant's reply specified index, based following conditions:","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/last_user_message.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve the Last User Message — last_user_message","title":"Retrieve the Last User Message — last_user_message","text":"wrapper around get_user_message() retrieve recent user message.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/last_user_message.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve the Last User Message — last_user_message","text":"","code":"last_user_message(.llm)"},{"path":"https://edubruell.github.io/tidyllm/reference/last_user_message.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve the Last User Message — last_user_message","text":".llm LLMMessage object.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/last_user_message.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieve the Last User Message — last_user_message","text":"content last user message.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/list_claude_batches.html","id":null,"dir":"Reference","previous_headings":"","what":"List Claude Batch Requests — list_claude_batches","title":"List Claude Batch Requests — list_claude_batches","text":"Retrieves batch request details Claude API.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/list_claude_batches.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List Claude Batch Requests — list_claude_batches","text":"","code":"list_claude_batches(   .api_url = \"https://api.anthropic.com/\",   .limit = 20,   .max_tries = 3,   .timeout = 60 )"},{"path":"https://edubruell.github.io/tidyllm/reference/list_claude_batches.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List Claude Batch Requests — list_claude_batches","text":".api_url Base URL Claude API (default: \"https://api.anthropic.com/\"). .limit Maximum number batches retrieve (default: 20). .max_tries Maximum retry attempts requests (default: 3). .timeout Request timeout seconds (default: 60).","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/list_claude_batches.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List Claude Batch Requests — list_claude_batches","text":"tibble batch details: batch ID, status, creation time, expiration time, request counts (succeeded, errored, expired, canceled).","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/list_openai_batches.html","id":null,"dir":"Reference","previous_headings":"","what":"List OpenAI Batch Requests — list_openai_batches","title":"List OpenAI Batch Requests — list_openai_batches","text":"Retrieves batch request details OpenAI Batch API.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/list_openai_batches.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List OpenAI Batch Requests — list_openai_batches","text":"","code":"list_openai_batches(.limit = 20, .max_tries = 3, .timeout = 60)"},{"path":"https://edubruell.github.io/tidyllm/reference/list_openai_batches.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List OpenAI Batch Requests — list_openai_batches","text":".limit Maximum number batches retrieve (default: 20). .max_tries Maximum retry attempts requests (default: 3). .timeout Request timeout seconds (default: 60).","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/list_openai_batches.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List OpenAI Batch Requests — list_openai_batches","text":"tibble batch details: batch ID, status, creation time, expiration time, request counts (total, completed, failed).","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/llm_message.html","id":null,"dir":"Reference","previous_headings":"","what":"Create or Update Large Language Model Message Object — llm_message","title":"Create or Update Large Language Model Message Object — llm_message","text":"function allows creation new LLMMessage object updating existing one. can handle addition text prompts various media types images, PDFs, text files, plots. function includes input validation ensure provided parameters correct format.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/llm_message.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create or Update Large Language Model Message Object — llm_message","text":"","code":"llm_message(   .llm = NULL,   .prompt = NULL,   .role = \"user\",   .system_prompt = \"You are a helpful assistant\",   .imagefile = NULL,   .pdf = NULL,   .textfile = NULL,   .capture_plot = FALSE,   .f = NULL )"},{"path":"https://edubruell.github.io/tidyllm/reference/llm_message.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create or Update Large Language Model Message Object — llm_message","text":".llm existing LLMMessage object initial text prompt. .prompt Text prompt add message history. .role role message sender, typically \"user\" \"assistant\". .system_prompt Default system prompt new LLMMessage needs created. .imagefile Path image file attached (optional). .pdf Path PDF file attached (optional). Can character vector length one (file path), list filename, start_page, end_page. .textfile Path text file read attached (optional). .capture_plot Boolean indicate whether plot captured attached image (optional). .f R function object coercible function via rlang::as_function, whose output captured attached (optional).","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/llm_message.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create or Update Large Language Model Message Object — llm_message","text":"Returns updated new LLMMessage object.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/mistral.html","id":null,"dir":"Reference","previous_headings":"","what":"Send LLMMessage to Mistral API — mistral","title":"Send LLMMessage to Mistral API — mistral","text":"Send LLMMessage Mistral API","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/mistral.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Send LLMMessage to Mistral API — mistral","text":"","code":"mistral(   .llm,   .model = \"mistral-large-latest\",   .stream = FALSE,   .seed = NULL,   .json = FALSE,   .temperature = 0.7,   .top_p = 1,   .stop = NULL,   .safe_prompt = FALSE,   .timeout = 120,   .max_tries = 3,   .max_tokens = 1024,   .min_tokens = NULL,   .dry_run = FALSE,   .verbose = FALSE )"},{"path":"https://edubruell.github.io/tidyllm/reference/mistral.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Send LLMMessage to Mistral API — mistral","text":".llm LLMMessage object. .model model identifier use (default: \"mistral-large-latest\"). .stream Whether stream back partial progress console. (default: FALSE). .seed seed use random sampling. set, different calls generate deterministic results (optional). .json Whether output JSON mode(default: FALSE). .temperature Sampling temperature use, 0.0 1.5. Higher values make output random, lower values make focused deterministic (default: 0.7). .top_p Nucleus sampling parameter, 0.0 1.0. model considers tokens top_p probability mass (default: 1). .stop Stop generation token detected, one tokens detected providing list (optional). .safe_prompt Whether inject safety prompt conversations (default: FALSE). .timeout connection time seconds (default: 120). .max_tries Maximum retries peform request .max_tokens maximum number tokens generate completion. Must >= 0 (default: 1024). .min_tokens minimum number tokens generate completion. Must >= 0 (optional). .dry_run TRUE, perform dry run return request object (default: FALSE). .verbose additional information shown API call? (default: FALSE)","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/mistral.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Send LLMMessage to Mistral API — mistral","text":"Returns updated LLMMessage object.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/mistral_embedding.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Embeddings Using Mistral API — mistral_embedding","title":"Generate Embeddings Using Mistral API — mistral_embedding","text":"Generate Embeddings Using Mistral API","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/mistral_embedding.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Embeddings Using Mistral API — mistral_embedding","text":"","code":"mistral_embedding(   .llm,   .model = \"mistral-embed\",   .timeout = 120,   .max_tries = 3,   .dry_run = FALSE )"},{"path":"https://edubruell.github.io/tidyllm/reference/mistral_embedding.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Embeddings Using Mistral API — mistral_embedding","text":".llm existing LLMMessage object (character vector texts embed) .model embedding model identifier (default: \"mistral-embed\"). .timeout Timeout API request seconds (default: 120). .max_tries Maximum retries peform request .dry_run TRUE, perform dry run return request object.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/mistral_embedding.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Embeddings Using Mistral API — mistral_embedding","text":"matrix column corresponds embedding message message history.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/ollama.html","id":null,"dir":"Reference","previous_headings":"","what":"Interact with local AI models via the Ollama API — ollama","title":"Interact with local AI models via the Ollama API — ollama","text":"Interact local AI models via Ollama API","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/ollama.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Interact with local AI models via the Ollama API — ollama","text":"","code":"ollama(   .llm,   .model = \"gemma2\",   .stream = FALSE,   .seed = NULL,   .json = FALSE,   .temperature = NULL,   .num_ctx = 2048,   .num_predict = NULL,   .top_k = NULL,   .top_p = NULL,   .min_p = NULL,   .mirostat = NULL,   .mirostat_eta = NULL,   .mirostat_tau = NULL,   .repeat_last_n = NULL,   .repeat_penalty = NULL,   .tfs_z = NULL,   .stop = NULL,   .ollama_server = \"http://localhost:11434\",   .timeout = 120,   .keep_alive = NULL,   .dry_run = FALSE )"},{"path":"https://edubruell.github.io/tidyllm/reference/ollama.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Interact with local AI models via the Ollama API — ollama","text":".llm LLMMessage object containing conversation history system prompt. .model Character string specifying Ollama model use (default: \"gemma2\") .stream Logical; whether stream response (default: FALSE) .seed Integer; seed reproducible generation (default: NULL) .json Logical; whether format response JSON (default: FALSE) .temperature Float 0-2; controls randomness responses (default: NULL) .num_ctx Integer; sets context window size (default: 2048) .num_predict Integer; maximum number tokens predict (default: NULL) .top_k Integer; controls diversity limiting top tokens considered (default: NULL) .top_p Float 0-1; nucleus sampling threshold (default: NULL) .min_p Float 0-1; minimum probability threshold (default: NULL) .mirostat Integer (0,1,2); enables Mirostat sampling algorithm (default: NULL) .mirostat_eta Float; Mirostat learning rate (default: NULL) .mirostat_tau Float; Mirostat target entropy (default: NULL) .repeat_last_n Integer; tokens look back repetition (default: NULL) .repeat_penalty Float; penalty repeated tokens (default: NULL) .tfs_z Float; tail free sampling parameter (default: NULL) .stop Character; custom stop sequence(s) (default: NULL) .ollama_server String; Ollama API endpoint (default: \"http://localhost:11434\") .timeout Integer; API request timeout seconds (default: 120) .keep_alive Character; long ollama model kept memory request (default: NULL - 5 Minutes) .dry_run Logical; TRUE, returns request object without execution (default: FALSE)","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/ollama.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Interact with local AI models via the Ollama API — ollama","text":"new LLMMessage object containing original messages plus model's response","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/ollama.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Interact with local AI models via the Ollama API — ollama","text":"function provides extensive control generation process various parameters: Temperature (0-2): Higher values increase creativity, lower values make responses focused Top-k/Top-p: Control diversity generated text Mirostat: Advanced sampling algorithm maintaining consistent complexity Repeat penalties: Prevent repetitive text Context window: Control much previous conversation considered","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/ollama.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Interact with local AI models via the Ollama API — ollama","text":"","code":"if (FALSE) { # \\dontrun{ llm_message(\"user\", \"Hello, how are you?\") response <- ollama(llm, .model = \"gemma2\", .temperature = 0.7)  # With custom parameters response <- ollama(   llm,   .model = \"llama2\",   .temperature = 0.8,   .top_p = 0.9,   .num_ctx = 4096 ) } # }"},{"path":"https://edubruell.github.io/tidyllm/reference/ollama_download_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Download a model from the Ollama API — ollama_download_model","title":"Download a model from the Ollama API — ollama_download_model","text":"function sends request Ollama API download specified model. can operate streaming mode provides live updates download status progress, single response mode.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/ollama_download_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Download a model from the Ollama API — ollama_download_model","text":"","code":"ollama_download_model(.model, .ollama_server = \"http://localhost:11434\")"},{"path":"https://edubruell.github.io/tidyllm/reference/ollama_download_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Download a model from the Ollama API — ollama_download_model","text":".model name model download. .ollama_server base URL Ollama API (default \"http://localhost:11434\").","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/ollama_embedding.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Embeddings Using Ollama API — ollama_embedding","title":"Generate Embeddings Using Ollama API — ollama_embedding","text":"Generate Embeddings Using Ollama API","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/ollama_embedding.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Embeddings Using Ollama API — ollama_embedding","text":"","code":"ollama_embedding(   .llm,   .model = \"all-minilm\",   .truncate = TRUE,   .ollama_server = \"http://localhost:11434\",   .timeout = 120,   .dry_run = FALSE )"},{"path":"https://edubruell.github.io/tidyllm/reference/ollama_embedding.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Embeddings Using Ollama API — ollama_embedding","text":".llm existing LLMMessage object (charachter vector texts embed) .model embedding model identifier (default: \"-minilm\"). .truncate Whether truncate inputs fit model's context length (default: TRUE). .ollama_server URL Ollama server used (default: \"http://localhost:11434\"). .timeout Timeout API request seconds (default: 120). .dry_run TRUE, perform dry run return request object.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/ollama_embedding.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Embeddings Using Ollama API — ollama_embedding","text":"matrix column corresponds embedding message message history.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/ollama_list_models.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve and return model information from the Ollama API — ollama_list_models","title":"Retrieve and return model information from the Ollama API — ollama_list_models","text":"function connects Ollama API retrieves information available models, returning tibble.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/ollama_list_models.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve and return model information from the Ollama API — ollama_list_models","text":"","code":"ollama_list_models(.ollama_server = \"http://localhost:11434\")"},{"path":"https://edubruell.github.io/tidyllm/reference/ollama_list_models.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve and return model information from the Ollama API — ollama_list_models","text":".ollama_server URL ollama server used","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/ollama_list_models.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieve and return model information from the Ollama API — ollama_list_models","text":"tibble containing model information, NULL models found.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/openai.html","id":null,"dir":"Reference","previous_headings":"","what":"Send LLM Messages to the OpenAI Chat Completions API — openai","title":"Send LLM Messages to the OpenAI Chat Completions API — openai","text":"function sends message history OpenAI Chat Completions API returns assistant's reply.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/openai.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Send LLM Messages to the OpenAI Chat Completions API — openai","text":"","code":"openai(   .llm,   .model = \"gpt-4o\",   .max_completion_tokens = NULL,   .frequency_penalty = NULL,   .logit_bias = NULL,   .logprobs = FALSE,   .top_logprobs = NULL,   .presence_penalty = NULL,   .seed = NULL,   .service_tier = \"auto\",   .stop = NULL,   .stream = FALSE,   .temperature = NULL,   .top_p = NULL,   .api_url = \"https://api.openai.com/\",   .timeout = 60,   .verbose = FALSE,   .json = FALSE,   .json_schema = NULL,   .max_tries = 3,   .dry_run = FALSE )"},{"path":"https://edubruell.github.io/tidyllm/reference/openai.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Send LLM Messages to the OpenAI Chat Completions API — openai","text":".llm LLMMessage object containing conversation history. .model identifier model use (default: \"gpt-4o\"). .max_completion_tokens upper bound number tokens can generated completion, including visible output tokens reasoning tokens. .frequency_penalty Number -2.0 2.0. Positive values penalize new tokens based existing frequency text far. .logit_bias named list modifying likelihood specified tokens appearing completion. .logprobs Whether return log probabilities output tokens (default: FALSE). .top_logprobs integer 0 20 specifying number likely tokens return token position. .presence_penalty Number -2.0 2.0. Positive values penalize new tokens based whether appear text far. .seed specified, system make best effort sample deterministically. .service_tier Specifies latency tier use processing request (default: \"auto\"). .stop 4 sequences API stop generating tokens. .stream set TRUE, answer streamed console comes (default: FALSE). .temperature sampling temperature use, 0 2. Higher values make output random. .top_p alternative sampling temperature, called nucleus sampling. .api_url Base URL API (default: \"https://api.openai.com/\"). .timeout Request timeout seconds (default: 60). .verbose additional information shown API call (default: FALSE). .json output JSON mode (default: FALSE). .json_schema JSON schema object R list enforce output structure (defined precedence JSON mode). .max_tries Maximum retries peform request .dry_run TRUE, perform dry run return request object (default: FALSE).","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/openai.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Send LLM Messages to the OpenAI Chat Completions API — openai","text":"new LLMMessage object containing original messages plus assistant's response.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/openai_embedding.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Embeddings Using OpenAI API — openai_embedding","title":"Generate Embeddings Using OpenAI API — openai_embedding","text":"Generate Embeddings Using OpenAI API","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/openai_embedding.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Embeddings Using OpenAI API — openai_embedding","text":"","code":"openai_embedding(   .llm,   .model = \"text-embedding-3-small\",   .truncate = TRUE,   .timeout = 120,   .dry_run = FALSE,   .max_tries = 3 )"},{"path":"https://edubruell.github.io/tidyllm/reference/openai_embedding.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Embeddings Using OpenAI API — openai_embedding","text":".llm existing LLMMessage object (character vector texts embed) .model embedding model identifier (default: \"text-embedding-3-small\"). .truncate Whether truncate inputs fit model's context length (default: TRUE). .timeout Timeout API request seconds (default: 120). .dry_run TRUE, perform dry run return request object. .max_tries Maximum retry attempts requests (default: 3).","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/openai_embedding.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Embeddings Using OpenAI API — openai_embedding","text":"matrix column corresponds embedding message message history.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/parse_duration_to_seconds.html","id":null,"dir":"Reference","previous_headings":"","what":"This internal function parses duration strings as returned by the OpenAI API — parse_duration_to_seconds","title":"This internal function parses duration strings as returned by the OpenAI API — parse_duration_to_seconds","text":"internal function parses duration strings returned OpenAI API","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/parse_duration_to_seconds.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"This internal function parses duration strings as returned by the OpenAI API — parse_duration_to_seconds","text":"","code":"parse_duration_to_seconds(.duration_str)"},{"path":"https://edubruell.github.io/tidyllm/reference/parse_duration_to_seconds.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"This internal function parses duration strings as returned by the OpenAI API — parse_duration_to_seconds","text":".duration_str duration string.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/parse_duration_to_seconds.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"This internal function parses duration strings as returned by the OpenAI API — parse_duration_to_seconds","text":"numeric number seconds","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/pdf_page_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Batch Process PDF into LLM Messages — pdf_page_batch","title":"Batch Process PDF into LLM Messages — pdf_page_batch","text":"function processes PDF file page page. page, extracts text converts page image. creates list LLMMessage objects text image multimodal processing. Users can specify range pages process provide custom function generate prompts page.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/pdf_page_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Batch Process PDF into LLM Messages — pdf_page_batch","text":"","code":"pdf_page_batch(   .pdf,   .general_prompt,   .system_prompt = \"You are a helpful assistant\",   .page_range = NULL,   .prompt_fn = NULL )"},{"path":"https://edubruell.github.io/tidyllm/reference/pdf_page_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Batch Process PDF into LLM Messages — pdf_page_batch","text":".pdf Path PDF file. .general_prompt default prompt applied page .prompt_fn provided. .system_prompt Optional system prompt initialize LLMMessage (default \"helpful assistant\"). .page_range vector two integers specifying start end pages process. NULL, pages processed. .prompt_fn optional custom function generates prompt page. function takes page text input returns string. NULL, .general_prompt used pages.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/pdf_page_batch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Batch Process PDF into LLM Messages — pdf_page_batch","text":"list LLMMessage objects, containing text image page.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/perform_api_request.html","id":null,"dir":"Reference","previous_headings":"","what":"Perform an API request to interact with language models — perform_api_request","title":"Perform an API request to interact with language models — perform_api_request","text":"Perform API request interact language models","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/perform_api_request.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Perform an API request to interact with language models — perform_api_request","text":"","code":"perform_api_request(   .request,   .api,   .stream = FALSE,   .timeout = 60,   .max_tries = 3,   .parse_response_fn = NULL,   .dry_run = FALSE )"},{"path":"https://edubruell.github.io/tidyllm/reference/perform_api_request.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Perform an API request to interact with language models — perform_api_request","text":".request httr2 request object. .api API identifier (e.g., \"claude\", \"openai\"). .stream Stream response TRUE. .timeout Request timeout seconds. .max_tries Maximum retry attempts requests (default: 3). .parse_response_fn function parse assistant's reply. .dry_run TRUE, perform dry run return request object.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/perform_api_request.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Perform an API request to interact with language models — perform_api_request","text":"list containing assistant's reply response headers.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/rate_limit_info.html","id":null,"dir":"Reference","previous_headings":"","what":"Get the current rate limit information for all or a specific API — rate_limit_info","title":"Get the current rate limit information for all or a specific API — rate_limit_info","text":"function retrieves rate limit details specified API, APIs stored .tidyllm_rate_limit_env API specified.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/rate_limit_info.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get the current rate limit information for all or a specific API — rate_limit_info","text":"","code":"rate_limit_info(.api_name = NULL)"},{"path":"https://edubruell.github.io/tidyllm/reference/rate_limit_info.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get the current rate limit information for all or a specific API — rate_limit_info","text":".api_name (Optional) name API whose rate limit info want get provided, rate limit info APIs environment returned","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/rate_limit_info.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get the current rate limit information for all or a specific API — rate_limit_info","text":"tibble containing rate limit information.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/ratelimit_from_header.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract rate limit information from API response headers — ratelimit_from_header","title":"Extract rate limit information from API response headers — ratelimit_from_header","text":"Extract rate limit information API response headers","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/ratelimit_from_header.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract rate limit information from API response headers — ratelimit_from_header","text":"","code":"ratelimit_from_header(.response_headers, .api)"},{"path":"https://edubruell.github.io/tidyllm/reference/ratelimit_from_header.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract rate limit information from API response headers — ratelimit_from_header","text":".response_headers Headers API response .api API type (\"claude\", \"openai\",\"groq\")","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/ratelimit_from_header.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract rate limit information from API response headers — ratelimit_from_header","text":"list containing rate limit information","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/send_claude_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Send a Batch of Messages to Claude API — send_claude_batch","title":"Send a Batch of Messages to Claude API — send_claude_batch","text":"function creates submits batch messages Claude API asynchronous processing.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/send_claude_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Send a Batch of Messages to Claude API — send_claude_batch","text":"","code":"send_claude_batch(   .llms,   .model = \"claude-3-5-sonnet-20241022\",   .max_tokens = 1024,   .temperature = NULL,   .top_k = NULL,   .top_p = NULL,   .stop_sequences = NULL,   .api_url = \"https://api.anthropic.com/\",   .verbose = FALSE,   .dry_run = FALSE,   .overwrite = FALSE,   .max_tries = 3,   .timeout = 60,   .id_prefix = \"tidyllm_claude_req_\" )"},{"path":"https://edubruell.github.io/tidyllm/reference/send_claude_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Send a Batch of Messages to Claude API — send_claude_batch","text":".llms list LLMMessage objects containing conversation histories. .model Character string specifying Claude model version (default: \"claude-3-5-sonnet-20241022\"). .max_tokens Integer specifying maximum tokens per response (default: 1024). .temperature Numeric 0 1 controlling response randomness. .top_k Integer diversity limiting top K tokens. .top_p Numeric 0 1 nucleus sampling. .stop_sequences Character vector sequences halt response generation. .api_url Base URL Claude API (default: \"https://api.anthropic.com/\"). .verbose Logical; TRUE, prints message batch ID (default: FALSE). .dry_run Logical; TRUE, returns prepared request object without executing (default: FALSE). .overwrite Logical; TRUE, allows overwriting existing batch ID associated request (default: FALSE). .max_tries Maximum number retries perform request. .timeout Integer specifying request timeout seconds (default: 60). .id_prefix Character string specify prefix generating custom IDs names .llms missing. Defaults \"tidyllm_claude_req_\".","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/send_claude_batch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Send a Batch of Messages to Claude API — send_claude_batch","text":"updated named list .llms identifiers align batch responses, including batch_id attribute.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/send_openai_batch.html","id":null,"dir":"Reference","previous_headings":"","what":"Send a Batch of Messages to OpenAI Batch API — send_openai_batch","title":"Send a Batch of Messages to OpenAI Batch API — send_openai_batch","text":"function creates submits batch messages OpenAI Batch API asynchronous processing.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/send_openai_batch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Send a Batch of Messages to OpenAI Batch API — send_openai_batch","text":"","code":"send_openai_batch(   .llms,   .model = \"gpt-4o\",   .max_completion_tokens = NULL,   .frequency_penalty = NULL,   .logit_bias = NULL,   .logprobs = FALSE,   .top_logprobs = NULL,   .presence_penalty = NULL,   .seed = NULL,   .service_tier = \"auto\",   .stop = NULL,   .temperature = NULL,   .top_p = NULL,   .dry_run = FALSE,   .overwrite = FALSE,   .json_schema = NULL,   .max_tries = 3,   .timeout = 60,   .verbose = FALSE,   .id_prefix = \"tidyllm_openai_req_\" )"},{"path":"https://edubruell.github.io/tidyllm/reference/send_openai_batch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Send a Batch of Messages to OpenAI Batch API — send_openai_batch","text":".llms list LLMMessage objects containing conversation histories. .model Character string specifying OpenAI model version (default: \"gpt-4o\"). .max_completion_tokens Integer specifying maximum tokens per response (default: NULL). .frequency_penalty Number -2.0 2.0. Positive values penalize new tokens based existing frequency text far. .logit_bias named list modifying likelihood specified tokens appearing completion. .logprobs Whether return log probabilities output tokens (default: FALSE). .top_logprobs integer 0 20 specifying number likely tokens return token position. .presence_penalty Number -2.0 2.0. Positive values penalize new tokens based whether appear text far. .seed specified, system make best effort sample deterministically. .service_tier Specifies latency tier use processing request (default: \"auto\"). .stop 4 sequences API stop generating tokens. .temperature sampling temperature use, 0 2. Higher values make output random. .top_p alternative sampling temperature, called nucleus sampling. .dry_run Logical; TRUE, returns prepared request object without executing (default: FALSE). .overwrite Logical; TRUE, allows overwriting existing batch ID associated request (default: FALSE). .json_schema JSON schema object R list enforce output structure (default: NULL). .max_tries Maximum number retries perform request (default: 3). .timeout Integer specifying request timeout seconds (default: 60). .verbose Logical; TRUE, additionaly info requests printed (default: FALSE). .id_prefix Character string specify prefix generating custom IDs names .llms missing (default: \"tidyllm_openai_req_\").","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/send_openai_batch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Send a Batch of Messages to OpenAI Batch API — send_openai_batch","text":"updated named list .llms identifiers align batch responses, including batch_id attribute.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/tidyllm-package.html","id":null,"dir":"Reference","previous_headings":"","what":"tidyllm: Tidy Integration of Large Language Models — tidyllm-package","title":"tidyllm: Tidy Integration of Large Language Models — tidyllm-package","text":"tidy interface integrating large language model (LLM) APIs 'Claude', 'Openai', 'Groq','Mistral' local models via 'Ollama' R workflows. package supports text media-based interactions, interactive message history, stateful rate limit handling, tidy, pipeline-oriented interface streamlined integration data workflows. Web services available https://www.anthropic.com, https://openai.com, https://groq.com, https://mistral.ai/ https://ollama.com.","code":""},{"path":[]},{"path":"https://edubruell.github.io/tidyllm/reference/tidyllm-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"tidyllm: Tidy Integration of Large Language Models — tidyllm-package","text":"Maintainer: Eduard Brüll eduard.bruell@zew.de","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/tidyllm_schema.html","id":null,"dir":"Reference","previous_headings":"","what":"This function creates a JSON schema suitable for use with the API functions in tidyllm, enforcing tidy data principles by disallowing nested structures. — tidyllm_schema","title":"This function creates a JSON schema suitable for use with the API functions in tidyllm, enforcing tidy data principles by disallowing nested structures. — tidyllm_schema","text":"function creates JSON schema suitable use API functions tidyllm, enforcing tidy data principles disallowing nested structures.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/tidyllm_schema.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"This function creates a JSON schema suitable for use with the API functions in tidyllm, enforcing tidy data principles by disallowing nested structures. — tidyllm_schema","text":"","code":"tidyllm_schema(name, ...)"},{"path":"https://edubruell.github.io/tidyllm/reference/tidyllm_schema.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"This function creates a JSON schema suitable for use with the API functions in tidyllm, enforcing tidy data principles by disallowing nested structures. — tidyllm_schema","text":"name character vector specifying schema name. serves identifier schema. ... Named arguments name represents field schema value specifies type. Supported types include R data types: \"character\": Represents charcater type \"string\": Allowed shorthand charachter type \"factor(...)\": string specific allowable values, represented enum JSON. Specify options factor(option1, option2). \"logical\": Represents boolean. \"numeric\": Represents number. \"type[]\": Appending [] allows vector given type, e.g., \"character[]\".","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/tidyllm_schema.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"This function creates a JSON schema suitable for use with the API functions in tidyllm, enforcing tidy data principles by disallowing nested structures. — tidyllm_schema","text":"list representing JSON schema specified fields types, suitable passing openai()'s .json_schema parameter.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/tidyllm_schema.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"This function creates a JSON schema suitable for use with the API functions in tidyllm, enforcing tidy data principles by disallowing nested structures. — tidyllm_schema","text":"tidyllm_schema() function designed make defining JSON schemas tidyllm concise user-friendly. maps R-like types JSON schema types validates inputs enforce tidy data principles. Nested structures allowed maintain compatibility tidy data conventions.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/tidyllm_schema.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"This function creates a JSON schema suitable for use with the API functions in tidyllm, enforcing tidy data principles by disallowing nested structures. — tidyllm_schema","text":"Factor types (factor(...)) treated enumerations JSON limited set allowable string values. Arrays given type can specified appending [] type.","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/tidyllm_schema.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"This function creates a JSON schema suitable for use with the API functions in tidyllm, enforcing tidy data principles by disallowing nested structures. — tidyllm_schema","text":"","code":"if (FALSE) { # \\dontrun{ # Define a schema with tidy data principles json_schema <- tidyllm_schema(   name = \"DocumentAnalysisSchema\",   Title = \"character\",   Authors = \"character[]\",   SuggestedFilename = \"character\",   Type = \"factor(Policy, Research)\",   Answer_Q1 = \"character\",   Answer_Q2 = \"character\",   Answer_Q3 = \"character\",   Answer_Q4 = \"character\",   KeyCitations = \"character[]\" )  # Pass the schema to openai() result <- openai(   .llm = msg,   .json_schema = json_schema ) } # }"},{"path":"https://edubruell.github.io/tidyllm/reference/update_rate_limit.html","id":null,"dir":"Reference","previous_headings":"","what":"Update the standard API rate limit info in the hidden .tidyllm_rate_limit_env environment — update_rate_limit","title":"Update the standard API rate limit info in the hidden .tidyllm_rate_limit_env environment — update_rate_limit","text":"function initializes stores ratelimit information API functions future use","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/update_rate_limit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update the standard API rate limit info in the hidden .tidyllm_rate_limit_env environment — update_rate_limit","text":"","code":"update_rate_limit(.api_name, .response_object)"},{"path":"https://edubruell.github.io/tidyllm/reference/update_rate_limit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update the standard API rate limit info in the hidden .tidyllm_rate_limit_env environment — update_rate_limit","text":".api_name name API initialize retrieve environment. .response_object preparsed response object cotaining info remaining requests, tokens rest times","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/wait_rate_limit.html","id":null,"dir":"Reference","previous_headings":"","what":"Wait for ratelimit restore times to ellapse if necessary — wait_rate_limit","title":"Wait for ratelimit restore times to ellapse if necessary — wait_rate_limit","text":"function implements standardized wait rate limit resets","code":""},{"path":"https://edubruell.github.io/tidyllm/reference/wait_rate_limit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Wait for ratelimit restore times to ellapse if necessary — wait_rate_limit","text":"","code":"wait_rate_limit(.api_name, .min_tokens_reset)"},{"path":"https://edubruell.github.io/tidyllm/reference/wait_rate_limit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Wait for ratelimit restore times to ellapse if necessary — wait_rate_limit","text":".api_name name API rate limit want wait .min_tokens_reset token boundary wish reset (Typically larger size message)","code":""},{"path":[]},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"major-features-0-1-11","dir":"Changelog","previous_headings":"","what":"Major Features","title":"Version 0.1.11 (Current Development Version)","text":"Support Anthropic OpenAI batch request API","code":""},{"path":[]},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"breaking-changes-0-1-10","dir":"Changelog","previous_headings":"","what":"Breaking Changes","title":"Version 0.1.10","text":"get_reply() split two type-stable functions: get_reply() text get_reply_data() structured outputs. first step towards long-term plan move schema definitions user requests instead API-functions might also reasonable later batch-API functions","code":""},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"improvements-0-1-10","dir":"Changelog","previous_headings":"","what":"Improvements","title":"Version 0.1.10","text":"Rate limiting updated use httr2::req_retry(): Rate limiting now uses right 429 headers come.","code":""},{"path":[]},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"major-features-0-1-9","dir":"Changelog","previous_headings":"","what":"Major Features","title":"Version 0.1.9","text":"Enhanced Input Validation: API functions now improved input validation, ensuring better alignment API documentation Improved error handling human-readable error messages failed requests API Advanced JSON Mode openai(): openai() function now supports advanced .json_schemas, allowing structured output JSON mode precise responses. Reasoning Models Support: Support O1 reasoning models added, better handling system prompts openai() function. Streaming callback functions refactored: Given streaming callback format Open AI, Mistral Groq nearly identical three now rely callback function.","code":""},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"breaking-changes-0-1-9","dir":"Changelog","previous_headings":"","what":"Breaking Changes","title":"Version 0.1.9","text":"chatgpt() Deprecated: chatgpt() function deprecated favor openai(). Users migrate openai() take advantage new features enhancements.","code":""},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"improvements-0-1-9","dir":"Changelog","previous_headings":"","what":"Improvements","title":"Version 0.1.9","text":"Better Error Handling: openai(), ollama(), claude() functions now return informative error messages API calls fail, helping debugging troubleshooting.","code":""},{"path":[]},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"major-features-0-1-8","dir":"Changelog","previous_headings":"","what":"Major Features","title":"Version 0.1.8","text":"Embedding functions process message histories combine text message content media attachments embedding models. ollama_embedding() generate embeddings using Ollama API. openai_embedding() generate embeddings using OpenAI API. mistral_embedding() generate embeddings using Mistral API.","code":""},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"improvements-0-1-8","dir":"Changelog","previous_headings":"","what":"Improvements","title":"Version 0.1.8","text":"PDF Page Support llm_message(): llm_message() function now supports specifying range pages PDF passing list filename, start_page, end_page. allows users extract process specific pages PDF.","code":""},{"path":[]},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"major-features-0-1-7","dir":"Changelog","previous_headings":"","what":"Major Features","title":"Version 0.1.7","text":"PDF Page Batch Processing: Introduced pdf_page_batch() function, processes PDF files page page, extracting text converting page image, allowing general prompt page-specific prompts. function generates list LLMMessage objects can sent API work batch-API functions tidyllm.","code":""},{"path":[]},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"major-features-0-1-6","dir":"Changelog","previous_headings":"","what":"Major Features","title":"Version 0.1.6","text":"Support Mistral API: New mistral() function use Mistral Models Le Platforme servers hosted EU, rate-limiting streaming support.","code":""},{"path":[]},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"major-features-0-1-5","dir":"Changelog","previous_headings":"","what":"Major Features","title":"Version 0.1.5","text":"last_user_message() pulls last message user sent. get_reply() gets assistant reply given index assistant messages. get_user_message() gets user message given index user messages.","code":""},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"improvements-0-1-5","dir":"Changelog","previous_headings":"","what":"Improvements","title":"Version 0.1.5","text":"Easier Troubleshooting API-function: API functions now support .dry_run argument, allowing users generate httr2-request easier debugging inspection. API Function Tests: Implemented httptest2-based tests mock responses API functions, covering basic functionality rate-limiting.","code":""},{"path":[]},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"major-features-0-1-4","dir":"Changelog","previous_headings":"","what":"Major Features","title":"Version 0.1.4","text":"Model Download: Introduced ollama_download_model() function download models Ollama API. supports streaming mode provides live progress bar updates download progress.","code":""},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"improvements-0-1-4","dir":"Changelog","previous_headings":"","what":"Improvements","title":"Version 0.1.4","text":"Refactoring llm_message()","code":""},{"path":[]},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"major-features-0-1-3","dir":"Changelog","previous_headings":"","what":"Major Features","title":"Version 0.1.3","text":"groq() function now supports images. complete streaming support across API-functions.","code":""},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"breaking-changes-0-1-3","dir":"Changelog","previous_headings":"","what":"Breaking Changes","title":"Version 0.1.3","text":"Groq Models: System prompts longer sent Groq models, since many models Groq support multimodal models Groq disallow .","code":""},{"path":[]},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"improvements-0-1-2","dir":"Changelog","previous_headings":"","what":"Improvements","title":"Version 0.1.2","text":"New unit tests llm_message(). Improvements streaming functions.","code":""},{"path":[]},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"major-features-0-1-1","dir":"Changelog","previous_headings":"","what":"Major Features","title":"Version 0.1.1","text":"JSON Mode: JSON mode now widely supported across API functions, allowing structured outputs APIs support . .json argument now passed API functions, specifying API respond, needed anymore last_reply(). Improved last_reply() Behavior: behavior last_reply() function changed. now automatically handles JSON replies parsing structured data falling back raw text case errors. can still force raw text replies even JSON output using .raw argument.","code":""},{"path":"https://edubruell.github.io/tidyllm/news/index.html","id":"breaking-changes-0-1-1","dir":"Changelog","previous_headings":"","what":"Breaking Changes","title":"Version 0.1.1","text":"last_reply(): .json argument longer used, JSON replies automatically parsed. Use .raw force raw text replies.","code":""}]
